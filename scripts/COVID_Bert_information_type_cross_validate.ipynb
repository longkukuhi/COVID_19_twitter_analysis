{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8yb2kzS-JW9"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5bqvajMe0Ql"
   },
   "outputs": [],
   "source": [
    "#Acknowlegement: Some codes used to initialise BERT are from BERT tutorial(https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2v2kcLX81EH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1218 14:20:00.545545 139713408845632 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
      "I1218 14:20:00.561142 139713408845632 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import string\n",
    "import os \n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import copy\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from covid_tools import *\n",
    "from BERTs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpjyJxpE-HMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 18 14:20:01 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 41%   29C    P8     9W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU situation\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# load orginal data(COVID-19 TREC-IS 2020 task) from json file.\n",
    "cv19_dc = pd.read_json('./Pythonbooks/Data/COVID/cv19_dc_test.json',lines=True,orient='records')\n",
    "cv19_ws = pd.read_json('./Pythonbooks/Data/COVID/cv19_washington_state_test.json',lines=True,orient='records')\n",
    "cv19_ny = pd.read_json('./Pythonbooks/Data/COVID/cv19_nyc_test.json',lines=True,orient='records')\n",
    "cv19_a_run_info_labeled = pd.read_csv('cache/cv19_TREC_2020_all_labeled_A.csv',\n",
    "                           converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                       \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "cv19_dc_labeled = pd.merge(cv19_dc,cv19_a_run_info_labeled,on=['id'])\n",
    "cv19_ws_labeled = pd.merge(cv19_ws,cv19_a_run_info_labeled,on=['id'])\n",
    "cv19_ny_labeled = pd.merge(cv19_ny,cv19_a_run_info_labeled,on=['id'])\n",
    "temp = [cv19_dc_labeled,cv19_ny_labeled,cv19_ws_labeled ]\n",
    "cv19_a_run_info_labeled =  pd.concat(temp)\n",
    "cv19_a_run_info_labeled = cv19_a_run_info_labeled.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv19_b_run_info_labeled = pd.read_csv('cache/cv19_TREC_2020_all_labeled_B.csv',\n",
    "                           converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                       \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "cv19_Houston_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Houston_2020.json',lines=True,orient='records')\n",
    "cv19_Atlanta_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Atlanta_2020.json',lines=True,orient='records')\n",
    "cv19_Melbourne2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Melbourne_2020.json',lines=True,orient='records')\n",
    "cv19_NYC_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_NYC_2020.json',lines=True,orient='records')\n",
    "cv19_acksonville_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Jacksonville_2020.json',lines=True,orient='records')\n",
    "\n",
    "# Merge original dataset with lablled dataset based on id of tweets\n",
    "cv19_Houston_2020_labeled = pd.merge(cv19_Houston_2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_Atlanta_2020_labeled = pd.merge(cv19_Atlanta_2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_Melbourne2020_labeled = pd.merge(cv19_Melbourne2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_NYC_2020_labeled = pd.merge(cv19_NYC_2020, cv19_b_run_info_labeled, on=['id'])\n",
    "cv19_acksonville_2020_labeled = pd.merge(cv19_acksonville_2020,cv19_b_run_info_labeled, on=['id'])\n",
    "temp = [cv19_Houston_2020_labeled,cv19_Atlanta_2020_labeled,cv19_Melbourne2020_labeled,cv19_NYC_2020_labeled, cv19_acksonville_2020_labeled ]\n",
    "cv19_b_run_info_labeled =  pd.concat(temp)\n",
    "cv19_b_run_info_labeled = cv19_b_run_info_labeled.reset_index()\n",
    "cv19_b_run_info_labeled = cv19_b_run_info_labeled.rename(columns={'full_text_x':'full_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4505"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv19_b_run_info_labeled )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSbMI-8I-RC7"
   },
   "source": [
    "# Pre-process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__CCHe61-rTI"
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','EmergingThreats','NewSubEvent','ServiceAvailable','Advice',]\n",
    "\n",
    "# # Extract training set\n",
    "# featureList = ['full_text'] #,'favorited'\n",
    "# dataset_inputs = extract_features(Tweets,featureList)\n",
    "\n",
    "# Extract labels\n",
    "labelList = ['categories']\n",
    "a_labels = Process_labels.extract_features(cv19_a_run_info_labeled,labelList)['categories']\n",
    "b_labels = Process_labels.extract_features(cv19_b_run_info_labeled,labelList)['categories']\n",
    "# Transfer a list of text labels into a list of number labels, like [0,1,0,0,0,0,0]\n",
    "a_labels = Process_labels.extractLabels(cv19_a_run_info_labeled,a_labels)\n",
    "b_labels = Process_labels.extractLabels(cv19_b_run_info_labeled,b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4505"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv19_b_run_info_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1C9EyXvghvU"
   },
   "source": [
    "#### Build label for each catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "0_U-AdJoghAm",
    "outputId": "0a51a58a-ecdb-4391-8432-aafa7a9b7045"
   },
   "outputs": [],
   "source": [
    "def build_single_label(labels):\n",
    "    labels_single = {}\n",
    "    for i in range(7):\n",
    "      print('Currrent is processing on categorie %s'%(categories[i]))\n",
    "      print()\n",
    "      # if i == 3:\n",
    "      #   continue\n",
    "      labels_one_category = []\n",
    "      for j in range(len(labels)):\n",
    "        labels_one_category.append(labels[j][i])\n",
    "      labels_single[i] = labels_one_category\n",
    "    return labels_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Currrent is processing on categorie Volunteer\n",
      "\n",
      "Currrent is processing on categorie EmergingThreats\n",
      "\n",
      "Currrent is processing on categorie NewSubEvent\n",
      "\n",
      "Currrent is processing on categorie ServiceAvailable\n",
      "\n",
      "Currrent is processing on categorie Advice\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Currrent is processing on categorie Volunteer\n",
      "\n",
      "Currrent is processing on categorie EmergingThreats\n",
      "\n",
      "Currrent is processing on categorie NewSubEvent\n",
      "\n",
      "Currrent is processing on categorie ServiceAvailable\n",
      "\n",
      "Currrent is processing on categorie Advice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','EmergingThreats','NewSubEvent','ServiceAvailable','Advice']\n",
    "# Sperate labels for each category and store them in one dict\n",
    "# A dict Contains labels for all categories e.g. trainL_single[category]\n",
    "labels_single_a = build_single_label(a_labels)\n",
    "labels_single_b = build_single_label(b_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbJTdlRHD1lp"
   },
   "source": [
    "## Tokenization: Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_inputs_A_run = pd.read_csv('priority/a_run/dataset_all.csv')\n",
    "# B_run_temp = pd.read_csv('priority/b_run/dataset_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_inputs_A_run = pd.read_csv('cache/cv19_TREC_2020_all_labeled_A.csv',\n",
    "#                            converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "#                                        \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "\n",
    "# dataset_inputs_B_run = pd.read_csv('cache/cv19_TREC_2020_all_labeled_B.csv',\n",
    "#                            converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "#                                        \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "\n",
    "# B_run_temp = pd.read_csv('priority/b_run/dataset_all.csv')\n",
    "# dataset_inputs_B_run = pd.merge(cv19_b_run_info_labeled,B_run_temp,on=['id'])\n",
    "# dataset_inputs_B_run = dataset_inputs_B_run.rename(columns={'full_text_x':'full_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_run_temp = pd.read_csv('priority/b_run/dataset_all.csv')\n",
    "# dataset_inputs_B_run = pd.merge(cv19_b_run_info_labeled,B_run_temp,on=['id'])\n",
    "dataset_inputs_B_run = cv19_b_run_info_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4505"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_inputs_B_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Texas-78/1 4:36 Daily-Total 438293, Up 10064, ...\n",
       "1       Texas counties are requesting refrigerated tru...\n",
       "2       Latest health update: link in profile. @ Houst...\n",
       "3       HTV Houston Television - COVID-19 Talk with Lo...\n",
       "4       All Masks, Disinfectant Wipes/Sprays &amp; San...\n",
       "                              ...                        \n",
       "4500    The commissioner for St. Johns County just sou...\n",
       "4501    Updated: Emergency vehicles in Duval County on...\n",
       "4502    Jacksonville #Jaguars ativou o RB Ryquell Arms...\n",
       "4503    Prayers for Lt.Cunningham's family &amp; his c...\n",
       "4504    Alexander Bernardo, 22yo Dietary Aide, Park Ri...\n",
       "Name: full_text, Length: 4505, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_inputs_B_run.full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_A_run = dataset_inputs_A_run.full_text.values\n",
    "# labels_A_run = dataset_inputs_A_run.target.values\n",
    "sentences_B_run = dataset_inputs_B_run.full_text.values\n",
    "# labels_B_run = dataset_inputs_B_run.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "MWJ5VTWDD-MP",
    "outputId": "cd97c99a-41fa-4c24-be46-85331f4bf25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(sentences_A_run)):\n",
    "  sentences_A_run[idx] = sentences_A_run[idx]  + ' '+ dataset_inputs_A_run.hashtags.values[idx] + ' ' + str(dataset_inputs_A_run.favorite_count.values[idx])\n",
    "\n",
    "# for idx in range(len(sentences_B_run)):\n",
    "#   sentences_B_run[idx] = sentences_B_run[idx]  + ' '+ dataset_inputs_B_run.hashtags.values[idx] + ' ' + str(dataset_inputs_B_run.favorite_count.values[idx])\n",
    "for idx in range(len(sentences_B_run)):\n",
    "  sentences_B_run[idx] = sentences_B_run[idx]  +' ' + str(dataset_inputs_B_run.favorite_count.values[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "q2JPyq9iFHSr",
    "outputId": "9fecc6d0-9e01-4470-8a0b-1bd53a6808d2"
   },
   "outputs": [],
   "source": [
    "def Encode_sents(sentences):\n",
    "    input_ids = []\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "\n",
    "                            #max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        # If the length of token is over the bert limitation, cut off the back\n",
    "        if(len(encoded_sent)>144):\n",
    "            diff = int((len(encoded_sent) - 145)/2)\n",
    "            encoded_sent = encoded_sent[diff:144+diff]\n",
    "        input_ids.append(encoded_sent)\n",
    "    return input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_A_run = Encode_sents(sentences_A_run)\n",
    "input_ids_B_run = Encode_sents(sentences_B_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HApbJUIo3uoT"
   },
   "source": [
    "## Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "wXkeVbwFFLmg",
    "outputId": "f546ad1c-537c-4262-e092-44b2d88b2e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 144 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "\n",
    "MAX_LEN = 144\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids_A_run = pad_sequences(input_ids_A_run, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_B_run = pad_sequences(input_ids_B_run, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSzbtxFP30ZK"
   },
   "source": [
    "## Attension mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sys-Jh5vFIEq"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "def build_attension_mask(input_ids):\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks_A_run = build_attension_mask(input_ids_A_run)\n",
    "attention_masks_B_run = build_attension_mask(input_ids_B_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThdCl_uCsRIW"
   },
   "source": [
    "# Classication one category by on category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jhBKQGW53XB"
   },
   "source": [
    "All training split dataset first. Then use BERT to evaluate and store the result into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_cross_vali_split(input_ids, labels, attention_masks):\n",
    "    # Use train_test_split to split our data into train and validation and test sets for\n",
    "    # training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Use 80% for training and 20% for validation and  5% for testing.\n",
    "    temp_inputs, test_inputs,  temp_labels, test_labels = train_test_split(input_ids, labels, test_size=0.05,random_state=2020)\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(temp_inputs, temp_labels, test_size=0.2,random_state=2020)\n",
    "\n",
    "    # Do the same for the masks.\n",
    "    temp_masks, test_masks, temp_labels, _ = train_test_split(attention_masks, labels, test_size=0.05,random_state=2020)\n",
    "\n",
    "    train_masks, validation_masks, _, _ = train_test_split(temp_masks, temp_labels, test_size=0.2,random_state=2020)\n",
    "    return train_inputs,  train_labels, train_masks, validation_inputs, validation_labels, validation_masks, test_inputs, test_labels, test_masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Vz_7WthsURS"
   },
   "outputs": [],
   "source": [
    "# A dict to store all metrics fro each category, Use name of category as the key to search\n",
    "all_metrics_all_category = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_test_one_dataset(model_pth,test_batch,batch_size,category_number):\n",
    "    # Build new BERT model\n",
    "    model  = BertForSequenceClassification.from_pretrained(\n",
    "      \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "      num_labels=category_number,  # The number of output labels--2 for binary classification.\n",
    "      # You can increase this for multi-class tasks.\n",
    "      output_attentions=False,  # Whether the model returns attentions weights.\n",
    "      output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    # send model to GPU\n",
    "\n",
    "    model.load_state_dict(torch.load(model_pth))\n",
    "\n",
    "\n",
    "    test_inputs = torch.tensor(test_batch[0]).to(torch.int64)\n",
    "    test_labels = torch.tensor(test_batch[1]).to(torch.int64)\n",
    "    test_masks = torch.tensor(test_batch[2]).to(torch.int64)\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Testing\n",
    "    # ========================================\n",
    "    print()\n",
    "    print('Testing...')\n",
    "    # record the start time\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Identify GPU to use\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    # put model in evaluation model\n",
    "    model.eval()\n",
    "\n",
    "    model.to(device)\n",
    "    # Build list to store metrics of each epoches\n",
    "    test_acc_all = []\n",
    "    test_precision_all = []\n",
    "    test_recall_all = []\n",
    "    test_f1_all = []\n",
    "    test_accuracy = 0\n",
    "    nb_test_steps, nb_test_examples = 0, 0\n",
    "\n",
    "    # Build the dict for each metrics and store metrics for each category\n",
    "    test_precision, test_recall, test_f1 = {}, {}, {}\n",
    "    for i in range(category_number):\n",
    "        test_f1[i], test_precision[i], test_recall[i] = 0, 0, 0\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():\n",
    "\n",
    "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "          # values prior to applying an activation function like the softmax.\n",
    "          logits = outputs[0]\n",
    "\n",
    "          # Move logits and labels to CPU\n",
    "          logits = logits.detach().cpu().numpy()\n",
    "          label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "          # Calculate the accuracy for this batch of test sentences.\n",
    "          tmp_test_accuracy = Scoring_nn.flat_accuracy(None, logits, label_ids)\n",
    "\n",
    "          tmp_test_precision = {}\n",
    "          tmp_test_recall = {}\n",
    "          tmp_test_f1 = {}\n",
    "          for i in range(category_number):\n",
    "            tmp_test_precision[i] = 0\n",
    "            tmp_test_recall[i] = 0\n",
    "            tmp_test_f1[i] = 0\n",
    "\n",
    "          for k in range(category_number):\n",
    "            tmp_test_precision[k], tmp_test_recall[k], tmp_test_f1[k] = Scoring_nn.flat_metrics(None, logits, label_ids, k)\n",
    "\n",
    "          # Accumulate the total accuracy.\n",
    "          test_accuracy += tmp_test_accuracy\n",
    "\n",
    "          for k in range(category_number):\n",
    "            test_precision[k] += tmp_test_precision[k]\n",
    "            test_recall[k] += tmp_test_recall[k]\n",
    "            test_f1[k] += tmp_test_f1[k]\n",
    "\n",
    "          # Track the number of batches\n",
    "          nb_test_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(test_accuracy / nb_test_steps))\n",
    "\n",
    "    # Calculate the average value for each metrics\n",
    "    precision_avg, recall_avg, f1_avg = 0, 0, 0\n",
    "    for k in range(category_number):\n",
    "        print(\"Category: %d\" % (k))\n",
    "        test_precision[k] = test_precision[k] / nb_test_steps\n",
    "        precision_avg += test_precision[k]\n",
    "        test_recall[k] = test_recall[k] / nb_test_steps\n",
    "        recall_avg += test_recall[k]\n",
    "        test_f1[k] = test_f1[k] / nb_test_steps\n",
    "        f1_avg += test_f1[k]\n",
    "        print(\"  Precision: {0:.4f}\".format(test_precision[k]))\n",
    "        print(\"  Recall: {0:.4f}\".format(test_recall[k]))\n",
    "        print(\"  F1: {0:.4f}\".format(test_f1[k]))\n",
    "    # Report other metrics\n",
    "    print(\"The average precision is: {0:.4f}\".format(precision_avg / category_number))\n",
    "    print(\"The average recall is: {0:.4f}\".format(recall_avg / category_number))\n",
    "    print(\"The average f1 is: {0:.4f}\".format(f1_avg / category_number))\n",
    "    print(\"  Testing took: {:}\".format(Scoring_nn.format_time(time.time() - t0)))\n",
    "    \n",
    "    del model, b_input_ids, b_input_mask, b_labels, outputs, logits, label_ids\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olHEfcJDhrll"
   },
   "source": [
    "### For 'GoodsServices'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVgSb3P5FPDa"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vw3vfQF4InQr"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        ):\n",
    "    import json\n",
    "    \n",
    "    # training\n",
    "    section_category = section_category\n",
    "    a_labels = a_batch[1][section_category]\n",
    "    b_labels = b_batch[1][section_category]\n",
    "\n",
    "    train_inputs_A_run,  train_labels_A_run, train_masks_A_run, validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run,test_inputs_A_run, test_labels_A_run, test_masks_A_run = BERT_cross_vali_split(a_batch[0], a_labels, a_batch[2])\n",
    "\n",
    "    train_inputs_B_run,  train_labels_B_run, train_masks_B_run, validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run,test_inputs_B_run, test_labels_B_run, test_masks_B_run = BERT_cross_vali_split(b_batch[0], b_labels, b_batch[2])\n",
    "    test_batch_A_run = [test_inputs_A_run,test_labels_A_run,test_masks_A_run]\n",
    "    test_batch_B_run = [test_inputs_B_run,test_labels_B_run,test_masks_B_run]\n",
    "\n",
    "    batch_size = 32\n",
    "    lr = 3e-5\n",
    "    category_number = 2\n",
    "    input_ids_A_run,  attention_masks_A_run = a_batch[0],a_batch[2]\n",
    "    input_ids_B_run, attention_masks_B_run = b_batch[0], b_batch[2]\n",
    "    \n",
    "    print('Train on dataset-a and test on dataset-b')\n",
    "    print('Train on dataset-a and test on dataset-b')\n",
    "    # Train on dataset-a and test on dataset-b\n",
    "    Bert = Bertnn_rectified_info(train_inputs_A_run,  train_labels_A_run, train_masks_A_run,\n",
    "                                 validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run, test_batch_B_run, batch_size=batch_size,lr=lr)\n",
    "    \n",
    "    all_metrics_one, all_tesst_metrics = Bert.searchUpsample(30)\n",
    "    all_metrics_one['test'] = all_tesst_metrics\n",
    "    all_metrics_all_category[section_category] = all_metrics_one\n",
    "    if not os.path.exists('BertSearchResult'):\n",
    "      os.makedirs('BertSearchResult')\n",
    "\n",
    "    filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)+ 'train_on_a'\n",
    "    with open(filename,'w') as file_obj:\n",
    "      json.dump(all_metrics_one,file_obj)\n",
    "      print('Successfully save file %s'%(filename))\n",
    "\n",
    "    del Bert\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    model_pth_old = './Best_val_Bert_rectified_binary.pth'\n",
    "    model_pth = './Best_info_rectified_best_'+str(section_category)+'_train_on_a_'+'.pth'\n",
    "    os.rename(model_pth_old,model_pth)    \n",
    "    test_batch = [input_ids_A_run, a_labels, attention_masks_A_run]\n",
    "    bert_test_one_dataset(model_pth,test_batch,batch_size,category_number)\n",
    "    \n",
    "    \n",
    "    print('Train on dataset-b and test on dataset-a')\n",
    "    print('Train on dataset-b and test on dataset-a')\n",
    "    # Train on dataset-b and test on dataset-a\n",
    "    Bert = Bertnn_rectified_info(train_inputs_B_run,  train_labels_B_run, train_masks_B_run, validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run, test_batch_A_run, batch_size=batch_size,lr=lr)\n",
    "    all_metrics_one, all_tesst_metrics = Bert.searchUpsample(30)\n",
    "    all_metrics_one['test'] = all_tesst_metrics\n",
    "    all_metrics_all_category[section_category] = all_metrics_one\n",
    "    if not os.path.exists('BertSearchResult'):\n",
    "      os.makedirs('BertSearchResult')\n",
    "\n",
    "    filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category) + 'train_on_b'\n",
    "    with open(filename,'w') as file_obj:\n",
    "      json.dump(all_metrics_one,file_obj)\n",
    "      print('Successfully save file %s'%(filename))\n",
    "\n",
    "    del Bert\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    model_pth_old = './Best_val_Bert_rectified_binary.pth'\n",
    "    model_pth = './Best_info_rectified_best_'+str(section_category)+'_train_on_b_'+'.pth'\n",
    "    os.rename(model_pth_old,model_pth)    \n",
    "\n",
    "    test_batch = [input_ids_B_run, b_labels, attention_masks_B_run]\n",
    "    bert_test_one_dataset(model_pth,test_batch,batch_size,category_number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9gPQyuHZipw",
    "outputId": "be5de905-0b35-492a-8308-710e13783062",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Bertnn_rectified_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1794259744c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcategory_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train on dataset-a and test on dataset-b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mBert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertnn_rectified_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs_A_run\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_labels_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_inputs_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_masks_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_B_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mall_metrics_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_tesst_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mall_metrics_one\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tesst_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Bertnn_rectified_info' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 3e-5\n",
    "\n",
    "category_number = 2\n",
    "# Train on dataset-a and test on dataset-b\n",
    "Bert = Bertnn_rectified_info(train_inputs_A_run,  train_labels_A_run, train_masks_A_run, validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run, test_batch_B_run, batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(30)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)+ 'train_on_a'\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "model_pth_old = './Best_val_Bert_rectified_binary.pth'\n",
    "model_pth = './Best_info_rectified_best_'+str(section_category)+'_train_on_a_'+'.pth'\n",
    "os.rename(model_pth_old,model_pth)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Dyn2ddG-jkfQ",
    "outputId": "d1085ac1-8fce-4aa1-ad12-b13ab71cb2bc"
   },
   "outputs": [],
   "source": [
    "all_metrics_one['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pth = './Best_info_rectified_best_'+str(section_category)+'_train_on_a_'+'.pth'\n",
    "test_batch = [input_ids_A_run, a_labels, attention_masks_A_run]\n",
    "bert_test_one_dataset(model_pth,test_batch,batch_size,category_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train on dataset-b and test on dataset-a\n",
    "Bert = Bertnn_rectified_info(train_inputs_B_run,  train_labels_B_run, train_masks_B_run, validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run, test_batch_A_run, batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(30)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category) + 'train_on_b'\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "model_pth_old = './Best_val_Bert_rectified_binary.pth'\n",
    "model_pth = './Best_info_rectified_best_'+str(section_category)+'_train_on_b_'+'.pth'\n",
    "os.rename(model_pth_old,model_pth)    \n",
    "\n",
    "test_batch = [input_ids_B_run, b_labels, attention_masks_B_run]\n",
    "bert_test_one_dataset(model_pth,test_batch,batch_size,category_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f48pz_B6io7Y"
   },
   "source": [
    "### For 'InformationWanted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQuojg0sWbjt"
   },
   "source": [
    "#### Cross event validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_category = 1\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEUrbPFwJU1W"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ck8eoWGDZip_",
    "outputId": "7156f0ab-6c8e-4ea4-9b68-849e5642e802",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "w_ddsuV_Wbj2",
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfDSm7suJNu_"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_dsfQffZiqB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "60d3c97b-7d53-4e97-e43e-a78001da7d48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_rectified_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "MKDy60_hZiqC",
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJiFSGLhiqKz"
   },
   "source": [
    "### For 'Volunteer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rh3iwCqWcSk"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_category = 2\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqB4vLBYJVim"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N0VVU00JZiqJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7156f0ab-6c8e-4ea4-9b68-849e5642e802"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "bSITnL01WcSr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAk234wpJOuX"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybkhk_NaZiqK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_rectified_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "A2nypAMJJOua",
    "outputId": "d1085ac1-8fce-4aa1-ad12-b13ab71cb2bc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_metrics_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4143f3b7477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_metrics_one\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_metrics_one' is not defined"
     ]
    }
   ],
   "source": [
    "all_metrics_one['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDYahFJ_irjM"
   },
   "source": [
    "### For 'EmergingThreats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqxvbG3XWc4M"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ozpkc196Wc4N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_category = 3\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxpATCNRJWRu"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjVH5bq4JP4W"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxW5G6xTWRWm"
   },
   "source": [
    "### For 'NewSubEvent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egQ4fqBGWdd8"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "six78S36Wdd9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids_A_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f32524d7fb95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids_A_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_single_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks_A_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids_A_run' is not defined"
     ]
    }
   ],
   "source": [
    "section_category = 4\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee-0UKtgJXAO"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zZZMnyiQZiqz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7156f0ab-6c8e-4ea4-9b68-849e5642e802"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "5_K4mVl0WdeC",
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_ge-7JOJQrH"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDc4WdigZiq0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_rectified_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "pCR0i2rpJQrK",
    "outputId": "d1085ac1-8fce-4aa1-ad12-b13ab71cb2bc"
   },
   "outputs": [],
   "source": [
    "all_metrics_one['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kfAWcW0WSQm"
   },
   "source": [
    "### For 'ServiceAvailable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXtcQIcrWeDc"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EM42X_VfWeDd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on dataset-a and test on dataset-b\n",
      "Train on dataset-a and test on dataset-b\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current increase 0 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 278 sampled in this category.\n",
      "After up-sample, there are 5762 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5484\n",
      "0.9517528635890316\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "278\n",
      "0.048247136410968415\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:23.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    361.    Elapsed: 0:00:57.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:08.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:20.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:31.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:42.\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.96\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.10\n",
      "  Recall: 0.08\n",
      "  F1: 0.08\n",
      "The average precision is: 0.5278\n",
      "The average recall is: 0.5390\n",
      "The average f1 is: 0.5298\n",
      "  Training epcoh took: 0:01:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9673\n",
      "  Recall: 0.9949\n",
      "  F1: 0.9803\n",
      "Category: 1\n",
      "  Precision: 0.1154\n",
      "  Recall: 0.0934\n",
      "  F1: 0.1007\n",
      "The average precision is: 0.5413\n",
      "The average recall is: 0.5442\n",
      "The average f1 is: 0.5405\n",
      "  Validation took: 0:00:05\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.540518\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    361.    Elapsed: 0:00:56.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:08.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:19.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:30.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:42.\n",
      "\n",
      "  Average training loss: 0.85\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.32\n",
      "  Recall: 0.28\n",
      "  F1: 0.29\n",
      "The average precision is: 0.6466\n",
      "The average recall is: 0.6368\n",
      "The average f1 is: 0.6357\n",
      "  Training epcoh took: 0:01:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9780\n",
      "  Recall: 0.9848\n",
      "  F1: 0.9808\n",
      "Category: 1\n",
      "  Precision: 0.2418\n",
      "  Recall: 0.2308\n",
      "  F1: 0.2275\n",
      "The average precision is: 0.6099\n",
      "The average recall is: 0.6078\n",
      "The average f1 is: 0.6041\n",
      "  Validation took: 0:00:06\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.604116\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    361.    Elapsed: 0:00:57.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:08.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:19.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:31.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:42.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.47\n",
      "  Recall: 0.45\n",
      "  F1: 0.45\n",
      "The average precision is: 0.7285\n",
      "The average recall is: 0.7225\n",
      "The average f1 is: 0.7215\n",
      "  Training epcoh took: 0:01:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9762\n",
      "  Recall: 0.9911\n",
      "  F1: 0.9832\n",
      "Category: 1\n",
      "  Precision: 0.2198\n",
      "  Recall: 0.1795\n",
      "  F1: 0.1908\n",
      "The average precision is: 0.5980\n",
      "The average recall is: 0.5853\n",
      "The average f1 is: 0.5870\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    361.    Elapsed: 0:00:57.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:08.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:19.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:31.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:42.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.49\n",
      "  Recall: 0.48\n",
      "  F1: 0.48\n",
      "The average precision is: 0.7429\n",
      "The average recall is: 0.7386\n",
      "The average f1 is: 0.7390\n",
      "  Training epcoh took: 0:01:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9758\n",
      "  Recall: 0.9862\n",
      "  F1: 0.9805\n",
      "Category: 1\n",
      "  Precision: 0.2308\n",
      "  Recall: 0.2198\n",
      "  F1: 0.2198\n",
      "The average precision is: 0.6033\n",
      "The average recall is: 0.6030\n",
      "The average f1 is: 0.6002\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 0\n",
      "Current increase 10 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 278 sampled in this category.\n",
      "After up-sample, there are 8542 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5484\n",
      "0.6420042144696793\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3058\n",
      "0.35799578553032074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    534.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    534.    Elapsed: 0:00:28.\n",
      "  Batch   120  of    534.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    534.    Elapsed: 0:00:55.\n",
      "  Batch   200  of    534.    Elapsed: 0:01:09.\n",
      "  Batch   240  of    534.    Elapsed: 0:01:23.\n",
      "  Batch   280  of    534.    Elapsed: 0:01:36.\n",
      "  Batch   320  of    534.    Elapsed: 0:01:50.\n",
      "  Batch   360  of    534.    Elapsed: 0:02:03.\n",
      "  Batch   400  of    534.    Elapsed: 0:02:17.\n",
      "  Batch   440  of    534.    Elapsed: 0:02:31.\n",
      "  Batch   480  of    534.    Elapsed: 0:02:45.\n",
      "  Batch   520  of    534.    Elapsed: 0:02:58.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.95\n",
      "  Recall: 0.95\n",
      "  F1: 0.95\n",
      "Category: 1\n",
      "  Precision: 0.91\n",
      "  Recall: 0.90\n",
      "  F1: 0.89\n",
      "The average precision is: 0.9306\n",
      "The average recall is: 0.9256\n",
      "The average f1 is: 0.9202\n",
      "  Training epcoh took: 0:03:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9791\n",
      "  Recall: 0.9703\n",
      "  F1: 0.9739\n",
      "Category: 1\n",
      "  Precision: 0.2353\n",
      "  Recall: 0.2656\n",
      "  F1: 0.2391\n",
      "The average precision is: 0.6072\n",
      "The average recall is: 0.6180\n",
      "The average f1 is: 0.6065\n",
      "  Validation took: 0:00:05\n",
      "Found new best average F1 score model when validation, old f1 is 0.604116 , new f1 is 0.606488\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    534.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    534.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    534.    Elapsed: 0:00:49.\n",
      "  Batch   160  of    534.    Elapsed: 0:01:57.\n",
      "  Batch   200  of    534.    Elapsed: 0:03:05.\n",
      "  Batch   240  of    534.    Elapsed: 0:04:12.\n",
      "  Batch   280  of    534.    Elapsed: 0:05:19.\n",
      "  Batch   320  of    534.    Elapsed: 0:05:47.\n",
      "  Batch   360  of    534.    Elapsed: 0:06:01.\n",
      "  Batch   400  of    534.    Elapsed: 0:06:14.\n",
      "  Batch   440  of    534.    Elapsed: 0:06:28.\n",
      "  Batch   480  of    534.    Elapsed: 0:06:45.\n",
      "  Batch   520  of    534.    Elapsed: 0:07:27.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9914\n",
      "The average recall is: 0.9938\n",
      "The average f1 is: 0.9920\n",
      "  Training epcoh took: 0:07:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9754\n",
      "  Recall: 0.9929\n",
      "  F1: 0.9836\n",
      "Category: 1\n",
      "  Precision: 0.2253\n",
      "  Recall: 0.1905\n",
      "  F1: 0.2015\n",
      "The average precision is: 0.6003\n",
      "The average recall is: 0.5917\n",
      "The average f1 is: 0.5925\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    534.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    534.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    534.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    534.    Elapsed: 0:00:55.\n",
      "  Batch   200  of    534.    Elapsed: 0:01:08.\n",
      "  Batch   240  of    534.    Elapsed: 0:01:22.\n",
      "  Batch   280  of    534.    Elapsed: 0:01:35.\n",
      "  Batch   320  of    534.    Elapsed: 0:01:49.\n",
      "  Batch   360  of    534.    Elapsed: 0:02:03.\n",
      "  Batch   400  of    534.    Elapsed: 0:02:16.\n",
      "  Batch   440  of    534.    Elapsed: 0:02:30.\n",
      "  Batch   480  of    534.    Elapsed: 0:02:43.\n",
      "  Batch   520  of    534.    Elapsed: 0:02:57.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9973\n",
      "The average recall is: 0.9976\n",
      "The average f1 is: 0.9972\n",
      "  Training epcoh took: 0:03:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9732\n",
      "  Recall: 0.9920\n",
      "  F1: 0.9819\n",
      "Category: 1\n",
      "  Precision: 0.2033\n",
      "  Recall: 0.1877\n",
      "  F1: 0.1875\n",
      "The average precision is: 0.5883\n",
      "The average recall is: 0.5898\n",
      "The average f1 is: 0.5847\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    534.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    534.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    534.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    534.    Elapsed: 0:00:55.\n",
      "  Batch   200  of    534.    Elapsed: 0:01:08.\n",
      "  Batch   240  of    534.    Elapsed: 0:01:22.\n",
      "  Batch   280  of    534.    Elapsed: 0:01:36.\n",
      "  Batch   320  of    534.    Elapsed: 0:01:49.\n",
      "  Batch   360  of    534.    Elapsed: 0:02:03.\n",
      "  Batch   400  of    534.    Elapsed: 0:02:16.\n",
      "  Batch   440  of    534.    Elapsed: 0:02:30.\n",
      "  Batch   480  of    534.    Elapsed: 0:02:44.\n",
      "  Batch   520  of    534.    Elapsed: 0:02:57.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9977\n",
      "The average recall is: 0.9982\n",
      "The average f1 is: 0.9979\n",
      "  Training epcoh took: 0:03:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9724\n",
      "  Recall: 0.9929\n",
      "  F1: 0.9819\n",
      "Category: 1\n",
      "  Precision: 0.1978\n",
      "  Recall: 0.1465\n",
      "  F1: 0.1622\n",
      "The average precision is: 0.5851\n",
      "The average recall is: 0.5697\n",
      "The average f1 is: 0.5721\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 10\n",
      "Current increase 20 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 278 sampled in this category.\n",
      "After up-sample, there are 11322 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5484\n",
      "0.48436671966083733\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5838\n",
      "0.5156332803391627\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    708.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    708.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    708.    Elapsed: 0:00:44.\n",
      "  Batch   160  of    708.    Elapsed: 0:00:59.\n",
      "  Batch   200  of    708.    Elapsed: 0:01:13.\n",
      "  Batch   240  of    708.    Elapsed: 0:01:28.\n",
      "  Batch   280  of    708.    Elapsed: 0:01:42.\n",
      "  Batch   320  of    708.    Elapsed: 0:01:57.\n",
      "  Batch   360  of    708.    Elapsed: 0:02:11.\n",
      "  Batch   400  of    708.    Elapsed: 0:02:26.\n",
      "  Batch   440  of    708.    Elapsed: 0:02:40.\n",
      "  Batch   480  of    708.    Elapsed: 0:02:54.\n",
      "  Batch   520  of    708.    Elapsed: 0:03:09.\n",
      "  Batch   560  of    708.    Elapsed: 0:03:23.\n",
      "  Batch   600  of    708.    Elapsed: 0:03:38.\n",
      "  Batch   640  of    708.    Elapsed: 0:03:52.\n",
      "  Batch   680  of    708.    Elapsed: 0:04:07.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Accuracy: 0.94\n",
      "Category: 0\n",
      "  Precision: 0.95\n",
      "  Recall: 0.92\n",
      "  F1: 0.93\n",
      "Category: 1\n",
      "  Precision: 0.94\n",
      "  Recall: 0.96\n",
      "  F1: 0.95\n",
      "The average precision is: 0.9451\n",
      "The average recall is: 0.9432\n",
      "The average f1 is: 0.9380\n",
      "  Training epcoh took: 0:04:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9652\n",
      "  Recall: 0.9762\n",
      "  F1: 0.9700\n",
      "Category: 1\n",
      "  Precision: 0.2088\n",
      "  Recall: 0.2125\n",
      "  F1: 0.2033\n",
      "The average precision is: 0.5870\n",
      "The average recall is: 0.5943\n",
      "The average f1 is: 0.5866\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    708.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    708.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    708.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    708.    Elapsed: 0:00:58.\n",
      "  Batch   200  of    708.    Elapsed: 0:01:12.\n",
      "  Batch   240  of    708.    Elapsed: 0:01:27.\n",
      "  Batch   280  of    708.    Elapsed: 0:01:41.\n",
      "  Batch   320  of    708.    Elapsed: 0:01:56.\n",
      "  Batch   360  of    708.    Elapsed: 0:02:10.\n",
      "  Batch   400  of    708.    Elapsed: 0:02:24.\n",
      "  Batch   440  of    708.    Elapsed: 0:02:39.\n",
      "  Batch   480  of    708.    Elapsed: 0:02:53.\n",
      "  Batch   520  of    708.    Elapsed: 0:03:08.\n",
      "  Batch   560  of    708.    Elapsed: 0:03:22.\n",
      "  Batch   600  of    708.    Elapsed: 0:03:37.\n",
      "  Batch   640  of    708.    Elapsed: 0:03:51.\n",
      "  Batch   680  of    708.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9964\n",
      "The average recall is: 0.9964\n",
      "The average f1 is: 0.9961\n",
      "  Training epcoh took: 0:04:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9713\n",
      "  Recall: 0.9929\n",
      "  F1: 0.9814\n",
      "Category: 1\n",
      "  Precision: 0.1923\n",
      "  Recall: 0.1621\n",
      "  F1: 0.1692\n",
      "The average precision is: 0.5818\n",
      "The average recall is: 0.5775\n",
      "The average f1 is: 0.5753\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    708.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    708.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    708.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    708.    Elapsed: 0:00:58.\n",
      "  Batch   200  of    708.    Elapsed: 0:01:13.\n",
      "  Batch   240  of    708.    Elapsed: 0:01:27.\n",
      "  Batch   280  of    708.    Elapsed: 0:01:41.\n",
      "  Batch   320  of    708.    Elapsed: 0:01:56.\n",
      "  Batch   360  of    708.    Elapsed: 0:02:10.\n",
      "  Batch   400  of    708.    Elapsed: 0:02:24.\n",
      "  Batch   440  of    708.    Elapsed: 0:02:39.\n",
      "  Batch   480  of    708.    Elapsed: 0:02:53.\n",
      "  Batch   520  of    708.    Elapsed: 0:03:08.\n",
      "  Batch   560  of    708.    Elapsed: 0:03:22.\n",
      "  Batch   600  of    708.    Elapsed: 0:03:37.\n",
      "  Batch   640  of    708.    Elapsed: 0:03:51.\n",
      "  Batch   680  of    708.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9989\n",
      "The average recall is: 0.9989\n",
      "The average f1 is: 0.9988\n",
      "  Training epcoh took: 0:04:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9758\n",
      "  Recall: 0.9886\n",
      "  F1: 0.9817\n",
      "Category: 1\n",
      "  Precision: 0.1996\n",
      "  Recall: 0.1740\n",
      "  F1: 0.1821\n",
      "The average precision is: 0.5877\n",
      "The average recall is: 0.5813\n",
      "The average f1 is: 0.5819\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    708.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    708.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    708.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    708.    Elapsed: 0:00:58.\n",
      "  Batch   200  of    708.    Elapsed: 0:01:12.\n",
      "  Batch   240  of    708.    Elapsed: 0:01:26.\n",
      "  Batch   280  of    708.    Elapsed: 0:01:41.\n",
      "  Batch   320  of    708.    Elapsed: 0:01:55.\n",
      "  Batch   360  of    708.    Elapsed: 0:02:10.\n",
      "  Batch   400  of    708.    Elapsed: 0:02:24.\n",
      "  Batch   440  of    708.    Elapsed: 0:02:38.\n",
      "  Batch   480  of    708.    Elapsed: 0:02:53.\n",
      "  Batch   520  of    708.    Elapsed: 0:03:08.\n",
      "  Batch   560  of    708.    Elapsed: 0:03:22.\n",
      "  Batch   600  of    708.    Elapsed: 0:03:37.\n",
      "  Batch   640  of    708.    Elapsed: 0:03:51.\n",
      "  Batch   680  of    708.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9994\n",
      "The average recall is: 0.9994\n",
      "The average f1 is: 0.9994\n",
      "  Training epcoh took: 0:04:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9742\n",
      "  Recall: 0.9915\n",
      "  F1: 0.9821\n",
      "Category: 1\n",
      "  Precision: 0.2088\n",
      "  Recall: 0.1978\n",
      "  F1: 0.1938\n",
      "The average precision is: 0.5915\n",
      "The average recall is: 0.5947\n",
      "The average f1 is: 0.5879\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Successfully save file ./BertSearchResult/rectified_5_32_3e-05_5train_on_a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing...\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "  Accuracy: 0.98\n",
      "Category: 0\n",
      "  Precision: 0.9946\n",
      "  Recall: 0.9823\n",
      "  F1: 0.9882\n",
      "Category: 1\n",
      "  Precision: 0.6429\n",
      "  Recall: 0.7433\n",
      "  F1: 0.6680\n",
      "The average precision is: 0.8188\n",
      "The average recall is: 0.8628\n",
      "The average f1 is: 0.8281\n",
      "  Testing took: 0:00:25\n",
      "Train on dataset-b and test on dataset-a\n",
      "Train on dataset-b and test on dataset-a\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current increase 0 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 121 sampled in this category.\n",
      "After up-sample, there are 3423 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3302\n",
      "0.9646508910312591\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "121\n",
      "0.03534910896874087\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:23.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 1.36\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.97\n",
      "  Recall: 1.00\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.01\n",
      "  Recall: 0.01\n",
      "  F1: 0.01\n",
      "The average precision is: 0.4898\n",
      "The average recall is: 0.5067\n",
      "The average f1 is: 0.4978\n",
      "  Training epcoh took: 0:00:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9641\n",
      "  Recall: 1.0000\n",
      "  F1: 0.9812\n",
      "Category: 1\n",
      "  Precision: 0.0556\n",
      "  Recall: 0.0556\n",
      "  F1: 0.0556\n",
      "The average precision is: 0.5098\n",
      "The average recall is: 0.5278\n",
      "The average f1 is: 0.5184\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.518399\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:56.\n",
      "\n",
      "  Average training loss: 0.75\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.21\n",
      "  Recall: 0.21\n",
      "  F1: 0.20\n",
      "The average precision is: 0.5959\n",
      "The average recall is: 0.5967\n",
      "The average f1 is: 0.5930\n",
      "  Training epcoh took: 0:00:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9790\n",
      "  Recall: 0.9931\n",
      "  F1: 0.9854\n",
      "Category: 1\n",
      "  Precision: 0.2593\n",
      "  Recall: 0.2469\n",
      "  F1: 0.2500\n",
      "The average precision is: 0.6191\n",
      "The average recall is: 0.6200\n",
      "The average f1 is: 0.6177\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.617708\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Accuracy: 0.98\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.33\n",
      "  Recall: 0.32\n",
      "  F1: 0.32\n",
      "The average precision is: 0.6587\n",
      "The average recall is: 0.6574\n",
      "The average f1 is: 0.6554\n",
      "  Training epcoh took: 0:00:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9741\n",
      "  Recall: 0.9987\n",
      "  F1: 0.9860\n",
      "Category: 1\n",
      "  Precision: 0.2130\n",
      "  Recall: 0.1852\n",
      "  F1: 0.1944\n",
      "The average precision is: 0.5936\n",
      "The average recall is: 0.5919\n",
      "The average f1 is: 0.5902\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:55.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.38\n",
      "  Recall: 0.39\n",
      "  F1: 0.38\n",
      "The average precision is: 0.6882\n",
      "The average recall is: 0.6908\n",
      "The average f1 is: 0.6879\n",
      "  Training epcoh took: 0:00:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9833\n",
      "  Recall: 0.9832\n",
      "  F1: 0.9827\n",
      "Category: 1\n",
      "  Precision: 0.2562\n",
      "  Recall: 0.2685\n",
      "  F1: 0.2562\n",
      "The average precision is: 0.6197\n",
      "The average recall is: 0.6258\n",
      "The average f1 is: 0.6194\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.619430\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 0\n",
      "Current increase 10 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 121 sampled in this category.\n",
      "After up-sample, there are 4633 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3302\n",
      "0.7127131448305634\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "1331\n",
      "0.28728685516943664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    290.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    290.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    290.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    290.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    290.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    290.    Elapsed: 0:01:20.\n",
      "  Batch   280  of    290.    Elapsed: 0:01:34.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Accuracy: 0.94\n",
      "Category: 0\n",
      "  Precision: 0.96\n",
      "  Recall: 0.96\n",
      "  F1: 0.96\n",
      "Category: 1\n",
      "  Precision: 0.87\n",
      "  Recall: 0.88\n",
      "  F1: 0.86\n",
      "The average precision is: 0.9134\n",
      "The average recall is: 0.9193\n",
      "The average f1 is: 0.9081\n",
      "  Training epcoh took: 0:01:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "Category: 0\n",
      "  Precision: 0.9790\n",
      "  Recall: 0.9965\n",
      "  F1: 0.9873\n",
      "Category: 1\n",
      "  Precision: 0.2222\n",
      "  Recall: 0.2037\n",
      "  F1: 0.2099\n",
      "The average precision is: 0.6006\n",
      "The average recall is: 0.6001\n",
      "The average f1 is: 0.5986\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    290.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    290.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    290.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    290.    Elapsed: 0:00:53.\n",
      "  Batch   200  of    290.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    290.    Elapsed: 0:01:20.\n",
      "  Batch   280  of    290.    Elapsed: 0:01:33.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.97\n",
      "  Recall: 0.98\n",
      "  F1: 0.97\n",
      "The average precision is: 0.9827\n",
      "The average recall is: 0.9844\n",
      "The average f1 is: 0.9819\n",
      "  Training epcoh took: 0:01:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9833\n",
      "  Recall: 0.9903\n",
      "  F1: 0.9863\n",
      "Category: 1\n",
      "  Precision: 0.2809\n",
      "  Recall: 0.2840\n",
      "  F1: 0.2704\n",
      "The average precision is: 0.6321\n",
      "The average recall is: 0.6371\n",
      "The average f1 is: 0.6283\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.619430 , new f1 is 0.628336\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    290.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    290.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    290.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    290.    Elapsed: 0:00:53.\n",
      "  Batch   200  of    290.    Elapsed: 0:01:06.\n",
      "  Batch   240  of    290.    Elapsed: 0:01:20.\n",
      "  Batch   280  of    290.    Elapsed: 0:01:33.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.97\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "The average precision is: 0.9864\n",
      "The average recall is: 0.9914\n",
      "The average f1 is: 0.9883\n",
      "  Training epcoh took: 0:01:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9820\n",
      "  Recall: 0.9915\n",
      "  F1: 0.9865\n",
      "Category: 1\n",
      "  Precision: 0.3056\n",
      "  Recall: 0.2778\n",
      "  F1: 0.2840\n",
      "The average precision is: 0.6438\n",
      "The average recall is: 0.6346\n",
      "The average f1 is: 0.6352\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.619430 , new f1 is 0.635222\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    290.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    290.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    290.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    290.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    290.    Elapsed: 0:01:26.\n",
      "  Batch   240  of    290.    Elapsed: 0:01:42.\n",
      "  Batch   280  of    290.    Elapsed: 0:01:55.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "The average precision is: 0.9895\n",
      "The average recall is: 0.9917\n",
      "The average f1 is: 0.9902\n",
      "  Training epcoh took: 0:01:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "Category: 0\n",
      "  Precision: 0.9809\n",
      "  Recall: 0.9941\n",
      "  F1: 0.9871\n",
      "Category: 1\n",
      "  Precision: 0.2963\n",
      "  Recall: 0.2438\n",
      "  F1: 0.2617\n",
      "The average precision is: 0.6386\n",
      "The average recall is: 0.6190\n",
      "The average f1 is: 0.6244\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 10\n",
      "Current increase 20 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 121 sampled in this category.\n",
      "After up-sample, there are 5843 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3302\n",
      "0.5651206571966455\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "2541\n",
      "0.43487934280335444\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    366.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    366.    Elapsed: 0:00:28.\n",
      "  Batch   120  of    366.    Elapsed: 0:00:42.\n",
      "  Batch   160  of    366.    Elapsed: 0:00:57.\n",
      "  Batch   200  of    366.    Elapsed: 0:01:11.\n",
      "  Batch   240  of    366.    Elapsed: 0:01:25.\n",
      "  Batch   280  of    366.    Elapsed: 0:01:39.\n",
      "  Batch   320  of    366.    Elapsed: 0:01:58.\n",
      "  Batch   360  of    366.    Elapsed: 0:02:24.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Accuracy: 0.94\n",
      "Category: 0\n",
      "  Precision: 0.96\n",
      "  Recall: 0.94\n",
      "  F1: 0.94\n",
      "Category: 1\n",
      "  Precision: 0.92\n",
      "  Recall: 0.95\n",
      "  F1: 0.92\n",
      "The average precision is: 0.9403\n",
      "The average recall is: 0.9420\n",
      "The average f1 is: 0.9333\n",
      "  Training epcoh took: 0:02:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9750\n",
      "  Recall: 0.9913\n",
      "  F1: 0.9826\n",
      "Category: 1\n",
      "  Precision: 0.1790\n",
      "  Recall: 0.1605\n",
      "  F1: 0.1636\n",
      "The average precision is: 0.5770\n",
      "The average recall is: 0.5759\n",
      "The average f1 is: 0.5731\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    366.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    366.    Elapsed: 0:00:28.\n",
      "  Batch   120  of    366.    Elapsed: 0:00:42.\n",
      "  Batch   160  of    366.    Elapsed: 0:00:56.\n",
      "  Batch   200  of    366.    Elapsed: 0:01:10.\n",
      "  Batch   240  of    366.    Elapsed: 0:01:24.\n",
      "  Batch   280  of    366.    Elapsed: 0:01:38.\n",
      "  Batch   320  of    366.    Elapsed: 0:01:52.\n",
      "  Batch   360  of    366.    Elapsed: 0:02:06.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9904\n",
      "The average recall is: 0.9921\n",
      "The average f1 is: 0.9906\n",
      "  Training epcoh took: 0:02:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9785\n",
      "  Recall: 0.9941\n",
      "  F1: 0.9856\n",
      "Category: 1\n",
      "  Precision: 0.2685\n",
      "  Recall: 0.2160\n",
      "  F1: 0.2309\n",
      "The average precision is: 0.6235\n",
      "The average recall is: 0.6051\n",
      "The average f1 is: 0.6082\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    366.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    366.    Elapsed: 0:00:28.\n",
      "  Batch   120  of    366.    Elapsed: 0:00:42.\n",
      "  Batch   160  of    366.    Elapsed: 0:00:56.\n",
      "  Batch   200  of    366.    Elapsed: 0:01:10.\n",
      "  Batch   240  of    366.    Elapsed: 0:01:24.\n",
      "  Batch   280  of    366.    Elapsed: 0:01:38.\n",
      "  Batch   240  of    366.    Elapsed: 0:01:25.\n",
      "  Batch   280  of    366.    Elapsed: 0:01:39.\n",
      "  Batch   320  of    366.    Elapsed: 0:01:53.\n",
      "  Batch   360  of    366.    Elapsed: 0:02:07.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9973\n",
      "The average recall is: 0.9975\n",
      "The average f1 is: 0.9972\n",
      "  Training epcoh took: 0:02:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.9764\n",
      "  Recall: 0.9965\n",
      "  F1: 0.9857\n",
      "Category: 1\n",
      "  Precision: 0.2315\n",
      "  Recall: 0.2052\n",
      "  F1: 0.2080\n",
      "The average precision is: 0.6039\n",
      "The average recall is: 0.6008\n",
      "The average f1 is: 0.5969\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Successfully save file ./BertSearchResult/rectified_5_32_3e-05_5train_on_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing...\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.9952\n",
      "  Recall: 0.9945\n",
      "  F1: 0.9947\n",
      "Category: 1\n",
      "  Precision: 0.6327\n",
      "  Recall: 0.6424\n",
      "  F1: 0.6230\n",
      "The average precision is: 0.8140\n",
      "The average recall is: 0.8185\n",
      "The average f1 is: 0.8089\n",
      "  Testing took: 0:00:15\n"
     ]
    }
   ],
   "source": [
    "section_category = 5\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSWDRnmrJXkm"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZdUHHRoPZirA",
    "outputId": "7156f0ab-6c8e-4ea4-9b68-849e5642e802",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "y3lqlmMyWeDi",
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBhLVv1YJR3O"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD_vdAnMZirB",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-13a961ab48c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_masks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mBert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertnn_rectified_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mall_metrics_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_tesst_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_rectified_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "bKG-GPVXZirE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pjh_EA9rWTqR"
   },
   "source": [
    "### For 'Advice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGbqnJyhWejl"
   },
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vl3z0MGWejl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on dataset-a and test on dataset-b\n",
      "Train on dataset-a and test on dataset-b\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current increase 0 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 443 sampled in this category.\n",
      "After up-sample, there are 5762 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5319\n",
      "0.923116973273169\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "443\n",
      "0.07688302672683096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:24.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:36.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:48.\n",
      "  Batch   200  of    361.    Elapsed: 0:01:00.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:11.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:23.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:35.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:47.\n",
      "\n",
      "  Average training loss: 1.86\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.92\n",
      "  Recall: 1.00\n",
      "  F1: 0.96\n",
      "Category: 1\n",
      "  Precision: 0.00\n",
      "  Recall: 0.00\n",
      "  F1: 0.00\n",
      "The average precision is: 0.4619\n",
      "The average recall is: 0.5008\n",
      "The average f1 is: 0.4794\n",
      "  Training epcoh took: 0:01:47\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.9224\n",
      "  Recall: 1.0000\n",
      "  F1: 0.9583\n",
      "Category: 1\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1: 0.0000\n",
      "The average precision is: 0.4612\n",
      "The average recall is: 0.5000\n",
      "The average f1 is: 0.4792\n",
      "  Validation took: 0:00:05\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.479163\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:25.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:37.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:49.\n",
      "  Batch   200  of    361.    Elapsed: 0:01:00.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:12.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:24.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:36.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:48.\n",
      "\n",
      "  Average training loss: 1.63\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.93\n",
      "  Recall: 0.99\n",
      "  F1: 0.96\n",
      "Category: 1\n",
      "  Precision: 0.05\n",
      "  Recall: 0.05\n",
      "  F1: 0.04\n",
      "The average precision is: 0.4890\n",
      "The average recall is: 0.5211\n",
      "The average f1 is: 0.5012\n",
      "  Training epcoh took: 0:01:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.9224\n",
      "  Recall: 1.0000\n",
      "  F1: 0.9585\n",
      "Category: 1\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1: 0.0000\n",
      "The average precision is: 0.4612\n",
      "The average recall is: 0.5000\n",
      "The average f1 is: 0.4793\n",
      "  Validation took: 0:00:06\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.479263\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:24.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:36.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:47.\n",
      "  Batch   200  of    361.    Elapsed: 0:00:59.\n",
      "  Batch   240  of    361.    Elapsed: 0:01:11.\n",
      "  Batch   280  of    361.    Elapsed: 0:01:23.\n",
      "  Batch   320  of    361.    Elapsed: 0:01:35.\n",
      "  Batch   360  of    361.    Elapsed: 0:01:47.\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.96\n",
      "  Recall: 0.99\n",
      "  F1: 0.97\n",
      "Category: 1\n",
      "  Precision: 0.36\n",
      "  Recall: 0.31\n",
      "  F1: 0.32\n",
      "The average precision is: 0.6585\n",
      "The average recall is: 0.6502\n",
      "The average f1 is: 0.6450\n",
      "  Training epcoh took: 0:01:47\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.9433\n",
      "  Recall: 0.9830\n",
      "  F1: 0.9616\n",
      "Category: 1\n",
      "  Precision: 0.2949\n",
      "  Recall: 0.2463\n",
      "  F1: 0.2542\n",
      "The average precision is: 0.6191\n",
      "The average recall is: 0.6147\n",
      "The average f1 is: 0.6079\n",
      "  Validation took: 0:00:06\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.607898\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    361.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    361.    Elapsed: 0:00:24.\n",
      "  Batch   120  of    361.    Elapsed: 0:00:36.\n",
      "  Batch   160  of    361.    Elapsed: 0:00:49.\n",
      "  Batch   200  of    361.    Elapsed: 0:01:41.\n",
      "  Batch   240  of    361.    Elapsed: 0:02:04.\n",
      "  Batch   280  of    361.    Elapsed: 0:02:15.\n",
      "  Batch   320  of    361.    Elapsed: 0:02:27.\n",
      "  Batch   360  of    361.    Elapsed: 0:02:39.\n",
      "\n",
      "  Average training loss: 0.72\n",
      "  Accuracy: 0.97\n",
      "Category: 0\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.59\n",
      "  Recall: 0.54\n",
      "  F1: 0.55\n",
      "The average precision is: 0.7864\n",
      "The average recall is: 0.7644\n",
      "The average f1 is: 0.7675\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.9469\n",
      "  Recall: 0.9672\n",
      "  F1: 0.9556\n",
      "Category: 1\n",
      "  Precision: 0.2921\n",
      "  Recall: 0.2705\n",
      "  F1: 0.2581\n",
      "The average precision is: 0.6195\n",
      "The average recall is: 0.6188\n",
      "The average f1 is: 0.6068\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 0\n",
      "Current increase 10 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 443 sampled in this category.\n",
      "After up-sample, there are 10192 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5319\n",
      "0.5218799058084772\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "4873\n",
      "0.47812009419152274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    637.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    637.    Elapsed: 0:00:33.\n",
      "  Batch   120  of    637.    Elapsed: 0:00:47.\n",
      "  Batch   160  of    637.    Elapsed: 0:01:02.\n",
      "  Batch   200  of    637.    Elapsed: 0:01:17.\n",
      "  Batch   240  of    637.    Elapsed: 0:01:31.\n",
      "  Batch   280  of    637.    Elapsed: 0:01:46.\n",
      "  Batch   320  of    637.    Elapsed: 0:02:00.\n",
      "  Batch   360  of    637.    Elapsed: 0:02:15.\n",
      "  Batch   400  of    637.    Elapsed: 0:02:29.\n",
      "  Batch   440  of    637.    Elapsed: 0:02:43.\n",
      "  Batch   480  of    637.    Elapsed: 0:03:44.\n",
      "  Batch   520  of    637.    Elapsed: 0:03:59.\n",
      "  Batch   560  of    637.    Elapsed: 0:04:13.\n",
      "  Batch   600  of    637.    Elapsed: 0:04:27.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Accuracy: 0.87\n",
      "Category: 0\n",
      "  Precision: 0.88\n",
      "  Recall: 0.86\n",
      "  F1: 0.86\n",
      "Category: 1\n",
      "  Precision: 0.86\n",
      "  Recall: 0.88\n",
      "  F1: 0.86\n",
      "The average precision is: 0.8716\n",
      "The average recall is: 0.8706\n",
      "The average f1 is: 0.8597\n",
      "  Training epcoh took: 0:04:41\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "Category: 0\n",
      "  Precision: 0.9342\n",
      "  Recall: 0.9529\n",
      "  F1: 0.9421\n",
      "Category: 1\n",
      "  Precision: 0.2866\n",
      "  Recall: 0.2381\n",
      "  F1: 0.2421\n",
      "The average precision is: 0.6104\n",
      "The average recall is: 0.5955\n",
      "The average f1 is: 0.5921\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    637.    Elapsed: 0:00:16.\n",
      "  Batch    80  of    637.    Elapsed: 0:00:31.\n",
      "  Batch   120  of    637.    Elapsed: 0:00:45.\n",
      "  Batch   160  of    637.    Elapsed: 0:00:59.\n",
      "  Batch   200  of    637.    Elapsed: 0:01:14.\n",
      "  Batch   240  of    637.    Elapsed: 0:01:28.\n",
      "  Batch   280  of    637.    Elapsed: 0:01:55.\n",
      "  Batch   320  of    637.    Elapsed: 0:02:26.\n",
      "  Batch   360  of    637.    Elapsed: 0:02:40.\n",
      "  Batch   400  of    637.    Elapsed: 0:02:54.\n",
      "  Batch   440  of    637.    Elapsed: 0:03:08.\n",
      "  Batch   480  of    637.    Elapsed: 0:03:23.\n",
      "  Batch   520  of    637.    Elapsed: 0:03:37.\n",
      "  Batch   560  of    637.    Elapsed: 0:04:01.\n",
      "  Batch   600  of    637.    Elapsed: 0:04:16.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 0.98\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9881\n",
      "The average recall is: 0.9888\n",
      "The average f1 is: 0.9876\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "Category: 0\n",
      "  Precision: 0.9410\n",
      "  Recall: 0.9778\n",
      "  F1: 0.9575\n",
      "Category: 1\n",
      "  Precision: 0.2289\n",
      "  Recall: 0.2042\n",
      "  F1: 0.2004\n",
      "The average precision is: 0.5850\n",
      "The average recall is: 0.5910\n",
      "The average f1 is: 0.5790\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    637.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    637.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    637.    Elapsed: 0:01:04.\n",
      "  Batch   160  of    637.    Elapsed: 0:01:36.\n",
      "  Batch   200  of    637.    Elapsed: 0:01:50.\n",
      "  Batch   240  of    637.    Elapsed: 0:02:04.\n",
      "  Batch   280  of    637.    Elapsed: 0:02:19.\n",
      "  Batch   320  of    637.    Elapsed: 0:02:38.\n",
      "  Batch   360  of    637.    Elapsed: 0:02:52.\n",
      "  Batch   400  of    637.    Elapsed: 0:03:06.\n",
      "  Batch   440  of    637.    Elapsed: 0:03:21.\n",
      "  Batch   480  of    637.    Elapsed: 0:03:35.\n",
      "  Batch   520  of    637.    Elapsed: 0:03:49.\n",
      "  Batch   560  of    637.    Elapsed: 0:04:04.\n",
      "  Batch   600  of    637.    Elapsed: 0:04:42.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9949\n",
      "The average recall is: 0.9957\n",
      "The average f1 is: 0.9949\n",
      "  Training epcoh took: 0:05:34\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.9449\n",
      "  Recall: 0.9772\n",
      "  F1: 0.9597\n",
      "Category: 1\n",
      "  Precision: 0.2766\n",
      "  Recall: 0.2289\n",
      "  F1: 0.2403\n",
      "The average precision is: 0.6107\n",
      "The average recall is: 0.6031\n",
      "The average f1 is: 0.6000\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    637.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    637.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    637.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    637.    Elapsed: 0:00:57.\n",
      "  Batch   200  of    637.    Elapsed: 0:01:18.\n",
      "  Batch   240  of    637.    Elapsed: 0:01:32.\n",
      "  Batch   280  of    637.    Elapsed: 0:01:46.\n",
      "  Batch   320  of    637.    Elapsed: 0:02:01.\n",
      "  Batch   360  of    637.    Elapsed: 0:02:15.\n",
      "  Batch   400  of    637.    Elapsed: 0:02:29.\n",
      "  Batch   440  of    637.    Elapsed: 0:03:03.\n",
      "  Batch   480  of    637.    Elapsed: 0:03:49.\n",
      "  Batch   520  of    637.    Elapsed: 0:04:03.\n",
      "  Batch   560  of    637.    Elapsed: 0:04:17.\n",
      "  Batch   600  of    637.    Elapsed: 0:04:32.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9980\n",
      "The average recall is: 0.9982\n",
      "The average f1 is: 0.9979\n",
      "  Training epcoh took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.9403\n",
      "  Recall: 0.9848\n",
      "  F1: 0.9608\n",
      "Category: 1\n",
      "  Precision: 0.2747\n",
      "  Recall: 0.1758\n",
      "  F1: 0.1974\n",
      "The average precision is: 0.6075\n",
      "The average recall is: 0.5803\n",
      "The average f1 is: 0.5791\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Current increase 20 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 5762 samples\n",
      "There are 443 sampled in this category.\n",
      "After up-sample, there are 14622 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "5319\n",
      "0.3637669265490357\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "9303\n",
      "0.6362330734509642\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    914.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    914.    Elapsed: 0:00:31.\n",
      "  Batch   120  of    914.    Elapsed: 0:00:46.\n",
      "  Batch   160  of    914.    Elapsed: 0:01:02.\n",
      "  Batch   200  of    914.    Elapsed: 0:01:18.\n",
      "  Batch   240  of    914.    Elapsed: 0:01:52.\n",
      "  Batch   280  of    914.    Elapsed: 0:02:49.\n",
      "  Batch   320  of    914.    Elapsed: 0:03:05.\n",
      "  Batch   360  of    914.    Elapsed: 0:03:20.\n",
      "  Batch   400  of    914.    Elapsed: 0:03:35.\n",
      "  Batch   440  of    914.    Elapsed: 0:03:51.\n",
      "  Batch   480  of    914.    Elapsed: 0:04:15.\n",
      "  Batch   520  of    914.    Elapsed: 0:04:30.\n",
      "  Batch   560  of    914.    Elapsed: 0:04:45.\n",
      "  Batch   600  of    914.    Elapsed: 0:05:00.\n",
      "  Batch   640  of    914.    Elapsed: 0:05:16.\n",
      "  Batch   680  of    914.    Elapsed: 0:05:31.\n",
      "  Batch   720  of    914.    Elapsed: 0:06:09.\n",
      "  Batch   760  of    914.    Elapsed: 0:06:55.\n",
      "  Batch   800  of    914.    Elapsed: 0:07:11.\n",
      "  Batch   840  of    914.    Elapsed: 0:07:26.\n",
      "  Batch   880  of    914.    Elapsed: 0:07:41.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Accuracy: 0.91\n",
      "Category: 0\n",
      "  Precision: 0.87\n",
      "  Recall: 0.80\n",
      "  F1: 0.82\n",
      "Category: 1\n",
      "  Precision: 0.91\n",
      "  Recall: 0.97\n",
      "  F1: 0.93\n",
      "The average precision is: 0.8897\n",
      "The average recall is: 0.8837\n",
      "The average f1 is: 0.8759\n",
      "  Training epcoh took: 0:07:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "Category: 0\n",
      "  Precision: 0.9578\n",
      "  Recall: 0.8807\n",
      "  F1: 0.9146\n",
      "Category: 1\n",
      "  Precision: 0.2450\n",
      "  Recall: 0.3844\n",
      "  F1: 0.2777\n",
      "The average precision is: 0.6014\n",
      "The average recall is: 0.6326\n",
      "The average f1 is: 0.5961\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    914.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    914.    Elapsed: 0:00:36.\n",
      "  Batch   120  of    914.    Elapsed: 0:00:52.\n",
      "  Batch   160  of    914.    Elapsed: 0:01:07.\n",
      "  Batch   200  of    914.    Elapsed: 0:01:22.\n",
      "  Batch   240  of    914.    Elapsed: 0:01:37.\n",
      "  Batch   280  of    914.    Elapsed: 0:01:53.\n",
      "  Batch   320  of    914.    Elapsed: 0:02:08.\n",
      "  Batch   360  of    914.    Elapsed: 0:02:23.\n",
      "  Batch   400  of    914.    Elapsed: 0:02:38.\n",
      "  Batch   440  of    914.    Elapsed: 0:02:53.\n",
      "  Batch   480  of    914.    Elapsed: 0:03:08.\n",
      "  Batch   520  of    914.    Elapsed: 0:03:24.\n",
      "  Batch   560  of    914.    Elapsed: 0:03:39.\n",
      "  Batch   600  of    914.    Elapsed: 0:04:37.\n",
      "  Batch   640  of    914.    Elapsed: 0:05:32.\n",
      "  Batch   680  of    914.    Elapsed: 0:05:47.\n",
      "  Batch   720  of    914.    Elapsed: 0:06:02.\n",
      "  Batch   760  of    914.    Elapsed: 0:06:17.\n",
      "  Batch   800  of    914.    Elapsed: 0:06:33.\n",
      "  Batch   840  of    914.    Elapsed: 0:06:48.\n",
      "  Batch   880  of    914.    Elapsed: 0:07:03.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 0.98\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9909\n",
      "The average recall is: 0.9889\n",
      "The average f1 is: 0.9891\n",
      "  Training epcoh took: 0:07:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "Category: 0\n",
      "  Precision: 0.9490\n",
      "  Recall: 0.9533\n",
      "  F1: 0.9499\n",
      "Category: 1\n",
      "  Precision: 0.2802\n",
      "  Recall: 0.2648\n",
      "  F1: 0.2582\n",
      "The average precision is: 0.6146\n",
      "The average recall is: 0.6091\n",
      "The average f1 is: 0.6040\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    914.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    914.    Elapsed: 0:00:30.\n",
      "  Batch   120  of    914.    Elapsed: 0:00:46.\n",
      "  Batch   160  of    914.    Elapsed: 0:01:01.\n",
      "  Batch   200  of    914.    Elapsed: 0:01:16.\n",
      "  Batch   240  of    914.    Elapsed: 0:01:32.\n",
      "  Batch   280  of    914.    Elapsed: 0:01:47.\n",
      "  Batch   320  of    914.    Elapsed: 0:02:02.\n",
      "  Batch   360  of    914.    Elapsed: 0:02:17.\n",
      "  Batch   400  of    914.    Elapsed: 0:02:32.\n",
      "  Batch   440  of    914.    Elapsed: 0:02:47.\n",
      "  Batch   480  of    914.    Elapsed: 0:03:02.\n",
      "  Batch   520  of    914.    Elapsed: 0:03:18.\n",
      "  Batch   560  of    914.    Elapsed: 0:03:33.\n",
      "  Batch   600  of    914.    Elapsed: 0:03:48.\n",
      "  Batch   640  of    914.    Elapsed: 0:04:03.\n",
      "  Batch   680  of    914.    Elapsed: 0:04:19.\n",
      "  Batch   720  of    914.    Elapsed: 0:04:34.\n",
      "  Batch   760  of    914.    Elapsed: 0:04:49.\n",
      "  Batch   800  of    914.    Elapsed: 0:05:04.\n",
      "  Batch   840  of    914.    Elapsed: 0:05:20.\n",
      "  Batch   880  of    914.    Elapsed: 0:05:35.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9969\n",
      "The average recall is: 0.9961\n",
      "The average f1 is: 0.9963\n",
      "  Training epcoh took: 0:05:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.9408\n",
      "  Recall: 0.9850\n",
      "  F1: 0.9615\n",
      "Category: 1\n",
      "  Precision: 0.2527\n",
      "  Recall: 0.1610\n",
      "  F1: 0.1889\n",
      "The average precision is: 0.5968\n",
      "The average recall is: 0.5730\n",
      "The average f1 is: 0.5752\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    914.    Elapsed: 0:00:15.\n",
      "  Batch    80  of    914.    Elapsed: 0:00:30.\n",
      "  Batch   120  of    914.    Elapsed: 0:00:45.\n",
      "  Batch   160  of    914.    Elapsed: 0:01:01.\n",
      "  Batch   200  of    914.    Elapsed: 0:01:16.\n",
      "  Batch   240  of    914.    Elapsed: 0:01:32.\n",
      "  Batch   280  of    914.    Elapsed: 0:01:47.\n",
      "  Batch   320  of    914.    Elapsed: 0:02:02.\n",
      "  Batch   360  of    914.    Elapsed: 0:02:18.\n",
      "  Batch   400  of    914.    Elapsed: 0:02:33.\n",
      "  Batch   440  of    914.    Elapsed: 0:02:48.\n",
      "  Batch   480  of    914.    Elapsed: 0:03:03.\n",
      "  Batch   520  of    914.    Elapsed: 0:03:18.\n",
      "  Batch   560  of    914.    Elapsed: 0:03:33.\n",
      "  Batch   600  of    914.    Elapsed: 0:03:48.\n",
      "  Batch   640  of    914.    Elapsed: 0:04:04.\n",
      "  Batch   680  of    914.    Elapsed: 0:04:19.\n",
      "  Batch   720  of    914.    Elapsed: 0:04:34.\n",
      "  Batch   760  of    914.    Elapsed: 0:04:49.\n",
      "  Batch   800  of    914.    Elapsed: 0:05:04.\n",
      "  Batch   840  of    914.    Elapsed: 0:05:20.\n",
      "  Batch   880  of    914.    Elapsed: 0:05:34.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9990\n",
      "The average recall is: 0.9986\n",
      "The average f1 is: 0.9987\n",
      "  Training epcoh took: 0:05:47\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.9453\n",
      "  Recall: 0.9867\n",
      "  F1: 0.9646\n",
      "Category: 1\n",
      "  Precision: 0.2802\n",
      "  Recall: 0.2216\n",
      "  F1: 0.2337\n",
      "The average precision is: 0.6128\n",
      "The average recall is: 0.6042\n",
      "The average f1 is: 0.5992\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Successfully save file ./BertSearchResult/rectified_6_32_3e-05_6train_on_a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing...\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9695\n",
      "  Recall: 0.9911\n",
      "  F1: 0.9798\n",
      "Category: 1\n",
      "  Precision: 0.7371\n",
      "  Recall: 0.5865\n",
      "  F1: 0.6265\n",
      "The average precision is: 0.8533\n",
      "The average recall is: 0.7888\n",
      "The average f1 is: 0.8031\n",
      "  Testing took: 0:00:25\n",
      "Train on dataset-b and test on dataset-a\n",
      "Train on dataset-b and test on dataset-a\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current increase 0 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 148 sampled in this category.\n",
      "After up-sample, there are 3423 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3275\n",
      "0.9567630733274906\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "148\n",
      "0.043236926672509494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:57.\n",
      "\n",
      "  Average training loss: 1.49\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.96\n",
      "  Recall: 1.00\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.01\n",
      "  Recall: 0.01\n",
      "  F1: 0.01\n",
      "The average precision is: 0.4822\n",
      "The average recall is: 0.5030\n",
      "The average f1 is: 0.4918\n",
      "  Training epcoh took: 0:01:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9549\n",
      "  Recall: 1.0000\n",
      "  F1: 0.9763\n",
      "Category: 1\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1: 0.0000\n",
      "The average precision is: 0.4774\n",
      "The average recall is: 0.5000\n",
      "The average f1 is: 0.4882\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.488172\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    214.    Elapsed: 0:00:56.\n",
      "\n",
      "  Average training loss: 0.91\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.97\n",
      "  Recall: 0.99\n",
      "  F1: 0.98\n",
      "Category: 1\n",
      "  Precision: 0.19\n",
      "  Recall: 0.17\n",
      "  F1: 0.18\n",
      "The average precision is: 0.5819\n",
      "The average recall is: 0.5833\n",
      "The average f1 is: 0.5785\n",
      "  Training epcoh took: 0:01:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9636\n",
      "  Recall: 0.9965\n",
      "  F1: 0.9791\n",
      "Category: 1\n",
      "  Precision: 0.1296\n",
      "  Recall: 0.0988\n",
      "  F1: 0.1074\n",
      "The average precision is: 0.5466\n",
      "The average recall is: 0.5476\n",
      "The average f1 is: 0.5433\n",
      "  Validation took: 0:00:03\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.543257\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    214.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    214.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    214.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    214.    Elapsed: 0:01:04.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.38\n",
      "  Recall: 0.36\n",
      "  F1: 0.37\n",
      "The average precision is: 0.6870\n",
      "The average recall is: 0.6791\n",
      "The average f1 is: 0.6799\n",
      "  Training epcoh took: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9686\n",
      "  Recall: 0.9901\n",
      "  F1: 0.9787\n",
      "Category: 1\n",
      "  Precision: 0.2222\n",
      "  Recall: 0.1759\n",
      "  F1: 0.1914\n",
      "The average precision is: 0.5954\n",
      "The average recall is: 0.5830\n",
      "The average f1 is: 0.5850\n",
      "  Validation took: 0:00:07\n",
      "Found new best average F1 score model when validation, old f1 is 0.000000 , new f1 is 0.585006\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    214.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    214.    Elapsed: 0:01:33.\n",
      "  Batch   120  of    214.    Elapsed: 0:02:18.\n",
      "  Batch   160  of    214.    Elapsed: 0:03:10.\n",
      "  Batch   200  of    214.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.51\n",
      "  Recall: 0.50\n",
      "  F1: 0.50\n",
      "The average precision is: 0.7523\n",
      "The average recall is: 0.7491\n",
      "The average f1 is: 0.7490\n",
      "  Training epcoh took: 0:04:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9679\n",
      "  Recall: 0.9843\n",
      "  F1: 0.9753\n",
      "Category: 1\n",
      "  Precision: 0.1759\n",
      "  Recall: 0.1759\n",
      "  F1: 0.1667\n",
      "The average precision is: 0.5719\n",
      "The average recall is: 0.5801\n",
      "The average f1 is: 0.5710\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n",
      "Best increase factor is 0\n",
      "Current increase 10 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 148 sampled in this category.\n",
      "After up-sample, there are 4903 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3275\n",
      "0.667958392820722\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "1628\n",
      "0.332041607179278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    307.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    307.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    307.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    307.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    307.    Elapsed: 0:01:34.\n",
      "  Batch   240  of    307.    Elapsed: 0:01:54.\n",
      "  Batch   280  of    307.    Elapsed: 0:02:08.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Accuracy: 0.93\n",
      "Category: 0\n",
      "  Precision: 0.95\n",
      "  Recall: 0.95\n",
      "  F1: 0.95\n",
      "Category: 1\n",
      "  Precision: 0.90\n",
      "  Recall: 0.90\n",
      "  F1: 0.89\n",
      "The average precision is: 0.9242\n",
      "The average recall is: 0.9258\n",
      "The average f1 is: 0.9155\n",
      "  Training epcoh took: 0:02:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9646\n",
      "  Recall: 0.9856\n",
      "  F1: 0.9739\n",
      "Category: 1\n",
      "  Precision: 0.1451\n",
      "  Recall: 0.1235\n",
      "  F1: 0.1204\n",
      "The average precision is: 0.5548\n",
      "The average recall is: 0.5546\n",
      "The average f1 is: 0.5471\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    307.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    307.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    307.    Elapsed: 0:00:40.\n",
      "  Batch   160  of    307.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    307.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    307.    Elapsed: 0:01:21.\n",
      "  Batch   280  of    307.    Elapsed: 0:01:34.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9919\n",
      "The average recall is: 0.9943\n",
      "The average f1 is: 0.9924\n",
      "  Training epcoh took: 0:01:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9659\n",
      "  Recall: 0.9881\n",
      "  F1: 0.9758\n",
      "Category: 1\n",
      "  Precision: 0.1605\n",
      "  Recall: 0.1481\n",
      "  F1: 0.1512\n",
      "The average precision is: 0.5632\n",
      "The average recall is: 0.5681\n",
      "The average f1 is: 0.5635\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    307.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    307.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    307.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    307.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    307.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    307.    Elapsed: 0:01:21.\n",
      "  Batch   280  of    307.    Elapsed: 0:01:34.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9945\n",
      "The average recall is: 0.9953\n",
      "The average f1 is: 0.9947\n",
      "  Training epcoh took: 0:01:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9638\n",
      "  Recall: 0.9988\n",
      "  F1: 0.9802\n",
      "Category: 1\n",
      "  Precision: 0.1296\n",
      "  Recall: 0.1065\n",
      "  F1: 0.1123\n",
      "The average precision is: 0.5467\n",
      "The average recall is: 0.5527\n",
      "The average f1 is: 0.5463\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    307.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    307.    Elapsed: 0:00:27.\n",
      "  Batch   120  of    307.    Elapsed: 0:00:41.\n",
      "  Batch   160  of    307.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    307.    Elapsed: 0:01:07.\n",
      "  Batch   240  of    307.    Elapsed: 0:01:21.\n",
      "  Batch   280  of    307.    Elapsed: 0:01:34.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9983\n",
      "The average recall is: 0.9991\n",
      "The average f1 is: 0.9986\n",
      "  Training epcoh took: 0:01:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9659\n",
      "  Recall: 0.9858\n",
      "  F1: 0.9748\n",
      "Category: 1\n",
      "  Precision: 0.1759\n",
      "  Recall: 0.1543\n",
      "  F1: 0.1574\n",
      "The average precision is: 0.5709\n",
      "The average recall is: 0.5701\n",
      "The average f1 is: 0.5661\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Current increase 20 times\n",
      "Current is processing category 1\n",
      "Before up-sample, there are 3423 samples\n",
      "There are 148 sampled in this category.\n",
      "After up-sample, there are 6383 samples\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3275\n",
      "0.5130816230612565\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Before up-sample, the ratio of current category and all samples\n",
      "0\n",
      "0.0\n",
      "\n",
      "After up-sample, the ratio of current category and all samples\n",
      "3108\n",
      "0.4869183769387435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    399.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    399.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    399.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    399.    Elapsed: 0:00:58.\n",
      "  Batch   200  of    399.    Elapsed: 0:01:12.\n",
      "  Batch   240  of    399.    Elapsed: 0:01:26.\n",
      "  Batch   280  of    399.    Elapsed: 0:01:41.\n",
      "  Batch   320  of    399.    Elapsed: 0:01:55.\n",
      "  Batch   360  of    399.    Elapsed: 0:02:09.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Accuracy: 0.91\n",
      "Category: 0\n",
      "  Precision: 0.91\n",
      "  Recall: 0.88\n",
      "  F1: 0.89\n",
      "Category: 1\n",
      "  Precision: 0.90\n",
      "  Recall: 0.94\n",
      "  F1: 0.91\n",
      "The average precision is: 0.9067\n",
      "The average recall is: 0.9102\n",
      "The average f1 is: 0.8984\n",
      "  Training epcoh took: 0:02:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "Category: 0\n",
      "  Precision: 0.9690\n",
      "  Recall: 0.9807\n",
      "  F1: 0.9740\n",
      "Category: 1\n",
      "  Precision: 0.1852\n",
      "  Recall: 0.1883\n",
      "  F1: 0.1753\n",
      "The average precision is: 0.5771\n",
      "The average recall is: 0.5845\n",
      "The average f1 is: 0.5746\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    399.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    399.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    399.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    399.    Elapsed: 0:00:57.\n",
      "  Batch   200  of    399.    Elapsed: 0:01:11.\n",
      "  Batch   240  of    399.    Elapsed: 0:01:26.\n",
      "  Batch   280  of    399.    Elapsed: 0:01:40.\n",
      "  Batch   320  of    399.    Elapsed: 0:01:54.\n",
      "  Batch   360  of    399.    Elapsed: 0:02:08.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 0.99\n",
      "  F1: 0.99\n",
      "Category: 1\n",
      "  Precision: 0.99\n",
      "  Recall: 1.00\n",
      "  F1: 0.99\n",
      "The average precision is: 0.9945\n",
      "The average recall is: 0.9941\n",
      "The average f1 is: 0.9939\n",
      "  Training epcoh took: 0:02:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9667\n",
      "  Recall: 0.9938\n",
      "  F1: 0.9794\n",
      "Category: 1\n",
      "  Precision: 0.1389\n",
      "  Recall: 0.1235\n",
      "  F1: 0.1259\n",
      "The average precision is: 0.5528\n",
      "The average recall is: 0.5586\n",
      "The average f1 is: 0.5527\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    399.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    399.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    399.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    399.    Elapsed: 0:00:57.\n",
      "  Batch   200  of    399.    Elapsed: 0:01:11.\n",
      "  Batch   240  of    399.    Elapsed: 0:01:26.\n",
      "  Batch   280  of    399.    Elapsed: 0:01:40.\n",
      "  Batch   320  of    399.    Elapsed: 0:01:54.\n",
      "  Batch   360  of    399.    Elapsed: 0:02:09.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9981\n",
      "The average recall is: 0.9987\n",
      "The average f1 is: 0.9983\n",
      "  Training epcoh took: 0:02:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9651\n",
      "  Recall: 0.9954\n",
      "  F1: 0.9793\n",
      "Category: 1\n",
      "  Precision: 0.1667\n",
      "  Recall: 0.1481\n",
      "  F1: 0.1543\n",
      "The average precision is: 0.5659\n",
      "The average recall is: 0.5718\n",
      "The average f1 is: 0.5668\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    399.    Elapsed: 0:00:14.\n",
      "  Batch    80  of    399.    Elapsed: 0:00:29.\n",
      "  Batch   120  of    399.    Elapsed: 0:00:43.\n",
      "  Batch   160  of    399.    Elapsed: 0:00:57.\n",
      "  Batch   200  of    399.    Elapsed: 0:01:11.\n",
      "  Batch   240  of    399.    Elapsed: 0:01:26.\n",
      "  Batch   280  of    399.    Elapsed: 0:01:40.\n",
      "  Batch   320  of    399.    Elapsed: 0:01:54.\n",
      "  Batch   360  of    399.    Elapsed: 0:02:09.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Accuracy: 1.00\n",
      "Category: 0\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "Category: 1\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1: 1.00\n",
      "The average precision is: 0.9994\n",
      "The average recall is: 0.9994\n",
      "The average f1 is: 0.9994\n",
      "  Training epcoh took: 0:02:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "Category: 0\n",
      "  Precision: 0.9637\n",
      "  Recall: 0.9975\n",
      "  F1: 0.9799\n",
      "Category: 1\n",
      "  Precision: 0.1389\n",
      "  Recall: 0.1080\n",
      "  F1: 0.1142\n",
      "The average precision is: 0.5513\n",
      "The average recall is: 0.5528\n",
      "The average f1 is: 0.5470\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Successfully save file ./BertSearchResult/rectified_6_32_3e-05_6train_on_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing...\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "  Accuracy: 0.99\n",
      "Category: 0\n",
      "  Precision: 0.9913\n",
      "  Recall: 0.9949\n",
      "  F1: 0.9929\n",
      "Category: 1\n",
      "  Precision: 0.6048\n",
      "  Recall: 0.5872\n",
      "  F1: 0.5792\n",
      "The average precision is: 0.7980\n",
      "The average recall is: 0.7911\n",
      "The average f1 is: 0.7861\n",
      "  Testing took: 0:00:15\n"
     ]
    }
   ],
   "source": [
    "section_category = 6\n",
    "# train_batch_a    = train_inputs_A_run,  train_labels_A_run, train_masks_A_run \n",
    "# val_batch_a    = validation_inputs_A_run, validation_labels_A_run, validation_masks_A_run\n",
    "a_batch = input_ids_A_run, labels_single_a, attention_masks_A_run \n",
    "\n",
    "# train_batch_b = train_inputs_B_run,  train_labels_B_run, train_masks_B_run \n",
    "# val_batch_b =    validation_inputs_B_run, validation_labels_B_run, validation_masks_B_run \n",
    "b_batch =     input_ids_B_run, labels_single_b, attention_masks_B_run \n",
    "    \n",
    "cross_event_validate(section_category, a_batch, b_batch,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpGYEq3GJYXZ"
   },
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pMl-2VJlZirM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7156f0ab-6c8e-4ea4-9b68-849e5642e802"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "L2DfmwSXWejr",
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyJhEtOoJScG"
   },
   "source": [
    "#### Incremental recitfied training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7CJocKtZirO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "test_batch = [test_inputs,test_labels,test_masks]\n",
    "Bert = Bertnn_rectified_info(train_inputs,train_labels,train_masks, validation_inputs,validation_labels,validation_masks,test_batch,batch_size=batch_size,lr=lr)\n",
    "all_metrics_one, all_tesst_metrics = Bert.searchUpsample(100)\n",
    "all_metrics_one['test'] = all_tesst_metrics\n",
    "all_metrics_all_category[section_category] = all_metrics_one\n",
    "if not os.path.exists('BertSearchResult'):\n",
    "  os.makedirs('BertSearchResult')\n",
    "\n",
    "filename = './BertSearchResult/'+'rectified'+'_'+str(section_category)+'_'+str(batch_size) + '_' + str(lr) + '_'  + str(section_category)\n",
    "with open(filename,'w') as file_obj:\n",
    "  json.dump(all_metrics_one,file_obj)\n",
    "  print('Successfully save file %s'%(filename))\n",
    "\n",
    "del Bert\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "U9SxZACXZirQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "203b4107-6a44-4a39-d1ce-f5ea357bf42a"
   },
   "outputs": [],
   "source": [
    "all_metrics_one"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Rh3iwCqWcSk",
    "iqxvbG3XWc4M",
    "egQ4fqBGWdd8",
    "bXtcQIcrWeDc",
    "wGbqnJyhWejl"
   ],
   "name": "COVID_Bert_information_type_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
