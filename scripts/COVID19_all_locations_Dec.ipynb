{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvpQIaud-ERQ"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load TREC-IS 2020 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DHVMwh3ttrf",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1221 13:12:24.284953 139910655899456 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
      "I1221 13:12:24.299455 139910655899456 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "from covid_tools import *\n",
    "from BERTs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load orginal data(COVID-19 TREC-IS 2020 task) from json file.\n",
    "cv19_dc = pd.read_json('./Pythonbooks/Data/COVID/cv19_dc_test.json',lines=True,orient='records')\n",
    "cv19_ws = pd.read_json('./Pythonbooks/Data/COVID/cv19_washington_state_test.json',lines=True,orient='records')\n",
    "cv19_ny = pd.read_json('./Pythonbooks/Data/COVID/cv19_nyc_test.json',lines=True,orient='records')\n",
    "cv19_a_run_info_labeled = pd.read_csv('cache/cv19_TREC_2020_all_labeled_A.csv',\n",
    "                           converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                       \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "cv19_dc_labeled = pd.merge(cv19_dc,cv19_a_run_info_labeled,on=['id'])\n",
    "cv19_ws_labeled = pd.merge(cv19_ws,cv19_a_run_info_labeled,on=['id'])\n",
    "cv19_ny_labeled = pd.merge(cv19_ny,cv19_a_run_info_labeled,on=['id'])\n",
    "\n",
    "cv19_dc_labeled = extract_majority_vote_label(cv19_dc_labeled)\n",
    "cv19_ny_labeled = extract_majority_vote_label(cv19_ny_labeled)\n",
    "cv19_ws_labeled = extract_majority_vote_label(cv19_ws_labeled)\n",
    "\n",
    "temp = [cv19_dc_labeled,cv19_ny_labeled,cv19_ws_labeled ]\n",
    "cv19_a_run_info_labeled =  pd.concat(temp)\n",
    "\n",
    "cv19_a_run_info_labeled = cv19_a_run_info_labeled.reset_index()\n",
    "\n",
    "cv19_a_run_info_labeled = Process_labels.extract_hashtags(cv19_a_run_info_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load TREC-IS 2020 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv19_b_run_info_labeled = pd.read_csv('cache/cv19_TREC_2020_all_labeled_B.csv',\n",
    "                           converters={\"priority\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                       \"categories\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "\n",
    "cv19_Houston_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Houston_2020.json',lines=True,orient='records')\n",
    "cv19_Atlanta_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Atlanta_2020.json',lines=True,orient='records')\n",
    "cv19_Melbourne2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Melbourne_2020.json',lines=True,orient='records')\n",
    "cv19_NYC_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_NYC_2020.json',lines=True,orient='records')\n",
    "cv19_acksonville_2020 = pd.read_json('./Pythonbooks/Data/COVID/cv_Jacksonville_2020.json',lines=True,orient='records')\n",
    "\n",
    "# Merge original dataset with lablled dataset based on id of tweets\n",
    "cv19_Houston_2020_labeled = pd.merge(cv19_Houston_2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_Atlanta_2020_labeled = pd.merge(cv19_Atlanta_2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_Melbourne2020_labeled = pd.merge(cv19_Melbourne2020, cv19_b_run_info_labeled,on=['id'])\n",
    "cv19_NYC_2020_labeled = pd.merge(cv19_NYC_2020, cv19_b_run_info_labeled, on=['id'])\n",
    "cv19_acksonville_2020_labeled = pd.merge(cv19_acksonville_2020,cv19_b_run_info_labeled, on=['id'])\n",
    "\n",
    "temp = [cv19_Houston_2020_labeled,cv19_Atlanta_2020_labeled,cv19_Melbourne2020_labeled,cv19_NYC_2020_labeled, cv19_acksonville_2020_labeled ]\n",
    "cv19_b_run_info_labeled =  pd.concat(temp)\n",
    "cv19_b_run_info_labeled = cv19_b_run_info_labeled.reset_index()\n",
    "\n",
    "cv19_b_run_info_labeled = extract_majority_vote_label(cv19_b_run_info_labeled)\n",
    "\n",
    "\n",
    "cv19_b_run_info_labeled = Process_labels.extract_hashtags(cv19_b_run_info_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv19_b_run_info_labeled = cv19_b_run_info_labeled.rename(columns={'full_text_x':'full_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','EmergingThreats','NewSubEvent','ServiceAvailable','Advice',]\n",
    "\n",
    "# # Extract training set\n",
    "# featureList = ['full_text'] #,'favorited'\n",
    "# dataset_inputs = extract_features(Tweets,featureList)\n",
    "\n",
    "# Extract labels\n",
    "labelList = ['categories']\n",
    "a_info_num_labels = Process_labels.extract_features(cv19_a_run_info_labeled,labelList)['categories']\n",
    "b_info_num_labels = Process_labels.extract_features(cv19_b_run_info_labeled,labelList)['categories']\n",
    "# Transfer a list of text labels into a list of number labels, like [0,1,0,0,0,0,0]\n",
    "a_info_num_labels = Process_labels.extractLabels(cv19_a_run_info_labeled,a_info_num_labels)\n",
    "b_info_num_labels = Process_labels.extractLabels(cv19_b_run_info_labeled,b_info_num_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_info_num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>categories</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>690</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-01 11:17:27+00:00</td>\n",
       "      <td>[0, 245]</td>\n",
       "      <td>{'hashtags': [{'text': 'Houston', 'indices': [...</td>\n",
       "      <td>{'media': [{'id': 1289520608485703682, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1247673787253960704, 'id_str': '1247673...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Houston Covid Covid19 medtwitter harriscounty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>489</td>\n",
       "      <td>[EmergingThreats, News, MultimediaShare, Locat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-16 00:42:58+00:00</td>\n",
       "      <td>[0, 124]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 130357315, 'id_str': '130357315', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-95.3996375,...</td>\n",
       "      <td>2020-07-18 22:12:52+00:00</td>\n",
       "      <td>[0, 91]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 13115772, 'id_str': '13115772', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>636</td>\n",
       "      <td>[News, MultimediaShare, Location, ServiceAvail...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-30 16:00:45+00:00</td>\n",
       "      <td>[0, 114]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2992907378, 'id_str': '2992907378', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-17 18:18:05+00:00</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1284190651790196737, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 739868121784320000, 'id_str': '73986812...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>809</td>\n",
       "      <td>[News, Sentiment, MultimediaShare, Location, S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-14 20:25:57+00:00</td>\n",
       "      <td>[0, 221]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1294369655931510784, 'id_str...</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'created_at': 'Fri Aug 14 19:35:49 +0000 2020...</td>\n",
       "      <td>1.294357e+18</td>\n",
       "      <td>1.294357e+18</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 366260560, 'id_str': '366260560', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>471</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-17 08:00:06+00:00</td>\n",
       "      <td>[0, 242]</td>\n",
       "      <td>{'hashtags': [{'text': 'school', 'indices': [5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://trueanthem.com/\" rel=\"nofollo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 23696276, 'id_str': '23696276', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>school classes Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>529</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-20 19:36:27+00:00</td>\n",
       "      <td>[0, 104]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 44040427, 'id_str': '44040427', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>795</td>\n",
       "      <td>[Hashtags, News, MultimediaShare, Location, Th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-30 12:53:10+00:00</td>\n",
       "      <td>[0, 105]</td>\n",
       "      <td>{'hashtags': [{'text': 'txwater', 'indices': [...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1485688854, 'id_str': '1485688854', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>txwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>416</td>\n",
       "      <td>[News, Sentiment, MultimediaShare, Location, T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 14:52:01+00:00</td>\n",
       "      <td>[0, 95]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 19416833, 'id_str': '19416833', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>447</td>\n",
       "      <td>[EmergingThreats, News, MultimediaShare, Locat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-12 04:21:24+00:00</td>\n",
       "      <td>[0, 98]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 80620895, 'id_str': '80620895', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>800</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-30 01:08:01+00:00</td>\n",
       "      <td>[0, 113]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 478786160, 'id_str': '478786160', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>535</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-14 07:34:17+00:00</td>\n",
       "      <td>[0, 114]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://ifttt.com\" rel=\"nofollow\"&gt;IFT...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 5648162, 'id_str': '5648162', 'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>752</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-11 14:57:02+00:00</td>\n",
       "      <td>[0, 104]</td>\n",
       "      <td>{'hashtags': [{'text': 'Startups', 'indices': ...</td>\n",
       "      <td>{'media': [{'id': 1293199750213312513, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://buffer.com\" rel=\"nofollow\"&gt;Bu...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 59264763, 'id_str': '59264763', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>663</td>\n",
       "      <td>[Hashtags, News, MultimediaShare, Location, Se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-04 17:11:50+00:00</td>\n",
       "      <td>[0, 84]</td>\n",
       "      <td>{'hashtags': [{'text': 'Covid19', 'indices': [...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://www.hootsuite.com\" rel=\"nofol...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 715596546318798849, 'id_str': '71559654...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>909</td>\n",
       "      <td>[Hashtags, Sentiment, MultimediaShare, Locatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-31 16:47:37+00:00</td>\n",
       "      <td>[0, 60]</td>\n",
       "      <td>{'hashtags': [{'text': 'Hydroxychloroquine', '...</td>\n",
       "      <td>{'media': [{'id': 1289241314278739968, 'id_str...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1278330638777618434, 'id_str': '1278330...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine COVID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>837</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-29 14:52:39+00:00</td>\n",
       "      <td>[0, 154]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 913105030567170048, 'id_str': '91310503...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>646</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-29 05:20:00+00:00</td>\n",
       "      <td>[0, 154]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://www.socialnewsdesk.com\" rel=\"n...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 46326721, 'id_str': '46326721', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>626</td>\n",
       "      <td>[EmergingThreats, News, MultimediaShare, Locat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-06 07:55:22+00:00</td>\n",
       "      <td>[0, 112]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 3141112620, 'id_str': '3141112620', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>406</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 16:40:27+00:00</td>\n",
       "      <td>[0, 191]</td>\n",
       "      <td>{'hashtags': [{'text': 'KHOU', 'indices': [186...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://www.socialnewsdesk.com\" rel=\"n...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 15791186, 'id_str': '15791186', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>845</td>\n",
       "      <td>[News, Sentiment, MultimediaShare, Location, T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-01 11:39:34+00:00</td>\n",
       "      <td>[0, 39]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1160503471, 'id_str': '1160503471', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>111</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-23 21:41:50+00:00</td>\n",
       "      <td>[11, 261]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1286386493674659842, 'id_str': '1286386...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>834</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-12 16:04:51+00:00</td>\n",
       "      <td>[0, 203]</td>\n",
       "      <td>{'hashtags': [{'text': 'Houston', 'indices': [...</td>\n",
       "      <td>{'media': [{'id': 1293579186486030347, 'id_str...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2383498635, 'id_str': '2383498635', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Houston RealEstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>176</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-14 21:43:54+00:00</td>\n",
       "      <td>[0, 86]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://www.socialflow.com\" rel=\"nofol...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 45830077, 'id_str': '45830077', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>421</td>\n",
       "      <td>[Hashtags, News, MultimediaShare, Location, Th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-15 14:33:25+00:00</td>\n",
       "      <td>[0, 163]</td>\n",
       "      <td>{'hashtags': [{'text': 'Coronavirus', 'indices...</td>\n",
       "      <td>{'media': [{'id': 1283408848137723905, 'id_str...</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 17477775, 'id_str': '17477775', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>616</td>\n",
       "      <td>[Hashtags, MultimediaShare, Location, ServiceA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-30 15:47:28+00:00</td>\n",
       "      <td>[0, 175]</td>\n",
       "      <td>{'hashtags': [{'text': 'frontline', 'indices':...</td>\n",
       "      <td>{'media': [{'id': 1288863789253877761, 'id_str...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://www.hootsuite.com\" rel=\"nofol...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 588039483, 'id_str': '588039483', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frontline FoodForHerosTX nommilove foodcravin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>440</td>\n",
       "      <td>[Hashtags, News, MultimediaShare, Location, Th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-26 06:00:33+00:00</td>\n",
       "      <td>[0, 278]</td>\n",
       "      <td>{'hashtags': [{'text': 'Report', 'indices': [2...</td>\n",
       "      <td>{'media': [{'id': 1287266536210669568, 'id_str...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1229752606714728454, 'id_str': '1229752...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Report China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>631</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-16 15:29:38+00:00</td>\n",
       "      <td>[0, 226]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 18384896, 'id_str': '18384896', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>510</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-30 12:52:36+00:00</td>\n",
       "      <td>[0, 232]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1288819761841930240, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2407823630, 'id_str': '2407823630', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>709</td>\n",
       "      <td>[News, Sentiment, MultimediaShare, Location, T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-14 03:17:33+00:00</td>\n",
       "      <td>[0, 41]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2286783277, 'id_str': '2286783277', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>634</td>\n",
       "      <td>4454</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-25 03:09:53+00:00</td>\n",
       "      <td>[0, 251]</td>\n",
       "      <td>{'hashtags': [{'text': 'NOW', 'indices': [0, 4...</td>\n",
       "      <td>{'media': [{'id': 1286861141373792257, 'id_str...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1330274059, 'id_str': '1330274059', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOW Jacksonville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>635</td>\n",
       "      <td>5129</td>\n",
       "      <td>[ThirdPartyObservation, Advice, MultimediaShare]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-19 11:49:19+00:00</td>\n",
       "      <td>[12, 291]</td>\n",
       "      <td>{'hashtags': [{'text': 'Florida', 'indices': [...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1242375708724215808, 'id_str': '1242375...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>636</td>\n",
       "      <td>4402</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-98.25, 27.78]}</td>\n",
       "      <td>2020-08-03 03:37:40+00:00</td>\n",
       "      <td>[0, 175]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mesonet.agron.iastate.edu/pro...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 34921867, 'id_str': '34921867', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>637</td>\n",
       "      <td>4502</td>\n",
       "      <td>[ThirdPartyObservation, Discussion, Location, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-27 14:01:52+00:00</td>\n",
       "      <td>[0, 278]</td>\n",
       "      <td>{'hashtags': [{'text': 'FOTW2020', 'indices': ...</td>\n",
       "      <td>{'media': [{'id': 1287750046574882816, 'id_str...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 830081900186304515, 'id_str': '83008190...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOTW2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>638</td>\n",
       "      <td>4113</td>\n",
       "      <td>[ThirdPartyObservation, Discussion, Location]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 02:58:15+00:00</td>\n",
       "      <td>[32, 90]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 896816994573574144, 'id_str': '89681699...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>639</td>\n",
       "      <td>5068</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-13 01:29:06+00:00</td>\n",
       "      <td>[0, 182]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://fl511.com\" rel=\"nofollow\"&gt;FL51...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199662147, 'id_str': '199662147', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>640</td>\n",
       "      <td>4461</td>\n",
       "      <td>[ThirdPartyObservation, Discussion, Location, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-12 15:43:42+00:00</td>\n",
       "      <td>[0, 160]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1282339584345624576, 'id_str...</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 46955476, 'id_str': '46955476', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>641</td>\n",
       "      <td>4239</td>\n",
       "      <td>[Sentiment, MultimediaShare, Location, ThirdPa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 08:44:53+00:00</td>\n",
       "      <td>[0, 264]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1286583110323376128, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>{'created_at': 'Thu Jul 23 21:50:00 +0000 2020...</td>\n",
       "      <td>1.286418e+18</td>\n",
       "      <td>1.286418e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1115784463483637762, 'id_str': '1115784...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>642</td>\n",
       "      <td>5115</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-11 17:02:02+00:00</td>\n",
       "      <td>[0, 182]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://fl511.com\" rel=\"nofollow\"&gt;FL51...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199662147, 'id_str': '199662147', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>643</td>\n",
       "      <td>4279</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-27 23:46:54+00:00</td>\n",
       "      <td>[0, 112]</td>\n",
       "      <td>{'hashtags': [{'text': 'OhioState', 'indices':...</td>\n",
       "      <td>{'media': [{'id': 1287897258697076737, 'id_str...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 93963430, 'id_str': '93963430', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OhioState Jaguars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485</th>\n",
       "      <td>644</td>\n",
       "      <td>5082</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-13 19:59:05+00:00</td>\n",
       "      <td>[0, 172]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://www.socialnewsdesk.com\" rel=\"n...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 18457416, 'id_str': '18457416', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>645</td>\n",
       "      <td>4198</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 02:15:00+00:00</td>\n",
       "      <td>[0, 156]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1286416410256838656, 'id_str...</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://ads.twitter.com\" rel=\"nofollo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 15675138, 'id_str': '15675138', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>646</td>\n",
       "      <td>4278</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-24 10:21:09+00:00</td>\n",
       "      <td>[13, 278]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1286607339957747713, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 890934534342148096, 'id_str': '89093453...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>647</td>\n",
       "      <td>5084</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-21 03:16:04+00:00</td>\n",
       "      <td>[0, 150]</td>\n",
       "      <td>{'hashtags': [{'text': 'fl511', 'indices': [12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://fl511.com\" rel=\"nofollow\"&gt;FL5...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199656446, 'id_str': '199656446', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>648</td>\n",
       "      <td>4066</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-04 17:40:08+00:00</td>\n",
       "      <td>[0, 158]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://fl511.com\" rel=\"nofollow\"&gt;FL5...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199656446, 'id_str': '199656446', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>649</td>\n",
       "      <td>4148</td>\n",
       "      <td>[Sentiment, MultimediaShare, Location, ThirdPa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-15 18:36:19+00:00</td>\n",
       "      <td>[16, 137]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1283470370608209922, 'id_str...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 99872727, 'id_str': '99872727', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>650</td>\n",
       "      <td>5143</td>\n",
       "      <td>[ThirdPartyObservation, Location, MultimediaSh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-18 15:40:26+00:00</td>\n",
       "      <td>[111, 218]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 933374396160462848, 'id_str': '93337439...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>651</td>\n",
       "      <td>4179</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-20 13:34:07+00:00</td>\n",
       "      <td>[0, 174]</td>\n",
       "      <td>{'hashtags': [{'text': 'fl511', 'indices': [14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://fl511.com\" rel=\"nofollow\"&gt;FL5...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199656446, 'id_str': '199656446', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>652</td>\n",
       "      <td>5103</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-18 15:05:00+00:00</td>\n",
       "      <td>[0, 170]</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 20798645, 'id_str': '20798645', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>653</td>\n",
       "      <td>4350</td>\n",
       "      <td>[ThirdPartyObservation, Discussion, Location, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-17 05:06:33+00:00</td>\n",
       "      <td>[0, 152]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1007400858, 'id_str': '1007400858', 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>654</td>\n",
       "      <td>4133</td>\n",
       "      <td>[ThirdPartyObservation, News, Location, Multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-16 17:00:22+00:00</td>\n",
       "      <td>[0, 166]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1283808704580788225, 'id_str...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://buffer.com\" rel=\"nofollow\"&gt;Bu...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1268198212403449856, 'id_str': '1268198...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>655</td>\n",
       "      <td>4067</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-23 01:05:00+00:00</td>\n",
       "      <td>[0, 209]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://www.socialnewsdesk.com\" rel=\"n...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 15572679, 'id_str': '15572679', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>656</td>\n",
       "      <td>4184</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-16 20:30:05+00:00</td>\n",
       "      <td>[0, 166]</td>\n",
       "      <td>{'hashtags': [{'text': 'fl511', 'indices': [13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://fl511.com\" rel=\"nofollow\"&gt;FL5...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199656446, 'id_str': '199656446', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>657</td>\n",
       "      <td>4258</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-04 17:59:07+00:00</td>\n",
       "      <td>[0, 158]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://fl511.com\" rel=\"nofollow\"&gt;FL5...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199656446, 'id_str': '199656446', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>658</td>\n",
       "      <td>5081</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-11 20:39:07+00:00</td>\n",
       "      <td>[0, 182]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://fl511.com\" rel=\"nofollow\"&gt;FL51...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199662147, 'id_str': '199662147', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>659</td>\n",
       "      <td>4395</td>\n",
       "      <td>[ThirdPartyObservation, Location, MultimediaSh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-07-12 03:54:44+00:00</td>\n",
       "      <td>[0, 274]</td>\n",
       "      <td>{'hashtags': [{'text': 'Karma', 'indices': [24...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 17701397, 'id_str': '17701397', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>660</td>\n",
       "      <td>4069</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-04 01:41:02+00:00</td>\n",
       "      <td>[0, 133]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://fl511.com\" rel=\"nofollow\"&gt;FL51...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 199662147, 'id_str': '199662147', 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>661</td>\n",
       "      <td>5049</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-20 13:23:09+00:00</td>\n",
       "      <td>[0, 97]</td>\n",
       "      <td>{'hashtags': [{'text': 'Jaguars', 'indices': [...</td>\n",
       "      <td>{'media': [{'id': 1296437612694179840, 'id_str...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 910492672023986177, 'id_str': '91049267...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jaguars NFL NFLBrasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4503</th>\n",
       "      <td>662</td>\n",
       "      <td>4149</td>\n",
       "      <td>[News, Sentiment, MultimediaShare, Location, T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-06 02:15:36+00:00</td>\n",
       "      <td>[0, 292]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>{'created_at': 'Thu Aug 06 02:02:35 +0000 2020...</td>\n",
       "      <td>1.291193e+18</td>\n",
       "      <td>1.291193e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 51240839, 'id_str': '51240839', 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>663</td>\n",
       "      <td>5119</td>\n",
       "      <td>[ThirdPartyObservation, Discussion, Location, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-08-17 23:52:00+00:00</td>\n",
       "      <td>[0, 280]</td>\n",
       "      <td>{'hashtags': [{'text': 'covid19', 'indices': [...</td>\n",
       "      <td>{'media': [{'id': 1293709890561675266, 'id_str...</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1228754267835707399, 'id_str': '1228754...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>covid19 healthcareheroes WearAMask</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4505 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Unnamed: 0                                         categories  \\\n",
       "0         0         690                                       [Irrelevant]   \n",
       "1         1         489  [EmergingThreats, News, MultimediaShare, Locat...   \n",
       "2         2         136                                       [Irrelevant]   \n",
       "3         3         636  [News, MultimediaShare, Location, ServiceAvail...   \n",
       "4         4         265                                       [Irrelevant]   \n",
       "...     ...         ...                                                ...   \n",
       "4500    659        4395  [ThirdPartyObservation, Location, MultimediaSh...   \n",
       "4501    660        4069                                       [Irrelevant]   \n",
       "4502    661        5049                                       [Irrelevant]   \n",
       "4503    662        4149  [News, Sentiment, MultimediaShare, Location, T...   \n",
       "4504    663        5119  [ThirdPartyObservation, Discussion, Location, ...   \n",
       "\n",
       "      contributors                                        coordinates  \\\n",
       "0              NaN                                               None   \n",
       "1              NaN                                               None   \n",
       "2              NaN  {'type': 'Point', 'coordinates': [-95.3996375,...   \n",
       "3              NaN                                               None   \n",
       "4              NaN                                               None   \n",
       "...            ...                                                ...   \n",
       "4500           NaN                                               None   \n",
       "4501           NaN                                               None   \n",
       "4502           NaN                                               None   \n",
       "4503           NaN                                               None   \n",
       "4504           NaN                                               None   \n",
       "\n",
       "                    created_at display_text_range  \\\n",
       "0    2020-08-01 11:17:27+00:00           [0, 245]   \n",
       "1    2020-07-16 00:42:58+00:00           [0, 124]   \n",
       "2    2020-07-18 22:12:52+00:00            [0, 91]   \n",
       "3    2020-07-30 16:00:45+00:00           [0, 114]   \n",
       "4    2020-07-17 18:18:05+00:00            [0, 89]   \n",
       "...                        ...                ...   \n",
       "4500 2020-07-12 03:54:44+00:00           [0, 274]   \n",
       "4501 2020-08-04 01:41:02+00:00           [0, 133]   \n",
       "4502 2020-08-20 13:23:09+00:00            [0, 97]   \n",
       "4503 2020-08-06 02:15:36+00:00           [0, 292]   \n",
       "4504 2020-08-17 23:52:00+00:00           [0, 280]   \n",
       "\n",
       "                                               entities  \\\n",
       "0     {'hashtags': [{'text': 'Houston', 'indices': [...   \n",
       "1     {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2     {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3     {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4     {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "...                                                 ...   \n",
       "4500  {'hashtags': [{'text': 'Karma', 'indices': [24...   \n",
       "4501  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4502  {'hashtags': [{'text': 'Jaguars', 'indices': [...   \n",
       "4503  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4504  {'hashtags': [{'text': 'covid19', 'indices': [...   \n",
       "\n",
       "                                      extended_entities  favorite_count  ...  \\\n",
       "0     {'media': [{'id': 1289520608485703682, 'id_str...               0  ...   \n",
       "1                                                   NaN               0  ...   \n",
       "2                                                   NaN               0  ...   \n",
       "3                                                   NaN               0  ...   \n",
       "4     {'media': [{'id': 1284190651790196737, 'id_str...               0  ...   \n",
       "...                                                 ...             ...  ...   \n",
       "4500                                                NaN               4  ...   \n",
       "4501                                                NaN               0  ...   \n",
       "4502  {'media': [{'id': 1296437612694179840, 'id_str...               2  ...   \n",
       "4503                                                NaN              11  ...   \n",
       "4504  {'media': [{'id': 1293709890561675266, 'id_str...             150  ...   \n",
       "\n",
       "                                          quoted_status quoted_status_id  \\\n",
       "0                                                   NaN              NaN   \n",
       "1                                                   NaN              NaN   \n",
       "2                                                   NaN              NaN   \n",
       "3                                                   NaN              NaN   \n",
       "4                                                   NaN              NaN   \n",
       "...                                                 ...              ...   \n",
       "4500                                                NaN              NaN   \n",
       "4501                                                NaN              NaN   \n",
       "4502                                                NaN              NaN   \n",
       "4503  {'created_at': 'Thu Aug 06 02:02:35 +0000 2020...     1.291193e+18   \n",
       "4504                                                NaN              NaN   \n",
       "\n",
       "     quoted_status_id_str retweet_count  retweeted  \\\n",
       "0                     NaN             0      False   \n",
       "1                     NaN             0      False   \n",
       "2                     NaN             0      False   \n",
       "3                     NaN             0      False   \n",
       "4                     NaN             0      False   \n",
       "...                   ...           ...        ...   \n",
       "4500                  NaN             1      False   \n",
       "4501                  NaN             0      False   \n",
       "4502                  NaN             0      False   \n",
       "4503         1.291193e+18             1      False   \n",
       "4504                  NaN            86      False   \n",
       "\n",
       "                                                 source truncated  \\\n",
       "0     <a href=\"https://mobile.twitter.com\" rel=\"nofo...     False   \n",
       "1     <a href=\"https://mobile.twitter.com\" rel=\"nofo...     False   \n",
       "2     <a href=\"http://instagram.com\" rel=\"nofollow\">...     False   \n",
       "3     <a href=\"http://twitter.com/download/iphone\" r...     False   \n",
       "4     <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...     False   \n",
       "...                                                 ...       ...   \n",
       "4500  <a href=\"http://twitter.com/download/iphone\" r...     False   \n",
       "4501  <a href=\"http://fl511.com\" rel=\"nofollow\">FL51...     False   \n",
       "4502  <a href=\"http://twitter.com/download/android\" ...     False   \n",
       "4503  <a href=\"https://mobile.twitter.com\" rel=\"nofo...     False   \n",
       "4504  <a href=\"https://mobile.twitter.com\" rel=\"nofo...     False   \n",
       "\n",
       "                                                   user  \\\n",
       "0     {'id': 1247673787253960704, 'id_str': '1247673...   \n",
       "1     {'id': 130357315, 'id_str': '130357315', 'name...   \n",
       "2     {'id': 13115772, 'id_str': '13115772', 'name':...   \n",
       "3     {'id': 2992907378, 'id_str': '2992907378', 'na...   \n",
       "4     {'id': 739868121784320000, 'id_str': '73986812...   \n",
       "...                                                 ...   \n",
       "4500  {'id': 17701397, 'id_str': '17701397', 'name':...   \n",
       "4501  {'id': 199662147, 'id_str': '199662147', 'name...   \n",
       "4502  {'id': 910492672023986177, 'id_str': '91049267...   \n",
       "4503  {'id': 51240839, 'id_str': '51240839', 'name':...   \n",
       "4504  {'id': 1228754267835707399, 'id_str': '1228754...   \n",
       "\n",
       "      withheld_in_countries                                           hashtags  \n",
       "0                       NaN   Houston Covid Covid19 medtwitter harriscounty...  \n",
       "1                       NaN                                                     \n",
       "2                       NaN                                                     \n",
       "3                       NaN                                                     \n",
       "4                       NaN                                                     \n",
       "...                     ...                                                ...  \n",
       "4500                    NaN                                             Karma   \n",
       "4501                    NaN                                                     \n",
       "4502                    NaN                             Jaguars NFL NFLBrasil   \n",
       "4503                    NaN                                                     \n",
       "4504                    NaN                covid19 healthcareheroes WearAMask   \n",
       "\n",
       "[4505 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv19_b_run_info_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_priority_num_labels = cv19_a_run_info_labeled.priority\n",
    "b_priority_num_labels = cv19_b_run_info_labeled.priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_label(labels):\n",
    "    labels_single = {}\n",
    "    for i in range(7):\n",
    "      print('Currrent is processing on categorie %s'%(categories[i]))\n",
    "      print()\n",
    "      # if i == 3:\n",
    "      #   continue\n",
    "      labels_one_category = []\n",
    "      for j in range(len(labels)):\n",
    "        labels_one_category.append(labels[j][i])\n",
    "      labels_single[i] = labels_one_category\n",
    "    return labels_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Currrent is processing on categorie Volunteer\n",
      "\n",
      "Currrent is processing on categorie EmergingThreats\n",
      "\n",
      "Currrent is processing on categorie NewSubEvent\n",
      "\n",
      "Currrent is processing on categorie ServiceAvailable\n",
      "\n",
      "Currrent is processing on categorie Advice\n",
      "\n",
      "Currrent is processing on categorie GoodsServices\n",
      "\n",
      "Currrent is processing on categorie InformationWanted\n",
      "\n",
      "Currrent is processing on categorie Volunteer\n",
      "\n",
      "Currrent is processing on categorie EmergingThreats\n",
      "\n",
      "Currrent is processing on categorie NewSubEvent\n",
      "\n",
      "Currrent is processing on categorie ServiceAvailable\n",
      "\n",
      "Currrent is processing on categorie Advice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','EmergingThreats','NewSubEvent','ServiceAvailable','Advice']\n",
    "# Sperate labels for each category and store them in one dict\n",
    "# A dict Contains labels for all categories e.g. trainL_single[category]\n",
    "labels_single_a = build_single_label(a_info_num_labels)\n",
    "labels_single_b = build_single_label(b_info_num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing on run a and run b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## info classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprossesing\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Use Spacy as nlp tool\n",
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "# Do not remove 'not' as stopword\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Speed up the processing of Spacy\n",
    "disable=['ner']\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "# These three functions are learnt from course named Text as data\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if (not token.is_stop):\n",
    "        if ((token.is_alpha or token.is_digit or token.like_url)):\n",
    "            normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=25000,ngram_range=(1,2),sublinear_tf=(1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_event_validate_lr_info_task(section_category, a_batch, b_batch, increase_time\n",
    "                        ):\n",
    "    import json\n",
    "    \n",
    "    # training\n",
    "    section_category = section_category\n",
    "    a_all_data = a_batch[0]\n",
    "    b_all_data = b_batch[0]\n",
    "    a_labels = a_batch[1][section_category]\n",
    "    b_labels = b_batch[1][section_category]\n",
    "    increaseTime = increase_time\n",
    "    print('Current is process category %d' %(section_category))\n",
    "    #\n",
    "    featureList = ['full_text','hashtags','favorite_count'] #,'favorited'\n",
    "    a_extracted = Process_labels.extract_features(a_all_data,featureList)\n",
    "    b_extracted = Process_labels.extract_features(b_all_data,featureList)\n",
    "\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    # Over-sample both data set\n",
    "    \n",
    "    # over-samle data set a\n",
    "    trainSet_temp = copy.deepcopy(a_extracted)\n",
    "    label_temp = copy.deepcopy(a_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "\n",
    "    print('a: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(1)\n",
    "\n",
    "    a_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    a_labels = label_temp\n",
    "    print('a: over-sample  length is %d and %d' %(len(a_extracted),len(label_temp)))\n",
    "    # Over-sample data set b\n",
    "    trainSet_temp = copy.deepcopy(b_extracted)\n",
    "    label_temp = copy.deepcopy(b_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "    print('b: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(1)\n",
    "\n",
    "    b_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    b_labels = label_temp\n",
    "    print('b: over-sample  length is %d and %d' %(len(b_extracted),len(label_temp)))\n",
    "\n",
    "    # Train on dataset-a and test on dataset-b\n",
    "    print('Cross event validation')\n",
    "\n",
    "    fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "        ])),\n",
    "        ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "            ('cv',count_vect),\n",
    "        ])),\n",
    "        ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "        ])),\n",
    "        ])\n",
    "\n",
    "\n",
    "    fu.fit(a_extracted)\n",
    "    a_features_union = fu.transform(a_extracted)\n",
    "    b_features_union = fu.transform(b_extracted)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(a_features_union, a_labels)\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), a_features_union, a_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision', 'f1', 'recall'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(b_features_union)\n",
    "    print(\"Test result on a:\")   \n",
    "    print(classification_report(predictions,  b_labels, digits=3))  \n",
    "    \n",
    "\n",
    "    # Train on dataset-b and test on dataset-a\n",
    "    fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "        ])),\n",
    "        ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "            ('cv',count_vect),\n",
    "        ])),\n",
    "        ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "        ])),\n",
    "        ])\n",
    "\n",
    "\n",
    "    fu.fit(b_extracted)\n",
    "    a_features_union = fu.transform(a_extracted)\n",
    "    b_features_union = fu.transform(b_extracted)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(b_features_union, b_labels)\n",
    "\n",
    "    predictions = lr.predict(a_features_union)\n",
    "\n",
    "    print(\"Test result on b:\")   \n",
    "    print(classification_report(predictions,  a_labels, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "32\n",
      "12\n",
      "38\n",
      "107\n",
      "166\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(np.sum(labels_single_b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current is process category 0\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 8015 and 8015\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4649 and 4649\n",
      "Cross event validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 9.80899882,  6.39320111,  5.19543862, 12.60073304, 11.08974409]), 'score_time': array([0.09212255, 0.00450754, 0.00606394, 0.09262538, 0.00664234]), 'test_accuracy': array([0.95321273, 0.97005614, 0.95446039, 0.95446039, 0.94323144]), 'train_accuracy': array([0.99968808, 0.99937617, 0.99922021, 0.99937617, 0.99844042]), 'test_precision': array([1.        , 1.        , 0.94736842, 0.94736842, 0.        ]), 'train_precision': array([0.99447514, 0.99444444, 0.99442897, 0.9972067 , 1.        ]), 'test_f1': array([0.28571429, 0.63636364, 0.33027523, 0.33027523, 0.        ]), 'train_f1': array([0.99722992, 0.99444444, 0.9930459 , 0.99442897, 0.98591549]), 'test_recall': array([0.16666667, 0.46666667, 0.2       , 0.2       , 0.        ]), 'train_recall': array([1.        , 0.99444444, 0.99166667, 0.99166667, 0.97222222])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.968     0.984      4649\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.968      4649\n",
      "   macro avg      0.500     0.484     0.492      4649\n",
      "weighted avg      1.000     0.968     0.984      4649\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-de7ea0ffc0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msection_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mincrease_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_over_sample_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     cross_event_validate_lr_info_task(section_category, a_batch, b_batch,increase_time\n\u001b[0m\u001b[1;32m      9\u001b[0m                         )\n",
      "\u001b[0;32m<ipython-input-12-291b63626acc>\u001b[0m in \u001b[0;36mcross_event_validate_lr_info_task\u001b[0;34m(section_category, a_batch, b_batch, increase_time)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_extracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0ma_features_union\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_extracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mb_features_union\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_extracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         Xs = Parallel(n_jobs=self.n_jobs)(\n\u001b[1;32m   1009\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;31m# if we have a weight for this transformer, multiply output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1879\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2881ca986bba>\u001b[0m in \u001b[0;36mtokenize_normalize\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#@Tokenize and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_normalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2881ca986bba>\u001b[0m in \u001b[0;36mspacy_tokenize\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a_batch = [cv19_a_run_info_labeled, labels_single_a]\n",
    "b_batch = [cv19_b_run_info_labeled, labels_single_b]\n",
    "best_over_sample_factors = [24,44,7,4,4,4,1]\n",
    "\n",
    "for i in range(7):\n",
    "    section_category = i\n",
    "    increase_time = best_over_sample_factors[i]\n",
    "    cross_event_validate_lr_info_task(section_category, a_batch, b_batch,increase_time\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## priority task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_event_validate_lr_priority_task(a_batch, b_batch, increase_time\n",
    "                        ):\n",
    "    import json\n",
    "    \n",
    "    # training\n",
    "    a_all_data = a_batch[0]\n",
    "    b_all_data = b_batch[0]\n",
    "    a_labels = a_batch[1]\n",
    "    b_labels = b_batch[1]\n",
    "    increaseTime = increase_time\n",
    "\n",
    "    #\n",
    "    featureList = ['full_text','hashtags','favorite_count'] #,'favorited'\n",
    "    a_extracted = Process_labels.extract_features(a_all_data,featureList)\n",
    "    b_extracted = Process_labels.extract_features(b_all_data,featureList)\n",
    "\n",
    "        # Over-sample both data set\n",
    "    \n",
    "    # over-samle data set a\n",
    "    trainSet_temp = copy.deepcopy(a_extracted)\n",
    "    label_temp = copy.deepcopy(a_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "\n",
    "    print('a: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(trainSet_temp['priority'][idx])\n",
    "\n",
    "    a_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    a_labels = label_temp\n",
    "    print('a: over-sample  length is %d and %d' %(len(a_extracted),len(label_temp)))\n",
    "    # Over-sample data set b\n",
    "    trainSet_temp = copy.deepcopy(b_extracted)\n",
    "    label_temp = copy.deepcopy(b_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "    print('b: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(trainSet_temp['priority'][idx])\n",
    "\n",
    "    b_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    b_labels = label_temp\n",
    "    print('b: over-sample  length is %d and %d' %(len(b_extracted),len(label_temp)))\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "    # Train on dataset-a and test on dataset-b\n",
    "    print('Cross event validation:')\n",
    "\n",
    "    fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "        ])),\n",
    "        ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "            ('cv',count_vect),\n",
    "        ])),\n",
    "        ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "        ])),\n",
    "        ])\n",
    "\n",
    "\n",
    "    fu.fit(a_extracted)\n",
    "    a_features_union = fu.transform(a_extracted)\n",
    "    b_features_union = fu.transform(b_extracted)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(a_features_union, a_labels)\n",
    "\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), a_features_union, a_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision_macro', 'f1_macro', 'recall_macro'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(b_features_union)\n",
    "    print(\"Test result on b:\")   \n",
    "    print(classification_report(predictions,  b_labels, digits=3))  \n",
    "     \n",
    "    \n",
    "\n",
    "    # Train on dataset-b and test on dataset-a\n",
    "    fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "        ])),\n",
    "        ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "            ('cv',count_vect),\n",
    "        ])),\n",
    "        ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "        ])),\n",
    "        ])\n",
    "\n",
    "    fu.fit(b_extracted)\n",
    "    a_features_union = fu.transform(a_extracted)\n",
    "    b_features_union = fu.transform(b_extracted)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(b_features_union, b_labels)\n",
    "\n",
    "    predictions = lr.predict(a_features_union)\n",
    "\n",
    "    print(\"Test result on a:\")   \n",
    "    print(classification_report(predictions,  a_labels, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_batch = [cv19_a_run_info_labeled, a_priority_num_labels]\n",
    "b_batch = [cv19_b_run_info_labeled, b_priority_num_labels]\n",
    "\n",
    "best_over_sample_factor = 6\n",
    "cross_event_validate_lr_priority_task( a_batch, b_batch, best_over_sample_factor\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT with lr info task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bert_with_lr_info_task(section_category, a_batch, b_batch, increase_time):\n",
    "    \n",
    "    # identify all data to use\n",
    "    section_category = section_category\n",
    "    a_all_data = a_batch[0]\n",
    "    b_all_data = b_batch[0]\n",
    "  \n",
    "    a_info_labels = a_batch[1][section_category]\n",
    "    b_info_labels = b_batch[1][section_category]\n",
    "\n",
    "    increaseTime = increase_time\n",
    "    category_number = 2\n",
    "    print('Current is process category %d' %(section_category))\n",
    "    # info classification\n",
    "\n",
    "    from transformers import BertTokenizer\n",
    "\n",
    "    # Load the BERT tokenizer.\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    featureList = ['full_text','hashtags','favorite_count'] #,'favorited'\n",
    "    a_extracted = Process_labels.extract_features(a_all_data,featureList)\n",
    "    b_extracted = Process_labels.extract_features(b_all_data,featureList)\n",
    "\n",
    "    # Over-sample both data set\n",
    "    \n",
    "    # over-samle data set a\n",
    "    trainSet_temp = copy.deepcopy(a_extracted)\n",
    "    label_temp = copy.deepcopy(a_info_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "\n",
    "    print('a: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(1)\n",
    "\n",
    "    a_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    a_labels = label_temp\n",
    "    print('a: over-sample  length is %d and %d' %(len(a_extracted),len(label_temp)))\n",
    "\n",
    "    # Over-sample data set b\n",
    "    trainSet_temp = copy.deepcopy(b_extracted)\n",
    "    label_temp = copy.deepcopy(b_info_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 1)[0]\n",
    "    print('b: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(1)\n",
    "\n",
    "    b_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    b_labels = label_temp\n",
    "    print('b: over-sample  length is %d and %d' %(len(b_extracted),len(label_temp)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "    dataset_inputs_A_run = a_extracted\n",
    "    dataset_inputs_B_run = b_extracted\n",
    "    sentences_A_run = dataset_inputs_A_run.full_text.values\n",
    "    # labels_A_run = dataset_inputs_A_run.target.values\n",
    "    sentences_B_run = dataset_inputs_B_run.full_text.values\n",
    "    # labels_B_run = dataset_inputs_B_run.target.values\n",
    "    for idx in range(len(sentences_A_run)):\n",
    "      sentences_A_run[idx] = sentences_A_run[idx]  + ' '+ dataset_inputs_A_run.hashtags.values[idx] + ' ' + str(dataset_inputs_A_run.favorite_count.values[idx])\n",
    "    for idx in range(len(sentences_B_run)):\n",
    "      sentences_B_run[idx] = sentences_B_run[idx]  + ' '+ dataset_inputs_B_run.hashtags.values[idx] + ' ' + str(dataset_inputs_B_run.favorite_count.values[idx])\n",
    "\n",
    "    input_ids_A_run = Encode_sents(sentences_A_run)\n",
    "    input_ids_B_run = Encode_sents(sentences_B_run)\n",
    "\n",
    "    max_len = 144\n",
    "    padded_a = np.array([i+[0]*(max_len - len(i)) for i in input_ids_A_run])\n",
    "    padded_b = np.array([i+[0]*(max_len - len(i)) for i in input_ids_B_run])\n",
    "    print('padded shape',padded_a.shape,padded_b.shape)\n",
    "    attension_mask_a = np.where(padded_a != 0, 1, 0)\n",
    "    attension_mask_b = np.where(padded_b != 0, 1, 0)\n",
    "\n",
    "    print('attension mask shape:',attension_mask_a.shape, attension_mask_b.shape)\n",
    "    input_ids_A_run, input_ids_B_run = torch.tensor(padded_a), torch.tensor(padded_b)\n",
    "    attension_mask_a, attension_mask_b = torch.tensor(attension_mask_a), torch.tensor(attension_mask_b)\n",
    "    \n",
    "    # Identify GPU to use\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # initial BERT model\n",
    "    model  = BertForSequenceClassification.from_pretrained(\n",
    "      \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "      num_labels=category_number,  # The number of output labels--2 for binary classification.\n",
    "      # You can increase this for multi-class tasks.\n",
    "      # output_attentions=False,  # Whether the model returns attentions weights.\n",
    "      # output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.to(device)\n",
    "    # input_ids_A_run = input_ids_A_run.to(device)\n",
    "    # input_ids_B_run = input_ids_B_run.to(device)\n",
    "    # attension_mask_a = attension_mask_a.to(device)\n",
    "    # attension_mask_b = attension_mask_b.to(device)\n",
    "    \n",
    "    features_a = []\n",
    "    for i in range(len(input_ids_A_run)):\n",
    "      input_id = input_ids_A_run[i].reshape(1,-1).to(device)\n",
    "      attension_mask = attension_mask_a[i].reshape(1,-1).to(device)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_state = model(input_id, attention_mask = attension_mask)\n",
    "      features_a.append(last_hidden_state[0][0,:].detach().cpu().numpy())\n",
    "      \n",
    "      del last_hidden_state, input_id\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    features_b = []\n",
    "    for i in range(len(input_ids_B_run)):\n",
    "      input_id = input_ids_B_run[i].reshape(1,-1).to(device)\n",
    "      attension_mask = attension_mask_b[i].reshape(1,-1).to(device)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_state = model(input_id, attention_mask = attension_mask)\n",
    "      features_b.append(last_hidden_state[0][0,:].detach().cpu().numpy())\n",
    "      \n",
    "      del last_hidden_state, input_id\n",
    "      torch.cuda.empty_cache()  \n",
    "\n",
    "    features_a = np.array(features_a)\n",
    "    features_b = np.array(features_b)\n",
    "    # Train on dataset-a and test on dataset-b\n",
    "    print('Length %d %d'%(len(input_ids_A_run), len(a_labels)))\n",
    "    print('Cross event validation')\n",
    "    # input_ids_A_run = np.array(input_ids_A_run)\n",
    "    # input_ids_B_run = np.array(input_ids_B_run)\n",
    "\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(features_a, a_labels)\n",
    "\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), features_a, a_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision', 'f1', 'recall'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(features_b)\n",
    "    print(\"Test result on b:\")   \n",
    "    print(classification_report(predictions,  b_labels, digits=3))  \n",
    "\n",
    "    #  Train on dataset-b and test on dataset-a\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(features_b, b_labels)\n",
    "\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), features_b, b_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision', 'f1', 'recall'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(features_a)\n",
    "    print(\"Test result on a:\")   \n",
    "    print(classification_report(predictions,  a_labels, digits=3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encode_sents(sentences):\n",
    "    from transformers import BertTokenizer\n",
    "\n",
    "    # Load the BERT tokenizer.\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    input_ids = []\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "\n",
    "                            #max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        # If the length of token is over the bert limitation, cut off the back\n",
    "        if(len(encoded_sent)>144):\n",
    "            diff = int((len(encoded_sent) - 145)/2)\n",
    "            encoded_sent = encoded_sent[diff:144+diff]\n",
    "#         encoded_sent = np.array(encoded_sent)    \n",
    "        input_ids.append(encoded_sent)\n",
    "    return input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current is process category 0\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 8015 and 8015\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4649 and 4649\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (8015, 144) (4649, 144)\n",
      "attension mask shape: (8015, 144) (4649, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 8015 8015\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.01193476, 0.19089532, 0.10322213, 0.10208082, 0.18499637]), 'score_time': array([0.08905029, 0.00686526, 0.00653958, 0.00675869, 0.00700903]), 'test_accuracy': array([0.94385527, 0.94385527, 0.94385527, 0.94385527, 0.94385527]), 'train_accuracy': array([0.94385527, 0.94385527, 0.94385527, 0.94385527, 0.94385527]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.968     0.984      4649\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.968      4649\n",
      "   macro avg      0.500     0.484     0.492      4649\n",
      "weighted avg      1.000     0.968     0.984      4649\n",
      "\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.09744549, 0.00611591, 0.00637579, 0.00662804, 0.00665069]), 'score_time': array([0.00285912, 0.00280261, 0.0028491 , 0.00281692, 0.00284719]), 'test_accuracy': array([0.96774194, 0.96774194, 0.96774194, 0.96774194, 0.96770721]), 'train_accuracy': array([0.96773326, 0.96773326, 0.96773326, 0.96773326, 0.96774194]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.944     0.971      8015\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.944      8015\n",
      "   macro avg      0.500     0.472     0.486      8015\n",
      "weighted avg      1.000     0.944     0.971      8015\n",
      "\n",
      "Current is process category 1\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 8595 and 8595\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 5913 and 5913\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (8595, 144) (5913, 144)\n",
      "attension mask shape: (8595, 144) (5913, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 8595 8595\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.10172153, 0.191576  , 0.09355688, 0.09504008, 0.09653783]), 'score_time': array([0.00757766, 0.00707698, 0.00700545, 0.00699639, 0.00685   ]), 'test_accuracy': array([0.87899942, 0.87550902, 0.87085515, 0.87841768, 0.87958115]), 'train_accuracy': array([0.87638162, 0.87681792, 0.87783595, 0.87594532, 0.87958115]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     0.756     0.861      5907\n",
      "           1      0.000     0.000     0.000         6\n",
      "\n",
      "    accuracy                          0.755      5913\n",
      "   macro avg      0.499     0.378     0.430      5913\n",
      "weighted avg      0.998     0.755     0.860      5913\n",
      "\n",
      "Training result:\n",
      "{'fit_time': array([0.10383725, 0.01211572, 0.01138973, 0.09890771, 0.09871793]), 'score_time': array([0.0917542 , 0.00345683, 0.08728242, 0.00558066, 0.00561357]), 'test_accuracy': array([0.7421809 , 0.79374472, 0.77007608, 0.74365482, 0.75380711]), 'train_accuracy': array([0.76765328, 0.75243129, 0.7653277 , 0.76854788, 0.76136123]), 'test_precision': array([0.13043478, 1.        , 0.61111111, 0.        , 0.        ]), 'train_precision': array([0.62559242, 0.04761905, 0.65      , 0.73170732, 0.67164179]), 'test_f1': array([0.0192926 , 0.26506024, 0.24444444, 0.        , 0.        ]), 'train_f1': array([0.19369039, 0.00170503, 0.14086687, 0.14117647, 0.07383101]), 'test_recall': array([0.01041667, 0.15277778, 0.15277778, 0.        , 0.        ]), 'train_recall': array([0.11458333, 0.00086806, 0.07899306, 0.078125  , 0.0390625 ])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.988     0.883     0.932      8458\n",
      "           1      0.043     0.328     0.077       137\n",
      "\n",
      "    accuracy                          0.874      8595\n",
      "   macro avg      0.516     0.606     0.505      8595\n",
      "weighted avg      0.973     0.874     0.919      8595\n",
      "\n",
      "Current is process category 2\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 7905 and 7905\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4589 and 4589\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (7905, 144) (4589, 144)\n",
      "attension mask shape: (7905, 144) (4589, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 7905 7905\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.10076451, 0.09475112, 0.09628344, 0.0949049 , 0.09427404]), 'score_time': array([0.00726151, 0.00635529, 0.00638556, 0.00597358, 0.00598359]), 'test_accuracy': array([0.95382669, 0.95382669, 0.95319418, 0.95319418, 0.95319418]), 'train_accuracy': array([0.95335231, 0.95335231, 0.95351044, 0.95351044, 0.95351044]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.979     0.989      4589\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.979      4589\n",
      "   macro avg      0.500     0.490     0.495      4589\n",
      "weighted avg      1.000     0.979     0.989      4589\n",
      "\n",
      "Training result:\n",
      "{'fit_time': array([0.0062089 , 0.00576615, 0.00601816, 0.00593019, 0.00577164]), 'score_time': array([0.00283027, 0.00277972, 0.00281262, 0.0028007 , 0.00282311]), 'test_accuracy': array([0.97930283, 0.97930283, 0.97930283, 0.97821351, 0.97928026]), 'train_accuracy': array([0.97902479, 0.97902479, 0.97902479, 0.97929719, 0.9790305 ]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.953     0.976      7905\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.953      7905\n",
      "   macro avg      0.500     0.477     0.488      7905\n",
      "weighted avg      1.000     0.953     0.976      7905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current is process category 3\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 9639 and 9639\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4657 and 4657\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (9639, 144) (4657, 144)\n",
      "attension mask shape: (9639, 144) (4657, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 9639 9639\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.10251403, 0.10010719, 0.09912086, 0.09831214, 0.08957672]), 'score_time': array([0.0966475 , 0.00436902, 0.00640273, 0.00643349, 0.09021282]), 'test_accuracy': array([0.73340249, 0.73340249, 0.72977178, 0.73340249, 0.73326414]), 'train_accuracy': array([0.73336792, 0.73336792, 0.73297886, 0.73207107, 0.73301349]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.959     0.979      4656\n",
      "           1      0.000     0.000     0.000         1\n",
      "\n",
      "    accuracy                          0.959      4657\n",
      "   macro avg      0.500     0.480     0.490      4657\n",
      "weighted avg      1.000     0.959     0.979      4657\n",
      "\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00789523, 0.00725245, 0.00724173, 0.00719142, 0.00691056]), 'score_time': array([0.08726954, 0.00282788, 0.0028224 , 0.00283337, 0.00284362]), 'test_accuracy': array([0.95922747, 0.95922747, 0.95918367, 0.95918367, 0.95918367]), 'train_accuracy': array([0.95919463, 0.95919463, 0.95920558, 0.95920558, 0.95920558]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.733     0.846      9639\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.733      9639\n",
      "   macro avg      0.500     0.367     0.423      9639\n",
      "weighted avg      1.000     0.733     0.846      9639\n",
      "\n",
      "Current is process category 4\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 8575 and 8575\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4933 and 4933\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (8575, 144) (4933, 144)\n",
      "attension mask shape: (8575, 144) (4933, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 8575 8575\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.10385227, 0.10077977, 0.10330486, 0.10053229, 0.09783292]), 'score_time': array([0.00716662, 0.00681448, 0.09414649, 0.00748134, 0.09469104]), 'test_accuracy': array([0.85539359, 0.85539359, 0.85539359, 0.85539359, 0.85539359]), 'train_accuracy': array([0.85539359, 0.85539359, 0.85539359, 0.85539359, 0.85539359]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.892     0.943      4933\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.892      4933\n",
      "   macro avg      0.500     0.446     0.471      4933\n",
      "weighted avg      1.000     0.892     0.943      4933\n",
      "\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00792694, 0.00655079, 0.00676727, 0.00601816, 0.00577998]), 'score_time': array([0.00302744, 0.00305843, 0.0030055 , 0.00308609, 0.00307679]), 'test_accuracy': array([0.89159068, 0.89159068, 0.89159068, 0.89148073, 0.89148073]), 'train_accuracy': array([0.89153573, 0.89153573, 0.89153573, 0.89156321, 0.89156321]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.855     0.922      8575\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.855      8575\n",
      "   macro avg      0.500     0.428     0.461      8575\n",
      "weighted avg      1.000     0.855     0.922      8575\n",
      "\n",
      "Current is process category 5\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 9011 and 9011\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 5169 and 5169\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (9011, 144) (5169, 144)\n",
      "attension mask shape: (9011, 144) (5169, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 9011 9011\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.19540644, 0.10759521, 0.10380864, 0.09235048, 0.09608936]), 'score_time': array([0.09564948, 0.09491897, 0.00681043, 0.00703073, 0.0066762 ]), 'test_accuracy': array([0.80199667, 0.80188679, 0.80188679, 0.80188679, 0.80188679]), 'train_accuracy': array([0.80188679, 0.80191427, 0.80191427, 0.80191427, 0.80191427]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.839     0.913      5169\n",
      "           1      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.839      5169\n",
      "   macro avg      0.500     0.420     0.456      5169\n",
      "weighted avg      1.000     0.839     0.913      5169\n",
      "\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.09532046, 0.00749278, 0.00758719, 0.00730038, 0.00689912]), 'score_time': array([0.00319862, 0.00295997, 0.00299692, 0.00298953, 0.00294065]), 'test_accuracy': array([0.83945841, 0.83945841, 0.83945841, 0.83945841, 0.839303  ]), 'train_accuracy': array([0.83941959, 0.83941959, 0.83941959, 0.83941959, 0.83945841]), 'test_precision': array([0., 0., 0., 0., 0.]), 'train_precision': array([0., 0., 0., 0., 0.]), 'test_f1': array([0., 0., 0., 0., 0.]), 'train_f1': array([0., 0., 0., 0., 0.]), 'test_recall': array([0., 0., 0., 0., 0.]), 'train_recall': array([0., 0., 0., 0., 0.])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.802     0.890      9009\n",
      "           1      0.000     0.000     0.000         2\n",
      "\n",
      "    accuracy                          0.802      9011\n",
      "   macro avg      0.500     0.401     0.445      9011\n",
      "weighted avg      1.000     0.802     0.890      9011\n",
      "\n",
      "Current is process category 6\n",
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 8175 and 8175\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4697 and 4697\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (8175, 144) (4697, 144)\n",
      "attension mask shape: (8175, 144) (4697, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a_batch = [cv19_a_run_info_labeled, labels_single_a]\n",
    "b_batch = [cv19_b_run_info_labeled, labels_single_b]\n",
    "best_over_sample_factors = [24,44,7,4,4,4,1]\n",
    "\n",
    "for i in range(7):\n",
    "    section_category = i\n",
    "    increase_time = best_over_sample_factors[i]\n",
    "    Bert_with_lr_info_task(section_category, a_batch, b_batch, increase_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT LR priorization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bert_with_lr_priority_task( a_batch, b_batch, increase_time):\n",
    "    \n",
    "    # identify all data to use\n",
    "    a_all_data = a_batch[0]\n",
    "    b_all_data = b_batch[0]\n",
    "  \n",
    "    a_info_labels = a_batch[1]\n",
    "    b_info_labels = b_batch[1]\n",
    "\n",
    "    increaseTime = increase_time\n",
    "    category_number = 4\n",
    "\n",
    "    # info classification\n",
    "\n",
    "    from transformers import BertTokenizer\n",
    "\n",
    "    # Load the BERT tokenizer.\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    featureList = ['full_text','hashtags','favorite_count'] #,'favorited'\n",
    "    a_extracted = Process_labels.extract_features(a_all_data,featureList)\n",
    "    b_extracted = Process_labels.extract_features(b_all_data,featureList)\n",
    "\n",
    "    # Over-sample both data set\n",
    "    \n",
    "    # over-samle data set a\n",
    "    trainSet_temp = copy.deepcopy(a_extracted)\n",
    "    label_temp = copy.deepcopy(a_info_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 2)[0]\n",
    "\n",
    "    print('a: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(trainSet_temp['priority'][idx])\n",
    "\n",
    "    a_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    a_labels = label_temp\n",
    "    print('a: over-sample  length is %d and %d' %(len(a_extracted),len(label_temp)))\n",
    "\n",
    "    # Over-sample data set b\n",
    "    trainSet_temp = copy.deepcopy(b_extracted)\n",
    "    label_temp = copy.deepcopy(b_info_labels)\n",
    "    idxs = np.where(np.array(label_temp) == 2)[0]\n",
    "    print('b: Original length is %d and %d' %(len(trainSet_temp),len(label_temp)))\n",
    "    # iterate all sample belong to current processing category and duplicate all features\n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      # if category == 8 or category == 3:\n",
    "      #   break\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(trainSet_temp['priority'][idx])\n",
    "\n",
    "    b_extracted = pd.DataFrame({'full_text': trainSet_temp['full_text'], 'hashtags': trainSet_temp['hashtags'],\n",
    "                         'favorite_count': trainSet_temp['favorite_count'],})\n",
    "    b_labels = label_temp\n",
    "    print('b: over-sample  length is %d and %d' %(len(b_extracted),len(label_temp)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "    dataset_inputs_A_run = a_extracted\n",
    "    dataset_inputs_B_run = b_extracted\n",
    "    sentences_A_run = dataset_inputs_A_run.full_text.values\n",
    "    # labels_A_run = dataset_inputs_A_run.target.values\n",
    "    sentences_B_run = dataset_inputs_B_run.full_text.values\n",
    "    # labels_B_run = dataset_inputs_B_run.target.values\n",
    "    for idx in range(len(sentences_A_run)):\n",
    "      sentences_A_run[idx] = sentences_A_run[idx]  + ' '+ dataset_inputs_A_run.hashtags.values[idx] + ' ' + str(dataset_inputs_A_run.favorite_count.values[idx])\n",
    "    for idx in range(len(sentences_B_run)):\n",
    "      sentences_B_run[idx] = sentences_B_run[idx]  + ' '+ dataset_inputs_B_run.hashtags.values[idx] + ' ' + str(dataset_inputs_B_run.favorite_count.values[idx])\n",
    "\n",
    "    input_ids_A_run = Encode_sents(sentences_A_run)\n",
    "    input_ids_B_run = Encode_sents(sentences_B_run)\n",
    "\n",
    "    max_len = 144\n",
    "    padded_a = np.array([i+[0]*(max_len - len(i)) for i in input_ids_A_run])\n",
    "    padded_b = np.array([i+[0]*(max_len - len(i)) for i in input_ids_B_run])\n",
    "    print('padded shape',padded_a.shape,padded_b.shape)\n",
    "    attension_mask_a = np.where(padded_a != 0, 1, 0)\n",
    "    attension_mask_b = np.where(padded_b != 0, 1, 0)\n",
    "\n",
    "    print('attension mask shape:',attension_mask_a.shape, attension_mask_b.shape)\n",
    "    input_ids_A_run, input_ids_B_run = torch.tensor(padded_a), torch.tensor(padded_b)\n",
    "    attension_mask_a, attension_mask_b = torch.tensor(attension_mask_a), torch.tensor(attension_mask_b)\n",
    "    \n",
    "    # Identify GPU to use\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # initial BERT model\n",
    "    model  = BertForSequenceClassification.from_pretrained(\n",
    "      \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "      num_labels=category_number,  # The number of output labels--2 for binary classification.\n",
    "      # You can increase this for multi-class tasks.\n",
    "      # output_attentions=False,  # Whether the model returns attentions weights.\n",
    "      # output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # input_ids_A_run = input_ids_A_run.to(device)\n",
    "    # input_ids_B_run = input_ids_B_run.to(device)\n",
    "    # attension_mask_a = attension_mask_a.to(device)\n",
    "    # attension_mask_b = attension_mask_b.to(device)\n",
    "\n",
    "    features_a = []\n",
    "    for i in range(len(input_ids_A_run)):\n",
    "      input_id = input_ids_A_run[i].reshape(1,-1).to(device)\n",
    "      attension_mask = attension_mask_a[i].reshape(1,-1).to(device)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_state = model(input_id, attention_mask = attension_mask)\n",
    "      features_a.append(last_hidden_state[0][0,:].detach().cpu().numpy())\n",
    "      \n",
    "      del last_hidden_state, input_id\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    features_b = []\n",
    "    for i in range(len(input_ids_B_run)):\n",
    "      input_id = input_ids_B_run[i].reshape(1,-1).to(device)\n",
    "      attension_mask = attension_mask_b[i].reshape(1,-1).to(device)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_state = model(input_id, attention_mask = attension_mask)\n",
    "      features_b.append(last_hidden_state[0][0,:].detach().cpu().numpy())\n",
    "      \n",
    "      del last_hidden_state, input_id\n",
    "      torch.cuda.empty_cache()  \n",
    "\n",
    "    features_a = np.array(features_a)\n",
    "    features_b = np.array(features_b)\n",
    "\n",
    "    # Train on dataset-a and test on dataset-b\n",
    "    print('Length %d %d'%(len(input_ids_A_run), len(a_labels)))\n",
    "    print('Cross event validation')\n",
    "    # input_ids_A_run = np.array(input_ids_A_run)\n",
    "    # input_ids_B_run = np.array(input_ids_B_run)\n",
    "\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(features_a, a_labels)\n",
    "\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), features_a, a_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision_macro', 'f1_macro', 'recall_macro'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(features_b)\n",
    "    print(\"Test result on b:\")   \n",
    "    print(classification_report(predictions,  b_labels, digits=3))  \n",
    "\n",
    "    #  Train on dataset-b and test on dataset-a\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(features_b, b_labels)\n",
    "\n",
    "    print('Training result:')\n",
    "    scores = cross_validate(LogisticRegression(max_iter=300), features_b, b_labels, cv=5,\n",
    "                              scoring=['accuracy', 'precision_macro', 'f1_macro', 'recall_macro'], return_train_score=True)\n",
    "    \n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    predictions = lr.predict(features_a)\n",
    "    print(\"Test result on a:\")   \n",
    "    print(classification_report(predictions,  a_labels, digits=3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "a: Original length is 3 and 7583\n",
      "a: over-sample  length is 7583 and 7583\n",
      "b: Original length is 3 and 4505\n",
      "b: over-sample  length is 4505 and 4505\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT tokenizer...\n",
      "padded shape (7583, 144) (4505, 144)\n",
      "attension mask shape: (7583, 144) (4505, 144)\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 7583 7583\n",
      "Cross event validation\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.09771442, 0.08669591, 0.0886662 , 0.07449603, 0.09466934]), 'score_time': array([0.01829505, 0.01761651, 0.01716113, 0.0174849 , 0.01746249]), 'test_accuracy': array([0.92682927, 0.92617007, 0.92617007, 0.926781  , 0.926781  ]), 'train_accuracy': array([0.92647544, 0.92664029, 0.92664029, 0.92648756, 0.92648756]), 'test_precision_macro': array([0.23170732, 0.23154252, 0.23154252, 0.23169525, 0.23169525]), 'train_precision_macro': array([0.23161886, 0.23166007, 0.23166007, 0.23162189, 0.23162189]), 'test_f1_macro': array([0.24050633, 0.24041752, 0.24041752, 0.24049983, 0.24049983]), 'train_f1_macro': array([0.24045867, 0.24048088, 0.24048088, 0.2404603 , 0.2404603 ]), 'test_recall_macro': array([0.25, 0.25, 0.25, 0.25, 0.25]), 'train_recall_macro': array([0.25, 0.25, 0.25, 0.25, 0.25])}\n",
      "\n",
      "Test result on b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Critical      0.000     0.000     0.000         0\n",
      "        High      0.000     0.000     0.000         0\n",
      "         Low      1.000     0.777     0.875      4505\n",
      "      Medium      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.777      4505\n",
      "   macro avg      0.250     0.194     0.219      4505\n",
      "weighted avg      1.000     0.777     0.875      4505\n",
      "\n",
      "Training result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.06459379, 0.06033158, 0.06516051, 0.06964517, 0.0791862 ]), 'score_time': array([0.01100016, 0.01059437, 0.01052332, 0.01029754, 0.01030135]), 'test_accuracy': array([0.77802442, 0.77802442, 0.77580466, 0.77691454, 0.77691454]), 'train_accuracy': array([0.77719201, 0.77719201, 0.77774695, 0.77802442, 0.77774695]), 'test_precision_macro': array([0.1945061 , 0.1945061 , 0.19416667, 0.19422863, 0.19422863]), 'train_precision_macro': array([0.194298  , 0.194298  , 0.36112654, 0.38201389, 0.34449847]), 'test_f1_macro': array([0.21878901, 0.21878901, 0.2184375 , 0.21861337, 0.21861337]), 'train_f1_macro': array([0.2186573 , 0.2186573 , 0.22055375, 0.221492  , 0.2214431 ]), 'test_recall_macro': array([0.25      , 0.25      , 0.24964286, 0.25      , 0.25      ]), 'train_recall_macro': array([0.25      , 0.25      , 0.2508299 , 0.25128692, 0.2511977 ])}\n",
      "\n",
      "Test result on a:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Critical      0.000     0.000     0.000         0\n",
      "        High      0.000     0.000     0.000         0\n",
      "         Low      0.997     0.926     0.961      7564\n",
      "      Medium      0.000     0.000     0.000        19\n",
      "\n",
      "    accuracy                          0.924      7583\n",
      "   macro avg      0.249     0.232     0.240      7583\n",
      "weighted avg      0.995     0.924     0.958      7583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_batch = [cv19_a_run_info_labeled, a_priority_num_labels]\n",
    "b_batch = [cv19_b_run_info_labeled, b_priority_num_labels]\n",
    "\n",
    "best_over_sample_factor = 80\n",
    "Bert_with_lr_priority_task( a_batch, b_batch, best_over_sample_factor\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TfPNoqq5oaM"
   },
   "source": [
    "## Analysis the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "A = cv19_a_run_info_labeled.full_text.values\n",
    "test = Encode_sents(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = count_vect.fit_transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MB8T_FyLX6Fo"
   },
   "source": [
    "#### labels statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "g2YK1Qyq3rH7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "288c30a7-8a18-4f0b-9adf-e51b4416fd57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv19_a_run_info_labeled.priority.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "th3oA_aPNfnv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "923d41f8-4a0d-4c65-8b81-e8dab0a415a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv19_b_run_info_labeled.priority.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "JqMJZeS2-plA",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c433b315-8f1f-4f15-d803-3ac63de68423",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv19_dc.favorite_count.value_counts().nlargest(20).plot(kind='bar')\n",
    "plt.title('The number of favorite count')\n",
    "plt.xlabel('The times of favorite')\n",
    "plt.ylabel('The number of tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWhOLDKeTlSW",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = cv19_ws_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "JrmdwsLeDaCk",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7ac287f9-01c6-4531-ec54-8a754c7f937f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = -1\n",
    "for category in cv19_ws_labeled.categories.values:\n",
    "  index += 1\n",
    "  # print(len(category))\n",
    "  if len(category) == 12:\n",
    "    print(index)\n",
    "    print(category)\n",
    "    print(cv19_ws_labeled.iloc[index].id)\n",
    "    print(cv19_ws_labeled.iloc[index].full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YrXANet9MBIn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "665ccc60-8088-44bd-a88d-919c4c2270b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "types_statistics(cv19_dc_labeled,'DC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zpNcrIxMXUtd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "252107c4-599b-4e30-f8a2-97d7f17ab3f1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "types_statistics(cv19_ws_labeled,'WS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ELE6MDgtXVk9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "de9866f1-bf64-44db-badc-fd66dd1453a7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "types_statistics(cv19_ny_labeled,'NY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB9jDU3jVr44"
   },
   "source": [
    "# Classification on Information feed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ji7Iw6PgXXa"
   },
   "source": [
    "# Work on WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5f7Lkeuttro"
   },
   "source": [
    "#### Type categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rdvo1rACttro"
   },
   "source": [
    "In this task you will process a stream of tweets about the COVID-19 outbreak in different affected regions and you need to assign one or more of a reduced set of 9 information type labels and one priority label (Critical, High, Medium or Low) for each tweet. The information type categories are:\n",
    "\n",
    "* Request-GoodsServices\n",
    "* Request-InformationWanted\n",
    "* CallToAction-Volunteer\n",
    "* CallToAction-MovePeople\n",
    "* Report-EmergingThreats\n",
    "* Report-NewSubEvent\n",
    "* Report-ServiceAvailable\n",
    "* Other-Advice\n",
    "* Other-Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZilYUF9ttrw"
   },
   "source": [
    "#### Define the tokenizer and normalizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gLDsCUYPttrw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c38fea5f-530b-42c7-f0b7-a3108cdaa97d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprossesing\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Use Spacy as nlp tool\n",
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "# Do not remove 'not' as stopword\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Speed up the processing of Spacy\n",
    "disable=['ner']\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rhjsUBjfPsa"
   },
   "source": [
    "#### Tokenization and vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OP5Emy3uvEJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These three functions are learnt from course named Text as data\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if (not token.is_stop):\n",
    "        if ((token.is_alpha or token.is_digit or token.like_url)):\n",
    "            normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=25000,ngram_range=(1,2),sublinear_tf=(1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5j6ZKFYsmkem"
   },
   "source": [
    "#### Identify the dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv19_b_run_info_labeled = cv19_b_run_info_labeled.rename(columns={'full_text_x':'full_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stlwKUrjQRLm",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "# Tweets = cv19_a_run_info_labeled\n",
    "trainSet, testSet =  cv19_a_run_info_labeled ,  cv19_b_run_info_labeled\n",
    "# generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzMeqGU4oY9W"
   },
   "source": [
    "#### Build labels for multi-label multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FAJg3NSop-s",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nCs2ZETTpNqg",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9a4b53dd-9120-4155-cdca-ac9602b71447",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = Process_labels.extract_features(None,trainSet,featureList)\n",
    "testF = Process_labels.extract_features(None,testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = Process_labels.extract_features(None,trainSet,labelList)['categories']\n",
    "testL_temp = Process_labels.extract_features(None,testSet,labelList)['categories']\n",
    "\n",
    "trainF = Process_labels.extract_hashtags(None,trainF)\n",
    "testF = Process_labels.extract_hashtags(None,testF)\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMBSjMgSyJ-r"
   },
   "source": [
    "#### Feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5z87_cWyL-W",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ])\n",
    "\n",
    "\n",
    "fu.fit(trainF)\n",
    "train_features_union = fu.transform(trainF)\n",
    "test_features_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dju1htLRVvqW",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set data strcture to store later result of models\n",
    "precision_all = {}\n",
    "recall_all = {}\n",
    "f1_all = {}\n",
    "# One function to record all result\n",
    "def record_result(name,precision,recall,f1):\n",
    "    precision_all[name] = precision\n",
    "    recall_all[name] = recall\n",
    "    f1_all[name]= f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YMT3Z91gOrP"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "k09tORbOg2y-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d2146602-bacd-49ea-c3e7-3a1d3e33207b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5JbC3XOZSwz"
   },
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnDO-sLQXbK_",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffx2LHTUXieL"
   },
   "source": [
    "#### DC with feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "XX0NhDUC0Vda",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "52e71ab4-fb37-40aa-ee66-eba25f94e2fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "Classical_classifiers.DecisionTreeCf(None,runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bN4BBcHFXmkZ"
   },
   "source": [
    "#### DC without feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpXKilj-Rgb5",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_train = count_vect.fit_transform(trainF['full_text'])\n",
    "cv_test = count_vect.transform(testF['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "SrWAmq8OaB9p",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "126e53e6-df0e-4089-9fad-1e438ee5b56c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test,testL]\n",
    "DecisionTreeCf(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMvtg52teWuU"
   },
   "source": [
    "#### Grid Search on decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOooIvB3bje0",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pW5_oX5cU_o",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainF_tmp = []\n",
    "for i in range(4845):\n",
    "  trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                     'favorite_count':trainF['favorite_count'][i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ngAoB5kd4aN",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "#     ('selector', ItemSelector(key='reviewText')),\n",
    "    # Note: This keeps count weights, not one-hot; we'll tune this parameter next.\n",
    "    ('fu', fu),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# pipe.fit(trainF, trainL)\n",
    "\n",
    "# predictions = pipe.predict(valSetF_extract)\n",
    "\n",
    "# evaluation_summary(\"Decision Tree TF-IDF\", predictions, valL)                   \n",
    "\n",
    "params = {\n",
    "    # Fill in the parameter\n",
    "    \n",
    "    'fu__full_text__tf_idf__ngram_range':[(1,2),(1,3)],\n",
    "    'fu__full_text__tf_idf__sublinear_tf': (True, False),\n",
    "    'fu__full_text__tf_idf__max_features': np.linspace(10000,50000,5,dtype=int),\n",
    "    'dt__max_depth': (100,200,300,400,500),\n",
    "    'dt__class_weight': ['balanced','None'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "grid_search = GridSearchCV(pipe,params,verbose=10,cv=5, scoring='f1_samples', n_jobs=-1,pre_dispatch='2*n_jobs')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "print(\"parameters:\")\n",
    "print(params)\n",
    "#n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "# FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params.keys()):\n",
    "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2n_UHiXW0KjL",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Cwkro7jmUWM",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KizkWkoY4Vof"
   },
   "source": [
    "### build labels for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6gbzeh0G0FE9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "50582b92-571b-4477-8080-ff0f860e5b3f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = cv19_ws_labeled\n",
    "trainSet, testSet = generateTrainingSetCV(data)\n",
    "# Extract training set and test set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = extract_features(trainSet,featureList)\n",
    "testF = extract_features(testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = extract_features(trainSet,labelList)['categories']\n",
    "testL_temp = extract_features(testSet,labelList)['categories']\n",
    "\n",
    "trainF = extract_hashtags(trainF)\n",
    "testF = extract_hashtags(testF)\n",
    "\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "IdaFH2Fl4ZLi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "234893f6-dfd7-42ce-cb7a-9000ec32d948",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "# A dict Contains labels for all categories e.g. trainL_single[category]\n",
    "trainL_single = {}\n",
    "testL_single = {}\n",
    "for i in range(9):\n",
    "  print('Currrent is processing on categorie %s'%(categories[i]))\n",
    "  print()\n",
    "  # if i == 3:\n",
    "  #   continue\n",
    "  trainL_one = []\n",
    "  testL_one = []\n",
    "  \n",
    "  for j in range(len(trainL)):\n",
    "    trainL_one.append(trainL[j][i])\n",
    "\n",
    "    \n",
    "  for j in range(len(testL)):\n",
    "    testL_one.append(testL[j][i])\n",
    "    \n",
    "  trainL_single[i] = trainL_one\n",
    "  testL_single[i] = testL_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "t3ncSSwRjmWJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "36cc6410-5fa6-47bf-d8bc-423098faf519",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "  print(np.sum(np.array(trainL_single[i])==1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "cWwy4rhfiI9-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "94591cae-efd9-4374-b072-522df39be8b8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "  print(np.sum(np.array(testL_single[i])==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wxJWlcr8hF5"
   },
   "source": [
    "#### Manually add more positive class into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrUu93TYjZuj",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# build special test set for type ' GoodService'\n",
    "idxs_tmp = np.where(np.array(trainL_single[0])==1)\n",
    "\n",
    "for key in testF.keys():\n",
    "  testF[key].append(trainF[key][idxs_tmp[0][-1]])\n",
    "  trainF[key].pop(idxs_tmp[0][-1])\n",
    "for i in range(9):\n",
    "  if i ==0:\n",
    "    testL_single[i].append(1)\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n",
    "  else:\n",
    "    testL_single[i].append(trainL_single[i][idxs_tmp[0][-1]])\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwAnXY0FYKOn",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(np.array(testL_single[0])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg22yfIqE4Om",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = testF['full_text'][-1]\n",
    "count = 0\n",
    "for doc in trainF['full_text']:\n",
    "  count += 1\n",
    "  if doc == text:\n",
    "    print('Match',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFgOkjLgEhjm"
   },
   "source": [
    "#### build up-sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3-LMRil9Rzf",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'WS'\n",
    "for increaseTime in range(150):\n",
    "  for category in range(9):\n",
    "    build_upsample_set(trainF,trainL_single,location,category,increaseTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhYUIaGC8xnn",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "for increaseTime in range(150):\n",
    "  for category in range(9):\n",
    "    build_upsample_set(trainF,trainL_single,location,category,increaseTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0u-MdtY8z7t",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'DC'\n",
    "for increaseTime in range(150):\n",
    "  for category in range(9):\n",
    "    build_upsample_set(trainF,trainL_single,location,category,increaseTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFGR3sVb5LwM"
   },
   "source": [
    "#### Plot up-sample figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdE3uPdkuQYX",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "# build dict to store metrics\n",
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7YjB6HABUEi",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'WS'\n",
    "for increaseTime in range(0, 100):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in range(9):\n",
    "    print('Currrent is classifying categorie %s'%(categories[category]))\n",
    "    print()\n",
    "\n",
    "    if category == 4 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 5 and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    if category == 6 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 7 and increaseTime >= 4:\n",
    "      continue\n",
    "\n",
    "    if category == 3 or category == 8:\n",
    "      continue\n",
    "\n",
    "    train_features_upsample, train_label_upsample, testL_union_tmp = up_sample(location, category,increaseTime, testF)\n",
    "    # print('Before up-sample: %d %f After: %d %f' %(trainL_single[category].count(1), trainL_single[category].count(1)/len(trainL_single[category]),\n",
    "    #                            train_features_upsample.count(1), train_label_upsample[category].count(1)/len(train_label_upsample[category])))\n",
    "        \n",
    "    runningSet = [train_features_upsample,train_label_upsample,testL_union_tmp, testL_single]\n",
    "    lr_Binary(runningSet,category) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUQ3BCoBe26y",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample(records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend()\n",
    "  plt.savefig(str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QU-RASHg5sN",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'MovePeople' or category == 'Any':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample(train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample(val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample(test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnQo3JaJe6fJ"
   },
   "source": [
    "#### Search best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7M4c-Jbe6fJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gH-60Pt9e6fN",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'WS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpwhttSbamjU",
    "outputId": "9115e8cd-2e78-440a-d313-b488a1325a76",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search the best over-sample factor of lr model of each category\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR(30,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRIOqnikNvIf"
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uFik5OFBbxa",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "    \n",
    "fu = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                # ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "trainF_union  = fu.fit_transform(trainF)\n",
    "testF_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnZEqvmwmsNm"
   },
   "source": [
    "### For 'GoodsServices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "ty-1NZi8L1Pr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "403e9ecd-371d-4723-c44f-7c0f0711e8da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# build the training set for feature union and gridSerachCV\n",
    "# trainF_tmp_GS = []\n",
    "# for i in range(len(trainF['full_text'])):\n",
    "#   trainF_tmp_GS.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "#                           'favorite_count':trainF['favorite_count'][i]})\n",
    "location = 'WS'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_GS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_GS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=45050,ngram_range=(1,2),sublinear_tf=True)\n",
    "best_lr_GS = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=3065,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_GS = 0\n",
    "increaseTime_GS = 24\n",
    "\n",
    "best_lr_GS, pipe_GS, fu_GS, testF_union_GS, errors_idxs_GS, all_errors_GS = Classical_classifiers.build_best_lr(None,location,best_lr_GS,count_vect_GS,Tfidf_vect_GS, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hmx3xtMFqy2z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fae36a49-8036-4dfd-d89c-6960c9a5b8eb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_GS, default_pipe_GS, default_fu_GS, default_errors_idxs_GS, default_all_errors_GS = build_default_lr(location, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FK1b_vFgES19",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ec8d8cc-1b28-47f0-9d92-ef257759d027",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "vcH5yY_FLdwO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5705c9e9-d661-4762-a67e-f9ed58c1c10f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7rYOlZR6N9l_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "552e3b92-0fea-4ecb-c93d-76c80d054420",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_GS, feature_names=fu_GS.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "wEfUT96YOE-L",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13fd0c75-3550-484d-9a77-64b83f1b68da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[0])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[0]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GeGm-pjjOvYS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4ba2d205-8a98-418c-97b9-f0fe662f3305",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[1])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[1]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JZWb8_Ems73"
   },
   "source": [
    "### For 'InformationWanted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wHJYEQaIPCI9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6f757e80-4047-4fbf-e975-0611e25ac4e9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_IW = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_IW = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=22777,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_IW = LogisticRegression(C=10.101,class_weight='balanced',max_iter=3668,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_IW = 1\n",
    "increaseTime_IW = 44\n",
    "\n",
    "best_lr_IW, pipe_IW, fu_IW, testF_union_IW, errors_idxs_IW, all_errors_IW = build_best_lr(location,best_lr_IW,count_vect_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7CFgN89qsJHl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0896a144-9aba-444a-af5c-f11facc833f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_IW, default_pipe_IW, default_fu_IW, default_errors_idxs_IW, default_all_errors_IW = build_default_lr(location, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "bRtx3f4FW4Py",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "28013fc9-695d-4adb-c070-6f5d7d2c8997",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_IW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xu0R_c2ZPLfD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "63a13ac8-1c6d-482d-8510-63ca7c17348b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_IW, feature_names=fu_IW.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWnUbqknyI2b",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf,testF_transformed_IW = build_best_lr_without_featureunion(location,best_lr_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "Q-w9MN3jPPis",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3f12fa2e-5613-40a2-fd5c-6a9986e18ee6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[0])\n",
    "eli5.show_prediction(cf, testF_transformed_IW[errors_idxs_IW[0]], feature_names=Tfidf_vect_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3E7C0kt00-kU",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.explain_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "TIIwdX-4LDSO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a6a59bc-0a30-4b62-f724-169961cf9fea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[1])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[1]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "CPWzEwXyLDfP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e718c475-29c5-4a01-b2ce-9386febf05fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[2])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[2]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmPLECIZmtND"
   },
   "source": [
    "### For 'Volunteer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kBFeD7JPRTO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c699a9e0-ecfc-4529-fc48-3df35630ae69",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_V = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_V = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=9909,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_V = LogisticRegression(C=555.555556,class_weight='balanced',max_iter=3869,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_V = 2\n",
    "increaseTime_V = 7\n",
    "\n",
    "best_lr_V, pipe_V, fu_V, trainF_up_one_union_V, errors_idxs_V, all_errors_V = build_best_lr(best_lr_V,count_vect_V,Tfidf_vect_V, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GxQZwCFmvL8e",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c929ec9c-985e-4a37-f0aa-9bcdc6dad28a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_V, default_pipe_V, default_fu_V, default_trainF_up_one_union_V, default_errors_idxs_V, default_all_errors_V = build_default_lr(location, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xVH1-mDfY8kr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "44b0a6cb-69e2-43ec-fd51-f64c719b1700",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "K-m6SI20hasY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a8209def-4a27-4c8f-9cd1-60bcabb018ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "p-exXU9wPgCJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7318016a-ce9c-452a-c371-ff6a8d97b3f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union_V,trainL_up_one_V,testF_union_V,testL_single[category]]\n",
    "total_evaluation(best_lr_V,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yE-ly4PPPlHX",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5a148d7d-b907-474a-bd4b-7aee30d1dff8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_V, feature_names=fu_V.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "colab_type": "code",
    "id": "up-4fpwPPnzy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7f9a1b80-56a0-4b09-c4c8-109c6524bc45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[errors_idxs_V[0]], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "rrm28kqXRaOm",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20ea8754-3dee-4d58-c154-188642b4c5a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[1], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "id": "zizVi6obPpMw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ec53f3ad-3450-48eb-a885-71a71223a85e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[2], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJVntfLEVV0f"
   },
   "source": [
    "### For 'EmergingThreats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JF2v_NMbD3Ao",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e8a28f23-ccff-4a42-dc7d-4bebe053dfc4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_ET = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_ET = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=6444,ngram_range=(1,3),sublinear_tf=True)\n",
    "best_lr_ET = LogisticRegression(C=80.808,class_weight='balanced',max_iter=9296,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_ET = 4\n",
    "increaseTime_ET = 4\n",
    "\n",
    "best_lr_ET, pipe_ET, fu_ET, trainF_up_one_union_ET, errors_idxs_ET, all_errors_ET = build_best_lr(best_lr_ET,count_vect_ET,Tfidf_vect_ET, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kh5LaXTpnCvx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f2314c7d-3bd2-4772-b627-21a85ee366ed",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_ET, default_pipe_ET, default_fu_ET, default_trainF_up_one_union_ET, default_errors_idxs_ET, default_all_errors_ET = build_default_lr(location, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "G5lYAj6Gm2q4",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7e5e594-46e0-4051-d280-e7583473b5a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "7CklOWRhBuxl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b511f7e1-2cfb-4efc-e008-f29ad5da8403",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cQ7XfQiMXXCH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "05a2c863-5b57-4e3c-f23e-b5b627764453",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for idx in errors_idxs:\n",
    "  if count == 0:\n",
    "    break\n",
    "  count -= 1\n",
    "  eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfGiH2Smm0GI"
   },
   "source": [
    "### For 'NewSubEvent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LImFHk4tPuut",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5739c3e8-ac30-40a5-83ac-baf271925685",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_NS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_NS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=19808,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_NS = LogisticRegression(C=545,class_weight='balanced',max_iter=8994,penalty='l2',solver='liblinear')\n",
    "\n",
    "category_NS = 5\n",
    "increaseTime_NS = 4\n",
    "\n",
    "best_lr_NS, pipe_NS, fu_NS, trainF_up_one_union_NS, errors_idxs_NS, all_errors_NS = build_best_lr(best_lr_NS,count_vect_NS,Tfidf_vect_NS, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q6pNz_t3v2rd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a0de6b6b-8c91-4a0d-9657-c7a289b37172",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_NS, default_pipe_NS, default_fu_NS, default_trainF_up_one_union_NS, default_errors_idxs_NS, default_all_errors_NS = build_default_lr(location, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "5B3lnRreQFLb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "18c72528-e6fb-4909-9e28-568c819b6085",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "JIHb2yAbQGRY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8f326004-fd56-4091-ddff-2698269810ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M6MdXjYaQHEK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "49dac464-9b37-450a-95f8-82b6dd99bb93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YIWiLqVCQH0R",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "654e8f07-e318-42aa-ccd2-0a4e46d85b11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Ilflf75SQKHy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7742a86-30c8-4c56-a0bd-5a997588e379",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "kiZixRZmQNRi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fe9dd500-8203-4a5e-c02b-44801b055059",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyREbamkm5gR"
   },
   "source": [
    "### For 'ServiceAvailable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "U8aXQGCpQVu0",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1971c7d1-e357-461a-9d0a-754ccea7236e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_SA = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_SA = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=41585,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_SA = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=653,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_SA = 6\n",
    "increaseTime_SA = 4\n",
    "\n",
    "best_lr_SA, pipe_SA, fu_SA, trainF_up_one_union_SA, errors_idxs_SA, all_errors_SA = build_best_lr(best_lr_SA,count_vect_SA,Tfidf_vect_SA, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bIKz3vBqwMLn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "339a6f26-44ff-4fd7-a784-430e572266df",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_SA, default_pipe_SA, default_fu_SA, default_trainF_up_one_union_SA, default_errors_idxs_SA, default_all_errors_SA = build_default_lr(location, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "Rr1OOYdeQixl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e78c587b-bdae-4487-ffb8-600a9e93ab0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "UuIqSrZsQjlT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "87d63ec5-30f6-419d-e59a-cfe4779916d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vrz3ps9qQkYW",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c668fb28-6546-4be6-b346-a85b5f8267f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ocAiSEbBQlep",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67aecda0-1985-4093-e827-79060bd245db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "B7vCUq_DQma5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bbc8d81f-597c-4d61-f3fe-95caf49ff667",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "f0e-SINKQnUS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f397b776-8b2a-462d-9513-bb3109e01f44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6ElpoyIQoh4",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4YWStBNm6cn"
   },
   "source": [
    "### For 'Advice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a4eVHuzSQqap",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a08bae3b-f4a4-415c-d3ce-19213e3090d2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_A = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_A = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=49010,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_A = LogisticRegression(C=343.434,class_weight='balanced',max_iter=7185,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_A = 7\n",
    "increaseTime_A = 1\n",
    "\n",
    "best_lr_A, pipe_A, fu_A, trainF_up_one_union_A, errors_idxs_A, all_errors_A = build_best_lr(best_lr_A,count_vect_A,Tfidf_vect_A, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z7XJxctHxI8q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "00441eb8-efa2-4eb2-872d-7796c20ea8f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_A, default_pipe_A, default_fu_A, default_trainF_up_one_union_A, default_errors_idxs_A, default_all_errors_A = build_default_lr(location, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "i_83Wd6CRKPK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0c38552b-e176-4237-9473-1a6fef644843",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "Futw61V0RLJz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "90c80ba4-88fb-4872-9b04-eff549b8962d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TpaqgBoXRL7o",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "eaea242c-8df3-460e-9032-c88bd2919c75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OMSfazoURMnf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1758598d-1494-4798-80bc-99ad9c9032a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "VadR_oszRNXz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b51195e9-7f93-48c6-c20b-82fb32bc94b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "wcLeU-NsRPU-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f8489b0b-611a-43f0-f239-0fb3ef3d12ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6CjfUrIRQNR",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2maQxaDiEcY"
   },
   "source": [
    "# Work on NY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a1T7x8UiEcc"
   },
   "source": [
    "#### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WCdqSYsiEcc",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=25000,ngram_range=(1,2),sublinear_tf=(1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_MMIhAYiEcf"
   },
   "source": [
    "#### Identify the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvklaCiBiEcf",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "Tweets = cv19_ny_labeled\n",
    "trainSet, testSet = generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q8C6kA7iEcg"
   },
   "source": [
    "#### Build labels for multi-label multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xieGS9FiEcg",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wYFEzTbLiEch",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7c2fdec2-0c0e-473d-df57-ed84982f6f51",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = EF(trainSet,labelList)['categories']\n",
    "testL_temp = EF(testSet,labelList)['categories']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogOD5x6diEcj"
   },
   "source": [
    "#### Feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cI3i5HIeiEcj",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ],n_jobs=-1)\n",
    "\n",
    "\n",
    "fu.fit(trainF)\n",
    "train_features_union = fu.transform(trainF)\n",
    "test_features_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37DknauLiEcl"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "s1gQpv9RiEcl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "880426eb-695f-4914-eb17-a7e3848a6d8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJSDZWqbiEcm"
   },
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcNPI-deiEcm",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_h-EaDAiEcn"
   },
   "source": [
    "#### DC with feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "nBFezlGZiEco",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "19251a54-f5bc-4ee3-f315-554ea0b70a09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "DecisionTreeCf(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXWUYHsIiEcp"
   },
   "source": [
    "#### DC without feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFt-xuUwiEcp",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_train = count_vect.fit_transform(trainF['full_text'])\n",
    "cv_test = count_vect.transform(testF['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "GPigF1DsiEcq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d73be3bb-f6ef-440e-df70-536ff46ca4f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test,testL]\n",
    "DecisionTreeCf(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzF081HBiEcr"
   },
   "source": [
    "#### Grid Search on decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48F4DtXmiEcs",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uICDZCemiEct",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainF_tmp = []\n",
    "for i in range(4845):\n",
    "  trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                     'favorite_count':trainF['favorite_count'][i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yJZ-pUKiEct",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "#     ('selector', ItemSelector(key='reviewText')),\n",
    "    # Note: This keeps count weights, not one-hot; we'll tune this parameter next.\n",
    "    ('fu', fu),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# pipe.fit(trainF, trainL)\n",
    "\n",
    "# predictions = pipe.predict(valSetF_extract)\n",
    "\n",
    "# evaluation_summary(\"Decision Tree TF-IDF\", predictions, valL)                   \n",
    "\n",
    "params = {\n",
    "    # Fill in the parameter\n",
    "    \n",
    "    'fu__full_text__tf_idf__ngram_range':[(1,2),(1,3)],\n",
    "    'fu__full_text__tf_idf__sublinear_tf': (True, False),\n",
    "    'fu__full_text__tf_idf__max_features': np.linspace(10000,50000,5,dtype=int),\n",
    "    'dt__max_depth': (100,200,300,400,500),\n",
    "    'dt__class_weight': ['balanced','None'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "grid_search = GridSearchCV(pipe,params,verbose=10,cv=5, scoring='f1_samples', n_jobs=-1,pre_dispatch='2*n_jobs')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "print(\"parameters:\")\n",
    "print(params)\n",
    "#n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "# FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params.keys()):\n",
    "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THstWk0RiEcu",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzJnYMQtiEcv",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tsz5zZMtiEcw"
   },
   "source": [
    "### build labels for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iGU5R8NpiEcw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "945d84a1-0d21-4a33-9dea-a13fa37eed28",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = cv19_ny_labeled\n",
    "trainSet, testSet = generateTrainingSetCV(data)\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = EF(trainSet,labelList)['categories']\n",
    "testL_temp = EF(testSet,labelList)['categories']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)\n",
    "\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "eoEX9kQciEcy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "275f7d13-dd46-48da-b920-67cfb34577ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "# A dict Contains labels for all categories e.g. trainL_single[category]\n",
    "trainL_single = {}\n",
    "testL_single = {}\n",
    "for i in range(9):\n",
    "  print('Currrent is processing on categorie %s'%(categories[i]))\n",
    "  print()\n",
    "  # if i == 3:\n",
    "  #   continue\n",
    "  trainL_one = []\n",
    "  testL_one = []\n",
    "  \n",
    "  for j in range(len(trainL)):\n",
    "    trainL_one.append(trainL[j][i])\n",
    "\n",
    "    \n",
    "  for j in range(len(testL)):\n",
    "    testL_one.append(testL[j][i])\n",
    "    \n",
    "  trainL_single[i] = trainL_one\n",
    "  testL_single[i] = testL_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uwc4igdzqU2T",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5b028be6-049e-4a22-9925-22b39bfa7d7e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "for increaseTime in range(150):\n",
    "  for category in range(9):\n",
    "    build_upsample_set(trainF,trainL_single,location,category,increaseTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjpIaE2-iEc1"
   },
   "source": [
    "#### Manually add positive class into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3S6tljkyiEc1",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# build special test set for type ' GoodService'\n",
    "idxs_tmp = np.where(np.array(trainL_single[0])==1)\n",
    "\n",
    "for key in testF.keys():\n",
    "  testF[key].append(trainF[key][idxs_tmp[0][-1]])\n",
    "  trainF[key].pop(idxs_tmp[0][-1])\n",
    "for i in range(9):\n",
    "  if i ==0:\n",
    "    testL_single[i].append(1)\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n",
    "  else:\n",
    "    testL_single[i].append(trainL_single[i][idxs_tmp[0][-1]])\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNjhR2OfiEc2",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(np.array(testL_single[0])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta5HD-E_iEc4",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = testF['full_text'][-1]\n",
    "count = 0\n",
    "for doc in trainF['full_text']:\n",
    "  count += 1\n",
    "  if doc == text:\n",
    "    print('Match',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuJ8UDxWiEc9"
   },
   "source": [
    "#### Plot up-sample figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9t0ZLWmiEc9",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_AT8wabiEc-",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "\n",
    "\n",
    "for increaseTime in range(0, 100):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in range(9):\n",
    "    print('Currrent is classifying categorie %s'%(categories[category]))\n",
    "    print()\n",
    "\n",
    "    if category == 4 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 5 and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    if category == 6 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 7 and increaseTime >= 4:\n",
    "      continue\n",
    "\n",
    "    if category == 3 or category == 8:\n",
    "      continue\n",
    "\n",
    "    train_features_upsample, train_label_upsample, testL_union_tmp = up_sample(location, category,increaseTime, testF)\n",
    "    # print('Before up-sample: %d %f After: %d %f' %(trainL_single[category].count(1), trainL_single[category].count(1)/len(trainL_single[category]),\n",
    "    #                            train_features_upsample.count(1), train_label_upsample[category].count(1)/len(train_label_upsample[category])))\n",
    "        \n",
    "    runningSet = [train_features_upsample,train_label_upsample,testL_union_tmp, testL_single]\n",
    "    lr_Binary(runningSet,category) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azY1eqaQiEdA",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample(records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend()\n",
    "  plt.savefig(str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyBjlY0tiEdB",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'MovePeople' or category == 'Any':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample(train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample(val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample(test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BC5rX9mriEdC"
   },
   "source": [
    "#### Search best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgnEpkiYiEdC",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQmcA7OniEdD",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "    \n",
    "\n",
    "def search_best_lR(increaseTimes,location):\n",
    "  \n",
    "  \n",
    "  # iterate over a range of increaseTime and all category\n",
    "\n",
    "  # store the best parameter for each increaseTime\n",
    "  metrics_all = {'f1_best_all': defaultdict(),'best_paras': defaultdict()}\n",
    "  best_parameters_all = defaultdict()\n",
    "  cv_score_all = defaultdict()\n",
    "  test_scores_all = defaultdict()\n",
    "  for increaseTime in range(0,increaseTimes):\n",
    "    metrics_one_run = {'f1_best_each_cate': defaultdict()}\n",
    "    best_parameters_one_run = defaultdict()\n",
    "    cv_score_one_run = defaultdict()\n",
    "    print(\"Performing grid search on IncreaseTime %d\" %(increaseTime))\n",
    "    for category in range(9):\n",
    "      if category == 0: # and increaseTime != 24:\n",
    "        continue\n",
    "      if category == 1 and increaseTime != 44:\n",
    "        continue\n",
    "      if category == 2: # and increaseTime != 7:\n",
    "        continue\n",
    "      if category == 4: #and increaseTime != 3:\n",
    "        continue\n",
    "      if category == 5: # and increaseTime != 3:\n",
    "        continue\n",
    "      if category == 6: #and increaseTime != 4:\n",
    "        continue\n",
    "      if category == 7 :  #and increaseTime != 1:\n",
    "        continue\n",
    "      if category == 3 or category == 8:\n",
    "        continue\n",
    "\n",
    "      print(\"Performing grid search on category %s\" %(categories[category]))\n",
    "      df_tmp = pd.read_csv('cache/'+location+'/upsampleData_'+str(category)+'_'+str(increaseTime)+'.csv')\n",
    "      trainF = {'full_text':df_tmp['full_text'], 'hashtags': df_tmp['hashtags'], 'favorite_count': df_tmp['favorite_count']}\n",
    "      trainL = list(df_tmp['target'])\n",
    "\n",
    "      # build the training set for feature union and gridSerachCV\n",
    "      trainF_tmp = []\n",
    "      testF_tmp = []\n",
    "      for i in range(len(trainF['full_text'])):\n",
    "        trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                          'favorite_count':trainF['favorite_count'][i]})\n",
    "      for i in range(len(testF['full_text'])):\n",
    "        testF_tmp.append({'full_text':testF['full_text'][i],'hashtags':testF['hashtags'][i],\n",
    "                          'favorite_count':testF['favorite_count'][i]})\n",
    "      \n",
    "      fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ])\n",
    "      \n",
    "      fullpipe = Pipeline([\n",
    "\n",
    "                  ('fu', fu),\n",
    "                  ('lr', LogisticRegression())\n",
    "                 ])\n",
    "            \n",
    "      params = {\n",
    "          # Fill in the parameter\n",
    "          \n",
    "          'fu__full_text__tf_idf__ngram_range':[(1,2),(1,3),(1,4)],\n",
    "          'fu__full_text__tf_idf__sublinear_tf': (True, False),\n",
    "          'fu__full_text__tf_idf__max_features': np.linspace(1000,50000,100,dtype=int),\n",
    "          'lr__penalty': ['l2'],\n",
    "          'lr__C': np.linspace(0.000001,1000,100,dtype=float),\n",
    "          'lr__class_weight': ['balanced'],\n",
    "          'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "          'lr__max_iter': np.linspace(50,10000,100,dtype=int),\n",
    "      }\n",
    "\n",
    "      # Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "      grid_search = RandomizedSearchCV(fullpipe,params,verbose=1,cv=3,n_iter=350, scoring='f1', n_jobs=-1,pre_dispatch='2*n_jobs',)\n",
    "      \n",
    "      # print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "      # print(\"parameters:\")\n",
    "      # print(params)\n",
    "      #n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "      # FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "      grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "      # evaluate the test result\n",
    "      # testF_union_tmp = grid_search.transform(testF)\n",
    "\n",
    "      predictions = grid_search.predict(testF_tmp)\n",
    "      precision = precision_score(predictions, testL_single[category])\n",
    "      recall = recall_score(predictions, testL_single[category])\n",
    "      accuracy = accuracy_score(predictions, testL_single[category])\n",
    "      f1 = fbeta_score(predictions, testL_single[category],  1,)\n",
    "\n",
    "      test_scores = {'accuracy': accuracy,'precision': precision,'f1':f1,'recall': recall}\n",
    "      print(\"Test result:\", test_scores)\n",
    "\n",
    "      print('type')\n",
    "      print(type(trainL[0]))\n",
    "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "      # print(\"Best parameters set:\")\n",
    "      best_parameters = grid_search.best_estimator_.get_params()\n",
    "      parameters_dict = {}\n",
    "      for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        parameters_dict[param_name] = best_parameters[param_name]\n",
    "\n",
    "\n",
    "      metrics_one_run['f1_best_each_cate'][category] = grid_search.best_score_\n",
    "      cv_score_one_run[category] = grid_search.cv_results_\n",
    "      best_parameters_one_run[category] = parameters_dict\n",
    "      print(best_parameters_one_run[category])\n",
    "      test_scores_all[increaseTime] = {category: test_scores}\n",
    "\n",
    "    best_parameters_all[increaseTime] = best_parameters_one_run\n",
    "    metrics_all['f1_best_all'][increaseTime] = metrics_one_run\n",
    "    cv_score_all[increaseTime] = best_parameters_one_run\n",
    "  return metrics_all, best_parameters_all, cv_score_all\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3YjdtY_iEdE",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR(45,location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zxBfKOtiEdG",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7mBMua5iEdH",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_parameters_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6rmixDaiEdH"
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzvjaEX4iEdI",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "    \n",
    "fu = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                # ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "trainF_union  = fu.fit_transform(trainF)\n",
    "testF_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Vo4Mb-8iEdI",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_best_lr(location,cf, vect_count, vect_tfidf, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = {'full_text': df_tmp['full_text'], 'hashtags': df_tmp['hashtags'],\n",
    "                     'favorite_count': df_tmp['favorite_count']}\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    fu_best = FeatureUnion(\n",
    "        [('full_text', myPipeline([('selector', ItemSelector(key='full_text')), ('tf_idf', vect_tfidf),\n",
    "                                   ])),\n",
    "         ('hashtags', myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                                  ('cv', vect_count),\n",
    "                                  ])),\n",
    "         # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "         # ])),\n",
    "         ], n_jobs=-1)\n",
    "    fu_best.fit(trainF_up_one)\n",
    "    trainF_up_one_union = fu_best.transform(trainF_up_one)\n",
    "    trainF_union = fu_best.transform(trainF)\n",
    "    testF_union = fu_best.transform(testF)\n",
    "\n",
    "    pipe = Pipeline([('fu', fu_best), ('lr', cf)\n",
    "                     ])\n",
    "    pipe.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Display result without up-sampleing: \")\n",
    "    runningSet = [trainF_union, trainL_single[category], testF_union, testL_single[category]]\n",
    "    lr_priority(runningSet,cf)\n",
    "\n",
    "    print(\"Display result with up-sampleing: \")\n",
    "    model_name = \"The best LR result with featureUnion\"\n",
    "    # print(trainF_up_one_union.shape)\n",
    "    # print(len(trainL_up_one))\n",
    "\n",
    "    runningSet = [trainF_up_one_union, trainL_up_one, testF_union, testL_single[category]]\n",
    "    evaluate_one_lr(cf, runningSet, category)\n",
    "\n",
    "    predictions = cf.predict(testF_union)\n",
    "    errors_idxs = get_errors_indexs(testL_single[category], predictions, testF)\n",
    "    all_errors = get_errors(testL_single[category], predictions, testF)\n",
    "\n",
    "    return cf, pipe, fu_best, testF_union, errors_idxs, all_errors\n",
    "\n",
    "def build_default_lr(location, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    vect_count = CountVectorizer(tokenizer=tokenize_normalize, )\n",
    "    vect_tfidf = TfidfVectorizer(tokenizer=tokenize_normalize, )\n",
    "    cf = LogisticRegression()\n",
    "\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = {'full_text': df_tmp['full_text'], 'hashtags': df_tmp['hashtags'],\n",
    "                     'favorite_count': df_tmp['favorite_count']}\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    fu_best = FeatureUnion(\n",
    "        [('full_text', myPipeline([('selector', ItemSelector(key='full_text')), ('tf_idf', vect_tfidf),\n",
    "                                   ])),\n",
    "         ('hashtags', myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                                  ('cv', vect_count),\n",
    "                                  ])),\n",
    "         # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "         # ])),\n",
    "         ], n_jobs=-1)\n",
    "    fu_best.fit(trainF_up_one)\n",
    "    trainF_up_one_union = fu_best.transform(trainF_up_one)\n",
    "    trainF_union = fu_best.transform(trainF)\n",
    "    testF_union = fu_best.transform(testF)\n",
    "\n",
    "    pipe = Pipeline([('fu', fu_best), ('lr', cf)\n",
    "                     ])\n",
    "    pipe.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Display result without up-sampleing: \")\n",
    "    runningSet = [trainF_union, trainL_single[category], testF_union, testL_single[category]]\n",
    "    display_result_without_upsample(cf, category, runningSet)\n",
    "\n",
    "    print(\"Display result with up-sampleing: \")\n",
    "    model_name = \"The best LR result with featureUnion\"\n",
    "\n",
    "    runningSet = [trainF_up_one_union, trainL_up_one, testF_union, testL_single]\n",
    "    evaluate_one_lr(cf, runningSet, category)\n",
    "\n",
    "    predictions = cf.predict(testF_union)\n",
    "    errors_idxs = get_errors_indexs(testL_single[category], predictions, testF)\n",
    "    all_errors = get_errors(testL_single[category], predictions, testF)\n",
    "\n",
    "    return cf, pipe, fu_best,  errors_idxs, all_errors\n",
    "\n",
    "def print_all_featrues_of_one_sample(sample,idx):\n",
    "    for key in sample.keys():\n",
    "        print(key,sample[key][idx])\n",
    "\n",
    "def build_best_lr_without_featureunion(location,cf, vect_tfidf, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = df_tmp['full_text']\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    trainF_up_one= vect_tfidf.transform(trainF_up_one)\n",
    "    testF_transformed = vect_tfidf.transform(testF['full_text'])\n",
    "\n",
    "    cf.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "    return cf,testF_transformed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTfC0g5NiEdJ"
   },
   "source": [
    "### For 'GoodsServices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "5pSB_A5-iEdJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "00f7842e-4188-401e-dfcb-91090645e683",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# build the training set for feature union and gridSerachCV\n",
    "# trainF_tmp_GS = []\n",
    "# for i in range(len(trainF['full_text'])):\n",
    "#   trainF_tmp_GS.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "#                           'favorite_count':trainF['favorite_count'][i]})\n",
    "location = 'NY'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_GS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_GS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=45050,ngram_range=(1,2),sublinear_tf=True)\n",
    "best_lr_GS = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=3065,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_GS = 0\n",
    "increaseTime_GS = 24\n",
    "\n",
    "best_lr_GS, pipe_GS, fu_GS, testF_union_GS, errors_idxs_GS, all_errors_GS = build_best_lr(location,best_lr_GS,count_vect_GS,Tfidf_vect_GS, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MgqNscciEdK",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_lr_GS, default_pipe_GS, default_fu_GS, default_errors_idxs_GS, default_all_errors_GS = build_default_lr(location, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Aa9ldaW8iEdL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ec8d8cc-1b28-47f0-9d92-ef257759d027",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "fNbSzA2diEdM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5705c9e9-d661-4762-a67e-f9ed58c1c10f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SZ-ra6hSiEdN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "552e3b92-0fea-4ecb-c93d-76c80d054420",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_GS, feature_names=fu_GS.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "snVSNxv5iEdO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13fd0c75-3550-484d-9a77-64b83f1b68da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[0])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[0]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "ZJQ0A5zZiEdP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4ba2d205-8a98-418c-97b9-f0fe662f3305",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[1])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[1]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nG-AkoDoiEdP"
   },
   "source": [
    "### For 'InformationWanted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "DG8PGTzViEdQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "460b1bd8-487c-465b-fa0d-dff749e7420c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_IW = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_IW = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=22777,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_IW = LogisticRegression(C=10.101,class_weight='balanced',max_iter=3668,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_IW = 1\n",
    "increaseTime_IW = 44\n",
    "\n",
    "best_lr_IW, pipe_IW, fu_IW, testF_union_IW, errors_idxs_IW, all_errors_IW = build_best_lr(location,best_lr_IW,count_vect_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iAmSpWw1iEdQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0896a144-9aba-444a-af5c-f11facc833f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_IW, default_pipe_IW, default_fu_IW, default_errors_idxs_IW, default_all_errors_IW = build_default_lr(location, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "oHv282vuiEdT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "28013fc9-695d-4adb-c070-6f5d7d2c8997",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_IW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Duy98aUMiEdU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "63a13ac8-1c6d-482d-8510-63ca7c17348b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_IW, feature_names=fu_IW.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80Knwk8piEdV",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf,testF_transformed_IW = build_best_lr_without_featureunion(location,best_lr_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "Zev9erCkiEdW",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3f12fa2e-5613-40a2-fd5c-6a9986e18ee6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[0])\n",
    "eli5.show_prediction(cf, testF_transformed_IW[errors_idxs_IW[0]], feature_names=Tfidf_vect_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvmCLPK3iEdX",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.explain_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "zX0KbpJBiEdY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a6a59bc-0a30-4b62-f724-169961cf9fea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[1])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[1]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "-qxqlqAuiEdZ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e718c475-29c5-4a01-b2ce-9386febf05fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[2])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[2]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qnr6DE71iEdZ"
   },
   "source": [
    "### For 'Volunteer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "flZ03f_DiEda",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b7c591d-e95e-40a5-e3f7-dd3b0059d6ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_V = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_V = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=9909,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_V = LogisticRegression(C=555.555556,class_weight='balanced',max_iter=3869,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_V = 2\n",
    "increaseTime_V = 7\n",
    "\n",
    "best_lr_V, pipe_V, fu_V, trainF_up_one_union_V, errors_idxs_V, all_errors_V = build_best_lr(location,best_lr_V,count_vect_V,Tfidf_vect_V, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E7E6Tl__iEdb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c929ec9c-985e-4a37-f0aa-9bcdc6dad28a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_V, default_pipe_V, default_fu_V, default_trainF_up_one_union_V, default_errors_idxs_V, default_all_errors_V = build_default_lr(location, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iDyi5J8EiEdb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "44b0a6cb-69e2-43ec-fd51-f64c719b1700",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "jc9LigcAiEdc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a8209def-4a27-4c8f-9cd1-60bcabb018ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_xMpzJ5iEdd",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "3Wl2F6gNiEde",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "040d1e67-4adc-4561-931c-dabd802bfe6d",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "jHVNxkMPiEdf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7318016a-ce9c-452a-c371-ff6a8d97b3f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union_V,trainL_up_one_V,testF_union_V,testL_single[category]]\n",
    "total_evaluation(best_lr_V,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ll7lrkJuiEdf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5a148d7d-b907-474a-bd4b-7aee30d1dff8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_V, feature_names=fu_V.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "colab_type": "code",
    "id": "vh7T-AJ6iEdh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7f9a1b80-56a0-4b09-c4c8-109c6524bc45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[errors_idxs_V[0]], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "zyqrQeUSiEdi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20ea8754-3dee-4d58-c154-188642b4c5a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[1], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "id": "PeTs8I0xiEdj",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ec53f3ad-3450-48eb-a885-71a71223a85e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[2], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm8dE336iEdk"
   },
   "source": [
    "### For 'EmergingThreats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "jJ4O92fLiEdk",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bc69b6b0-f3f8-489e-8987-60d80435d0a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_ET = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_ET = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=6444,ngram_range=(1,3),sublinear_tf=True)\n",
    "best_lr_ET = LogisticRegression(C=80.808,class_weight='balanced',max_iter=9296,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_ET = 4\n",
    "increaseTime_ET = 4\n",
    "\n",
    "best_lr_ET, pipe_ET, fu_ET, trainF_up_one_union_ET, errors_idxs_ET, all_errors_ET = build_best_lr(location,best_lr_ET,count_vect_ET,Tfidf_vect_ET, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9YRdKYrniEdl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f2314c7d-3bd2-4772-b627-21a85ee366ed",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_ET, default_pipe_ET, default_fu_ET, default_trainF_up_one_union_ET, default_errors_idxs_ET, default_all_errors_ET = build_default_lr(location, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "PV68QzMgiEdm",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7e5e594-46e0-4051-d280-e7583473b5a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "qkA2wyq1iEdn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b511f7e1-2cfb-4efc-e008-f29ad5da8403",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2OWk-eE8iEdo",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "05a2c863-5b57-4e3c-f23e-b5b627764453",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for idx in errors_idxs:\n",
    "  if count == 0:\n",
    "    break\n",
    "  count -= 1\n",
    "  eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S79wZCHPiEdp"
   },
   "source": [
    "### For 'NewSubEvent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "LHjgW92QiEdp",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e27cc0bf-f561-4d26-8c8b-87b78d616cbb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_NS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_NS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=19808,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_NS = LogisticRegression(C=545,class_weight='balanced',max_iter=8994,penalty='l2',solver='liblinear')\n",
    "\n",
    "category_NS = 5\n",
    "increaseTime_NS = 4\n",
    "\n",
    "best_lr_NS, pipe_NS, fu_NS, trainF_up_one_union_NS, errors_idxs_NS, all_errors_NS = build_best_lr(location,best_lr_NS,count_vect_NS,Tfidf_vect_NS, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "moixPLtOiEdr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a0de6b6b-8c91-4a0d-9657-c7a289b37172",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_NS, default_pipe_NS, default_fu_NS, default_trainF_up_one_union_NS, default_errors_idxs_NS, default_all_errors_NS = build_default_lr(location, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "KhMsfEvMiEdr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "18c72528-e6fb-4909-9e28-568c819b6085",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "OgS-MnW6iEds",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8f326004-fd56-4091-ddff-2698269810ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jKdwqOvDiEdt",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "49dac464-9b37-450a-95f8-82b6dd99bb93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ANoZia20iEdu",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "654e8f07-e318-42aa-ccd2-0a4e46d85b11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "4LpIptJNiEdv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7742a86-30c8-4c56-a0bd-5a997588e379",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "xlMsRbB7iEdw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fe9dd500-8203-4a5e-c02b-44801b055059",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btaU1_eciEdx"
   },
   "source": [
    "### For 'ServiceAvailable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "psZ4o8PwiEdx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7729bfc4-15b1-405d-c9a6-9ad59b8c9399",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_SA = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_SA = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=41585,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_SA = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=653,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_SA = 6\n",
    "increaseTime_SA = 4\n",
    "\n",
    "best_lr_SA, pipe_SA, fu_SA, trainF_up_one_union_SA, errors_idxs_SA, all_errors_SA = build_best_lr(location,best_lr_SA,count_vect_SA,Tfidf_vect_SA, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "alOYxM6biEdy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "339a6f26-44ff-4fd7-a784-430e572266df",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_SA, default_pipe_SA, default_fu_SA, default_trainF_up_one_union_SA, default_errors_idxs_SA, default_all_errors_SA = build_default_lr(location, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "RQhO0d-KiEdy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e78c587b-bdae-4487-ffb8-600a9e93ab0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "axdJWg7GiEdz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "87d63ec5-30f6-419d-e59a-cfe4779916d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UMie7BNpiEd1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c668fb28-6546-4be6-b346-a85b5f8267f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2eX2SBuDiEd1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67aecda0-1985-4093-e827-79060bd245db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "q4U_GCm5iEd3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bbc8d81f-597c-4d61-f3fe-95caf49ff667",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "0MEA8NQxiEd4",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f397b776-8b2a-462d-9513-bb3109e01f44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKd-c7-TiEd5",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tk-1vTT0iEd5"
   },
   "source": [
    "### For 'Advice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "QUdQVDfSiEd6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "303db307-3140-4086-f7db-d9f95e94d9c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_A = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_A = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=49010,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_A = LogisticRegression(C=343.434,class_weight='balanced',max_iter=7185,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_A = 7\n",
    "increaseTime_A = 1\n",
    "\n",
    "best_lr_A, pipe_A, fu_A, trainF_up_one_union_A, errors_idxs_A, all_errors_A = build_best_lr(location,best_lr_A,count_vect_A,Tfidf_vect_A, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0C8kewx9iEd6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "00441eb8-efa2-4eb2-872d-7796c20ea8f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_A, default_pipe_A, default_fu_A, default_trainF_up_one_union_A, default_errors_idxs_A, default_all_errors_A = build_default_lr(location, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "8JBgZPKiiEd7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0c38552b-e176-4237-9473-1a6fef644843",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "dCUKmjdoiEd9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "90c80ba4-88fb-4872-9b04-eff549b8962d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5PzmQFz_iEeC",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "eaea242c-8df3-460e-9032-c88bd2919c75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4zO1DEcbiEeD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1758598d-1494-4798-80bc-99ad9c9032a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "hLuY4HBNiEeD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b51195e9-7f93-48c6-c20b-82fb32bc94b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "2KZ4GgaWiEeE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f8489b0b-611a-43f0-f239-0fb3ef3d12ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWEbPC_xiEeF",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVmSd1avt0zi"
   },
   "source": [
    "# Work on DC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVICBqS_t0zl"
   },
   "source": [
    "#### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHLpAMVjt0zl",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=25000,ngram_range=(1,2),sublinear_tf=(1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcijXHCdt0zn"
   },
   "source": [
    "#### Identify the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOsE5z0it0zo",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "Tweets = cv19_dc_labeled\n",
    "trainSet, testSet = generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uXBYf8bt0zp"
   },
   "source": [
    "#### Build labels for multi-label multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5H52yBokt0zp",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gpTC8u4Et0zq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d90697b7-20d8-40ee-ae28-da980905d6e3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = EF(trainSet,labelList)['categories']\n",
    "testL_temp = EF(testSet,labelList)['categories']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xD6iwymlt0zs"
   },
   "source": [
    "#### Feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX4lJ0PTt0zs",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ],n_jobs=-1)\n",
    "\n",
    "\n",
    "fu.fit(trainF)\n",
    "train_features_union = fu.transform(trainF)\n",
    "test_features_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-bLLvX8t0zt"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "C86z6l0Dt0zt",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "880426eb-695f-4914-eb17-a7e3848a6d8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrgz0Xgyt0zu"
   },
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-1xxOyqt0zv",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67AI_IkZt0zw"
   },
   "source": [
    "#### DC with feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "ktdV0eIZt0zw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "19251a54-f5bc-4ee3-f315-554ea0b70a09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [train_features_union,trainL, test_features_union,testL]\n",
    "DecisionTreeCf(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5utc8OHqt0zx"
   },
   "source": [
    "#### DC without feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5K0T1Klt0zx",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_train = count_vect.fit_transform(trainF['full_text'])\n",
    "cv_test = count_vect.transform(testF['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "67DadWfLt0zy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d73be3bb-f6ef-440e-df70-536ff46ca4f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test,testL]\n",
    "DecisionTreeCf(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbOaWEixt0zz"
   },
   "source": [
    "#### Grid Search on decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S57xvI6kt0z0",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "    ])),\n",
    "    ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "        ('cv',count_vect),\n",
    "    ])),\n",
    "    ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "    ])),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMNXIQgvt0z1",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainF_tmp = []\n",
    "for i in range(4845):\n",
    "  trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                     'favorite_count':trainF['favorite_count'][i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gq6eT2P9t0z2",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "#     ('selector', ItemSelector(key='reviewText')),\n",
    "    # Note: This keeps count weights, not one-hot; we'll tune this parameter next.\n",
    "    ('fu', fu),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# pipe.fit(trainF, trainL)\n",
    "\n",
    "# predictions = pipe.predict(valSetF_extract)\n",
    "\n",
    "# evaluation_summary(\"Decision Tree TF-IDF\", predictions, valL)                   \n",
    "\n",
    "params = {\n",
    "    # Fill in the parameter\n",
    "    \n",
    "    'fu__full_text__tf_idf__ngram_range':[(1,2),(1,3)],\n",
    "    'fu__full_text__tf_idf__sublinear_tf': (True, False),\n",
    "    'fu__full_text__tf_idf__max_features': np.linspace(10000,50000,5,dtype=int),\n",
    "    'dt__max_depth': (100,200,300,400,500),\n",
    "    'dt__class_weight': ['balanced','None'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "grid_search = GridSearchCV(pipe,params,verbose=10,cv=5, scoring='f1_samples', n_jobs=-1,pre_dispatch='2*n_jobs')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "print(\"parameters:\")\n",
    "print(params)\n",
    "#n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "# FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params.keys()):\n",
    "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-aj_1uot0z3",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkv6ACwXt0z4",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nx9l40-Ot0z5"
   },
   "source": [
    "### build labels for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "suS65J5Gt0z5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7b4ad899-5545-4b57-cbac-88a7daf0722e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = cv19_dc_labeled\n",
    "trainSet, testSet = generateTrainingSetCV(data)\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['categories']\n",
    "trainL_temp = EF(trainSet,labelList)['categories']\n",
    "testL_temp = EF(testSet,labelList)['categories']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)\n",
    "\n",
    "trainL = extractLabels_legacy(trainF,trainL_temp)\n",
    "testL = extractLabels_legacy(testF,testL_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "TN6US4HOt0z7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "57962400-f446-407a-9273-252f3b329f73",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "# A dict Contains labels for all categories e.g. trainL_single[category]\n",
    "trainL_single = {}\n",
    "testL_single = {}\n",
    "for i in range(9):\n",
    "  print('Currrent is processing on categorie %s'%(categories[i]))\n",
    "  print()\n",
    "  # if i == 3:\n",
    "  #   continue\n",
    "  trainL_one = []\n",
    "  testL_one = []\n",
    "  \n",
    "  for j in range(len(trainL)):\n",
    "    trainL_one.append(trainL[j][i])\n",
    "\n",
    "    \n",
    "  for j in range(len(testL)):\n",
    "    testL_one.append(testL[j][i])\n",
    "    \n",
    "  trainL_single[i] = trainL_one\n",
    "  testL_single[i] = testL_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Rn6jAd56t0z8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ec10eb20-abde-40dc-b079-77bf01b19836",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'DC'\n",
    "for increaseTime in range(150):\n",
    "  for category in range(9):\n",
    "    build_upsample_set(trainF,trainL_single,location,category,increaseTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ2sV7y4t0z9"
   },
   "source": [
    "#### Manually add positive class into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNFNuAr0t0z9",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# build special test set for type ' GoodService'\n",
    "idxs_tmp = np.where(np.array(trainL_single[0])==1)\n",
    "\n",
    "for key in testF.keys():\n",
    "  testF[key].append(trainF[key][idxs_tmp[0][-1]])\n",
    "  trainF[key].pop(idxs_tmp[0][-1])\n",
    "for i in range(9):\n",
    "  if i ==0:\n",
    "    testL_single[i].append(1)\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n",
    "  else:\n",
    "    testL_single[i].append(trainL_single[i][idxs_tmp[0][-1]])\n",
    "    trainL_single[i].pop(idxs_tmp[0][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7G34QrD8t0z-",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(np.array(testL_single[0])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMPfnU7dt0z_",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = testF['full_text'][-1]\n",
    "count = 0\n",
    "for doc in trainF['full_text']:\n",
    "  count += 1\n",
    "  if doc == text:\n",
    "    print('Match',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHg06OmFt00A"
   },
   "source": [
    "#### Plot up-sample figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rYVEMCqt00A",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4is1gm_t00B",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "\n",
    "\n",
    "for increaseTime in range(0, 100):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in range(9):\n",
    "    print('Currrent is classifying categorie %s'%(categories[category]))\n",
    "    print()\n",
    "\n",
    "    if category == 4 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 5 and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    if category == 6 and increaseTime >= 6:\n",
    "      continue\n",
    "\n",
    "    if category == 7 and increaseTime >= 4:\n",
    "      continue\n",
    "\n",
    "    if category == 3 or category == 8:\n",
    "      continue\n",
    "\n",
    "    train_features_upsample, train_label_upsample, testL_union_tmp = up_sample(location, category,increaseTime, testF)\n",
    "    # print('Before up-sample: %d %f After: %d %f' %(trainL_single[category].count(1), trainL_single[category].count(1)/len(trainL_single[category]),\n",
    "    #                            train_features_upsample.count(1), train_label_upsample[category].count(1)/len(train_label_upsample[category])))\n",
    "        \n",
    "    runningSet = [train_features_upsample,train_label_upsample,testL_union_tmp, testL_single]\n",
    "    lr_Binary(runningSet,category) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-nEwzIot00C",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample(records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend()\n",
    "  plt.savefig(str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K_ffdagt00D",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'MovePeople' or category == 'Any':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample(train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample(val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample(test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B41iNn2lt00E"
   },
   "source": [
    "#### Search best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5PSGyGXt00E",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['GoodsServices','InformationWanted','Volunteer','MovePeople','EmergingThreats','NewSubEvent','ServiceAvailable','Advice','Any']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-yUa9uyt00F",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "    \n",
    "\n",
    "def search_best_lR(increaseTimes,location):\n",
    "  \n",
    "  \n",
    "  # iterate over a range of increaseTime and all category\n",
    "\n",
    "  # store the best parameter for each increaseTime\n",
    "  metrics_all = {'f1_best_all': defaultdict(),'best_paras': defaultdict()}\n",
    "  best_parameters_all = defaultdict()\n",
    "  cv_score_all = defaultdict()\n",
    "  test_scores_all = defaultdict()\n",
    "  for increaseTime in range(0,increaseTimes):\n",
    "    metrics_one_run = {'f1_best_each_cate': defaultdict()}\n",
    "    best_parameters_one_run = defaultdict()\n",
    "    cv_score_one_run = defaultdict()\n",
    "    print(\"Performing grid search on IncreaseTime %d\" %(increaseTime))\n",
    "    for category in range(9):\n",
    "      if category == 0: # and increaseTime != 24:\n",
    "        continue\n",
    "      if category == 1 and increaseTime != 44:\n",
    "        continue\n",
    "      if category == 2: # and increaseTime != 7:\n",
    "        continue\n",
    "      if category == 4: #and increaseTime != 3:\n",
    "        continue\n",
    "      if category == 5: # and increaseTime != 3:\n",
    "        continue\n",
    "      if category == 6: #and increaseTime != 4:\n",
    "        continue\n",
    "      if category == 7 :  #and increaseTime != 1:\n",
    "        continue\n",
    "      if category == 3 or category == 8:\n",
    "        continue\n",
    "\n",
    "      print(\"Performing grid search on category %s\" %(categories[category]))\n",
    "      df_tmp = pd.read_csv('cache/'+location+'/upsampleData_'+str(category)+'_'+str(increaseTime)+'.csv')\n",
    "      trainF = {'full_text':df_tmp['full_text'], 'hashtags': df_tmp['hashtags'], 'favorite_count': df_tmp['favorite_count']}\n",
    "      trainL = list(df_tmp['target'])\n",
    "\n",
    "      # build the training set for feature union and gridSerachCV\n",
    "      trainF_tmp = []\n",
    "      testF_tmp = []\n",
    "      for i in range(len(trainF['full_text'])):\n",
    "        trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                          'favorite_count':trainF['favorite_count'][i]})\n",
    "      for i in range(len(testF['full_text'])):\n",
    "        testF_tmp.append({'full_text':testF['full_text'][i],'hashtags':testF['hashtags'][i],\n",
    "                          'favorite_count':testF['favorite_count'][i]})\n",
    "      \n",
    "      fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',Pipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',Pipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ])\n",
    "      \n",
    "      fullpipe = Pipeline([\n",
    "\n",
    "                  ('fu', fu),\n",
    "                  ('lr', LogisticRegression())\n",
    "                 ])\n",
    "            \n",
    "      params = {\n",
    "          # Fill in the parameter\n",
    "          \n",
    "          'fu__full_text__tf_idf__ngram_range':[(1,2),(1,3),(1,4)],\n",
    "          'fu__full_text__tf_idf__sublinear_tf': (True, False),\n",
    "          'fu__full_text__tf_idf__max_features': np.linspace(1000,50000,100,dtype=int),\n",
    "          'lr__penalty': ['l2'],\n",
    "          'lr__C': np.linspace(0.000001,1000,100,dtype=float),\n",
    "          'lr__class_weight': ['balanced'],\n",
    "          'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "          'lr__max_iter': np.linspace(50,10000,100,dtype=int),\n",
    "      }\n",
    "\n",
    "      # Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "      grid_search = RandomizedSearchCV(fullpipe,params,verbose=1,cv=3,n_iter=350, scoring='f1', n_jobs=-1,pre_dispatch='2*n_jobs',)\n",
    "      \n",
    "      # print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "      # print(\"parameters:\")\n",
    "      # print(params)\n",
    "      #n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "      # FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "      grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "      # evaluate the test result\n",
    "      # testF_union_tmp = grid_search.transform(testF)\n",
    "\n",
    "      predictions = grid_search.predict(testF_tmp)\n",
    "      precision = precision_score(predictions, testL_single[category])\n",
    "      recall = recall_score(predictions, testL_single[category])\n",
    "      accuracy = accuracy_score(predictions, testL_single[category])\n",
    "      f1 = fbeta_score(predictions, testL_single[category],  1,)\n",
    "\n",
    "      test_scores = {'accuracy': accuracy,'precision': precision,'f1':f1,'recall': recall}\n",
    "      print(\"Test result:\", test_scores)\n",
    "\n",
    "      print('type')\n",
    "      print(type(trainL[0]))\n",
    "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "      # print(\"Best parameters set:\")\n",
    "      best_parameters = grid_search.best_estimator_.get_params()\n",
    "      parameters_dict = {}\n",
    "      for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        parameters_dict[param_name] = best_parameters[param_name]\n",
    "\n",
    "\n",
    "      metrics_one_run['f1_best_each_cate'][category] = grid_search.best_score_\n",
    "      cv_score_one_run[category] = grid_search.cv_results_\n",
    "      best_parameters_one_run[category] = parameters_dict\n",
    "      print(best_parameters_one_run[category])\n",
    "      test_scores_all[increaseTime] = {category: test_scores}\n",
    "\n",
    "    best_parameters_all[increaseTime] = best_parameters_one_run\n",
    "    metrics_all['f1_best_all'][increaseTime] = metrics_one_run\n",
    "    cv_score_all[increaseTime] = best_parameters_one_run\n",
    "  return metrics_all, best_parameters_all, cv_score_all\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAvQ7cpKt00G",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR(45,location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQGx8MAYt00H",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMH0KFzAt00I",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_parameters_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihLp1kJAt00K"
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJ4FGsnAt00K",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "    \n",
    "fu = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                # ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "trainF_union  = fu.fit_transform(trainF)\n",
    "testF_union = fu.transform(testF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTQMlm2Rt00M",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_best_lr(location,cf, vect_count, vect_tfidf, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = {'full_text': df_tmp['full_text'], 'hashtags': df_tmp['hashtags'],\n",
    "                     'favorite_count': df_tmp['favorite_count']}\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    fu_best = FeatureUnion(\n",
    "        [('full_text', myPipeline([('selector', ItemSelector(key='full_text')), ('tf_idf', vect_tfidf),\n",
    "                                   ])),\n",
    "         ('hashtags', myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                                  ('cv', vect_count),\n",
    "                                  ])),\n",
    "         # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "         # ])),\n",
    "         ], n_jobs=-1)\n",
    "    fu_best.fit(trainF_up_one)\n",
    "    trainF_up_one_union = fu_best.transform(trainF_up_one)\n",
    "    trainF_union = fu_best.transform(trainF)\n",
    "    testF_union = fu_best.transform(testF)\n",
    "\n",
    "    pipe = Pipeline([('fu', fu_best), ('lr', cf)\n",
    "                     ])\n",
    "    pipe.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Display result without up-sampleing: \")\n",
    "    runningSet = [trainF_union, trainL_single[category], testF_union, testL_single[category]]\n",
    "    lr_priority(runningSet,cf)\n",
    "\n",
    "    print(\"Display result with up-sampleing: \")\n",
    "    model_name = \"The best LR result with featureUnion\"\n",
    "    # print(trainF_up_one_union.shape)\n",
    "    # print(len(trainL_up_one))\n",
    "\n",
    "    runningSet = [trainF_up_one_union, trainL_up_one, testF_union, testL_single[category]]\n",
    "    evaluate_one_lr(cf, runningSet, category)\n",
    "\n",
    "    predictions = cf.predict(testF_union)\n",
    "    errors_idxs = get_errors_indexs(testL_single[category], predictions, testF)\n",
    "    all_errors = get_errors(testL_single[category], predictions, testF)\n",
    "\n",
    "    return cf, pipe, fu_best, testF_union, errors_idxs, all_errors\n",
    "\n",
    "def build_default_lr(location, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    vect_count = CountVectorizer(tokenizer=tokenize_normalize, )\n",
    "    vect_tfidf = TfidfVectorizer(tokenizer=tokenize_normalize, )\n",
    "    cf = LogisticRegression()\n",
    "\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = {'full_text': df_tmp['full_text'], 'hashtags': df_tmp['hashtags'],\n",
    "                     'favorite_count': df_tmp['favorite_count']}\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    fu_best = FeatureUnion(\n",
    "        [('full_text', myPipeline([('selector', ItemSelector(key='full_text')), ('tf_idf', vect_tfidf),\n",
    "                                   ])),\n",
    "         ('hashtags', myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                                  ('cv', vect_count),\n",
    "                                  ])),\n",
    "         # ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "         # ])),\n",
    "         ], n_jobs=-1)\n",
    "    fu_best.fit(trainF_up_one)\n",
    "    trainF_up_one_union = fu_best.transform(trainF_up_one)\n",
    "    trainF_union = fu_best.transform(trainF)\n",
    "    testF_union = fu_best.transform(testF)\n",
    "\n",
    "    pipe = Pipeline([('fu', fu_best), ('lr', cf)\n",
    "                     ])\n",
    "    pipe.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Display result without up-sampleing: \")\n",
    "    runningSet = [trainF_union, trainL_single[category], testF_union, testL_single[category]]\n",
    "    display_result_without_upsample(cf, category, runningSet)\n",
    "\n",
    "    print(\"Display result with up-sampleing: \")\n",
    "    model_name = \"The best LR result with featureUnion\"\n",
    "\n",
    "    runningSet = [trainF_up_one_union, trainL_up_one, testF_union, testL_single]\n",
    "    evaluate_one_lr(cf, runningSet, category)\n",
    "\n",
    "    predictions = cf.predict(testF_union)\n",
    "    errors_idxs = get_errors_indexs(testL_single[category], predictions, testF)\n",
    "    all_errors = get_errors(testL_single[category], predictions, testF)\n",
    "\n",
    "    return cf, pipe, fu_best,  errors_idxs, all_errors\n",
    "\n",
    "def print_all_featrues_of_one_sample(sample,idx):\n",
    "    for key in sample.keys():\n",
    "        print(key,sample[key][idx])\n",
    "\n",
    "def build_best_lr_without_featureunion(location,cf, vect_tfidf, category, increaseTime, trainF, trainL_single, testF, testL_single):\n",
    "    df_tmp = pd.read_csv('cache/' + location + '/upsampleData_' + str(category) + '_' + str(increaseTime) + '.csv')\n",
    "    trainF_up_one = df_tmp['full_text']\n",
    "    trainL_up_one = list(df_tmp['target'])\n",
    "\n",
    "    trainF_up_one= vect_tfidf.transform(trainF_up_one)\n",
    "    testF_transformed = vect_tfidf.transform(testF['full_text'])\n",
    "\n",
    "    cf.fit(trainF_up_one, trainL_up_one)\n",
    "\n",
    "    return cf,testF_transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fi2yx07Ox6sh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "91fc4600-6f7c-4a9d-d9c2-d067e6d1059d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(np.array(trainL_single[1])==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQh4tFrkt00N"
   },
   "source": [
    "### For 'GoodsServices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4UHoVO4t00N",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# build the training set for feature union and gridSerachCV\n",
    "# trainF_tmp_GS = []\n",
    "# for i in range(len(trainF['full_text'])):\n",
    "#   trainF_tmp_GS.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "#                           'favorite_count':trainF['favorite_count'][i]})\n",
    "location = 'DC'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_GS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_GS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=45050,ngram_range=(1,2),sublinear_tf=True)\n",
    "best_lr_GS = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=3065,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_GS = 0\n",
    "increaseTime_GS = 24\n",
    "\n",
    "best_lr_GS, pipe_GS, fu_GS, testF_union_GS, errors_idxs_GS, all_errors_GS = build_best_lr(location,best_lr_GS,count_vect_GS,Tfidf_vect_GS, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpBh4V6pt00O",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_lr_GS, default_pipe_GS, default_fu_GS, default_errors_idxs_GS, default_all_errors_GS = build_default_lr(location, category_GS,increaseTime_GS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JU2o-cORt00P",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ec8d8cc-1b28-47f0-9d92-ef257759d027",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "6vOU0VrIt00R",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5705c9e9-d661-4762-a67e-f9ed58c1c10f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vZa0106It00S",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "552e3b92-0fea-4ecb-c93d-76c80d054420",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_GS, feature_names=fu_GS.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "3k3JDYTGt00T",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13fd0c75-3550-484d-9a77-64b83f1b68da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[0])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[0]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GK6n_BJat00U",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4ba2d205-8a98-418c-97b9-f0fe662f3305",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_GS[1])\n",
    "eli5.show_prediction(best_lr_GS,testF_union_GS[errors_idxs_GS[1]], feature_names=fu_GS.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lssqJsAdt00W"
   },
   "source": [
    "### For 'InformationWanted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "v1q2bMwgt00W",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "44f87d3f-a55e-40d7-f4eb-bfa50d58a689",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_IW = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_IW = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=22777,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_IW = LogisticRegression(C=10.101,class_weight='balanced',max_iter=3668,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_IW = 1\n",
    "increaseTime_IW = 44\n",
    "\n",
    "best_lr_IW, pipe_IW, fu_IW, testF_union_IW, errors_idxs_IW, all_errors_IW = build_best_lr(location,best_lr_IW,count_vect_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iz5zJyjjt00X",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0896a144-9aba-444a-af5c-f11facc833f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_IW, default_pipe_IW, default_fu_IW, default_errors_idxs_IW, default_all_errors_IW = build_default_lr(location, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "L43irSutt00Y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "28013fc9-695d-4adb-c070-6f5d7d2c8997",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_IW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NU84Dz8Xt00Z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "63a13ac8-1c6d-482d-8510-63ca7c17348b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_IW, feature_names=fu_IW.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbULrRgut00b",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf,testF_transformed_IW = build_best_lr_without_featureunion(location,best_lr_IW,Tfidf_vect_IW, category_IW,increaseTime_IW, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "bnrbql14t00c",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3f12fa2e-5613-40a2-fd5c-6a9986e18ee6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[0])\n",
    "eli5.show_prediction(cf, testF_transformed_IW[errors_idxs_IW[0]], feature_names=Tfidf_vect_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDTUh09st00d",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.explain_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "UeXbcauNt00e",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a6a59bc-0a30-4b62-f724-169961cf9fea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[1])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[1]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "3MYLIivgt00f",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e718c475-29c5-4a01-b2ce-9386febf05fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_all_featrues_of_one_sample(testF, errors_idxs_IW[2])\n",
    "eli5.show_prediction(best_lr_IW,testF_union_IW[errors_idxs_IW[2]], feature_names=fu_IW.get_feature_names(), top=(50,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--DL7HISt00g"
   },
   "source": [
    "### For 'Volunteer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "s-bGwiiMt00g",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a93eb6ff-1f6c-45bb-8f22-98d43c08cdf7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_V = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_V = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=9909,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_V = LogisticRegression(C=555.555556,class_weight='balanced',max_iter=3869,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_V = 2\n",
    "increaseTime_V = 7\n",
    "\n",
    "best_lr_V, pipe_V, fu_V, trainF_up_one_union_V, errors_idxs_V, all_errors_V = build_best_lr(location,best_lr_V,count_vect_V,Tfidf_vect_V, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Kuw5Khxdt00h",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c929ec9c-985e-4a37-f0aa-9bcdc6dad28a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_V, default_pipe_V, default_fu_V, default_trainF_up_one_union_V, default_errors_idxs_V, default_all_errors_V = build_default_lr(location, category_V,increaseTime_V, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jPieW0Slt00k",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "44b0a6cb-69e2-43ec-fd51-f64c719b1700",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_idxs_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "LmDAdLU_t00m",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a8209def-4a27-4c8f-9cd1-60bcabb018ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tueaj5tlt00n",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "BnNtxXvmt00o",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "040d1e67-4adc-4561-931c-dabd802bfe6d",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "zonUQ2gCt00p",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7318016a-ce9c-452a-c371-ff6a8d97b3f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union_V,trainL_up_one_V,testF_union_V,testL_single[category]]\n",
    "total_evaluation(best_lr_V,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NWZOpFBft00p",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5a148d7d-b907-474a-bd4b-7aee30d1dff8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr_V, feature_names=fu_V.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "colab_type": "code",
    "id": "UsnUVidot00q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7f9a1b80-56a0-4b09-c4c8-109c6524bc45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[errors_idxs_V[0]], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "VQj7l2IAt00s",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20ea8754-3dee-4d58-c154-188642b4c5a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[1], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "id": "KJTnp-i3t00s",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ec53f3ad-3450-48eb-a885-71a71223a85e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr_V,trainF_up_one_union_V[2], feature_names=fu_V.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVilPOt4t00t"
   },
   "source": [
    "### For 'EmergingThreats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "oGCXj1kwt00u",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "701ae809-3841-46da-b465-c3b1cb5b8fb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_ET = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_ET = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=6444,ngram_range=(1,3),sublinear_tf=True)\n",
    "best_lr_ET = LogisticRegression(C=80.808,class_weight='balanced',max_iter=9296,penalty='l2',solver='newton-cg')\n",
    "\n",
    "category_ET = 4\n",
    "increaseTime_ET = 4\n",
    "\n",
    "best_lr_ET, pipe_ET, fu_ET, trainF_up_one_union_ET, errors_idxs_ET, all_errors_ET = build_best_lr(location,best_lr_ET,count_vect_ET,Tfidf_vect_ET, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FCVezFQjt00u",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f2314c7d-3bd2-4772-b627-21a85ee366ed",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_ET, default_pipe_ET, default_fu_ET, default_trainF_up_one_union_ET, default_errors_idxs_ET, default_all_errors_ET = build_default_lr(location, category_ET,increaseTime_ET, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "t8JYlJxWt00w",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7e5e594-46e0-4051-d280-e7583473b5a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "ICafEobrt00x",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b511f7e1-2cfb-4efc-e008-f29ad5da8403",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AxFDg9USt00y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "05a2c863-5b57-4e3c-f23e-b5b627764453",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for idx in errors_idxs:\n",
    "  if count == 0:\n",
    "    break\n",
    "  count -= 1\n",
    "  eli5.show_prediction(best_lr,trainF_up_one_union[idx], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyBZfPg8t00z"
   },
   "source": [
    "### For 'NewSubEvent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "MSHqhV10t00z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "710309b4-3359-4883-cac5-a777fab84887",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_NS = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_NS = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=19808,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_NS = LogisticRegression(C=545,class_weight='balanced',max_iter=8994,penalty='l2',solver='liblinear')\n",
    "\n",
    "category_NS = 5\n",
    "increaseTime_NS = 4\n",
    "\n",
    "best_lr_NS, pipe_NS, fu_NS, trainF_up_one_union_NS, errors_idxs_NS, all_errors_NS = build_best_lr(location,best_lr_NS,count_vect_NS,Tfidf_vect_NS, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wW2dZ9Omt001",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a0de6b6b-8c91-4a0d-9657-c7a289b37172",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_NS, default_pipe_NS, default_fu_NS, default_trainF_up_one_union_NS, default_errors_idxs_NS, default_all_errors_NS = build_default_lr(location, category_NS,increaseTime_NS, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "bw9tsY0pt002",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "18c72528-e6fb-4909-9e28-568c819b6085",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "Dfbxbzupt002",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8f326004-fd56-4091-ddff-2698269810ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H18cLLtot005",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "49dac464-9b37-450a-95f8-82b6dd99bb93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MelMCKm3t005",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "654e8f07-e318-42aa-ccd2-0a4e46d85b11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "tZSmpJrct006",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7742a86-30c8-4c56-a0bd-5a997588e379",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "HXsrmqIKt007",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fe9dd500-8203-4a5e-c02b-44801b055059",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "As779vExt009"
   },
   "source": [
    "### For 'ServiceAvailable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "tdZbJnGdt009",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bc6a7fca-c852-4a3b-eed0-1ce2dc11b877",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_SA = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_SA = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=41585,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_SA = LogisticRegression(C=989.8989,class_weight='balanced',max_iter=653,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_SA = 6\n",
    "increaseTime_SA = 4\n",
    "\n",
    "best_lr_SA, pipe_SA, fu_SA, trainF_up_one_union_SA, errors_idxs_SA, all_errors_SA = build_best_lr(location,best_lr_SA,count_vect_SA,Tfidf_vect_SA, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1RpTYQGot00-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "339a6f26-44ff-4fd7-a784-430e572266df",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_SA, default_pipe_SA, default_fu_SA, default_trainF_up_one_union_SA, default_errors_idxs_SA, default_all_errors_SA = build_default_lr(location, category_SA,increaseTime_SA, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "N7yUCZxSt00_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e78c587b-bdae-4487-ffb8-600a9e93ab0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "EtKAE8Nat01A",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "87d63ec5-30f6-419d-e59a-cfe4779916d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EkpiZ5l2t01B",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c668fb28-6546-4be6-b346-a85b5f8267f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5V0cIx4st01C",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67aecda0-1985-4093-e827-79060bd245db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "LiqObWoit01C",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bbc8d81f-597c-4d61-f3fe-95caf49ff667",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "wWgIeI_2t01D",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f397b776-8b2a-462d-9513-bb3109e01f44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bb5f6L8t01F",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZasB4NTt01G"
   },
   "source": [
    "### For 'Advice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "esrd6pPwt01G",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e8ff6bd1-38d3-4715-a384-ca555079fc32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_A = CountVectorizer(tokenizer=tokenize_normalize,)\n",
    "Tfidf_vect_A = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=49010,ngram_range=(1,4),sublinear_tf=True)\n",
    "best_lr_A = LogisticRegression(C=343.434,class_weight='balanced',max_iter=7185,penalty='l2',solver='lbfgs')\n",
    "\n",
    "category_A = 7\n",
    "increaseTime_A = 1\n",
    "\n",
    "best_lr_A, pipe_A, fu_A, trainF_up_one_union_A, errors_idxs_A, all_errors_A = build_best_lr(location,best_lr_A,count_vect_A,Tfidf_vect_A, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lTfX_rHMt01H",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "00441eb8-efa2-4eb2-872d-7796c20ea8f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_lr_A, default_pipe_A, default_fu_A, default_trainF_up_one_union_A, default_errors_idxs_A, default_all_errors_A = build_default_lr(location, category_A,increaseTime_A, trainF,trainL_single,testF,testL_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "U_ZqRhV4t01H",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0c38552b-e176-4237-9473-1a6fef644843",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [trainF_union,trainL_single[category],testF_union,testL_single[category]]\n",
    "display_result_without_upsample(category,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "99wVG7nPt01I",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "90c80ba4-88fb-4872-9b04-eff549b8962d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"The best LR result with featureUnion\"\n",
    "runningSet = [trainF_up_one_union,trainL_up_one,testF_union,testL_single[category]]\n",
    "total_evaluation(best_lr,model_name,runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-tsqMe5Jt01J",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "eaea242c-8df3-460e-9032-c88bd2919c75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = best_lr.predict(trainF_up_one_union)\n",
    "errors_idxs = get_errors_indexs(trainL_up_one, predictions, trainF_up_one)\n",
    "all_errors = get_errors(trainL_up_one, predictions, trainF_up_one)\n",
    "len(errors_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vFFskQRXt01K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1758598d-1494-4798-80bc-99ad9c9032a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "eli5.show_weights(best_lr, feature_names=fu.get_feature_names(), top=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "NTry_mcmt01K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b51195e9-7f93-48c6-c20b-82fb32bc94b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = {'Predicted':predictions,'True_labels':trainL_up_one[category],}\n",
    "df = pd.DataFrame(temp)\n",
    "showCountingOfTrueLabels(df,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "RKknaRKJt01L",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f8489b0b-611a-43f0-f239-0fb3ef3d12ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(best_lr,trainF_up_one_union[0], feature_names=fu.get_feature_names(), top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6PoxFCYt01M",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XU3LmgODVwfp"
   },
   "source": [
    "# Classification on priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nj44Sh6_ZTtz"
   },
   "source": [
    "## NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4vb9RZcNk50",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprossesing\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "if 'not' in all_stopwords:\n",
    "    all_stopwords.remove('not')\n",
    "\n",
    "disable=['ner']\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "    \n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if (not token.is_stop):\n",
    "        if ((token.is_alpha or token.is_digit or token.like_url)):\n",
    "            normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_EUpwFxVzhq",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "Tweets = cv19_ny_labeled\n",
    "trainSet,testSet = generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juE2crAK94_k",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['priority']\n",
    "trainL = EF(trainSet,labelList)['priority']\n",
    "testL = EF(testSet,labelList)['priority']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvBsmjeraHU-",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSGz1TB6OzW2"
   },
   "source": [
    "#### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kL5_yeCXO0G7",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "\n",
    "fu_Tfidf = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "fu_count = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "cv_train = fu_count.fit_transform(trainF)\n",
    "cv_test = fu_count.transform(testF)\n",
    "\n",
    "tfv_train = fu_Tfidf.fit_transform(trainF)\n",
    "tfv_test = fu_Tfidf.transform(testF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_J5OXIQjO4yG",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set data strcture to store later result of models\n",
    "precision_all = {}\n",
    "recall_all = {}\n",
    "f1_all = {}\n",
    "# Define analysis function\n",
    "\n",
    "# One function to record all result\n",
    "def record_result(name,precision,recall,f1):\n",
    "    precision_all[name] = precision\n",
    "    recall_all[name] = recall\n",
    "    f1_all[name]= f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rzeho97bJsXh"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "uef23ZfpJtb1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "cb2fbd66-9e47-4ea9-be85-269f5cf7a1c6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofAWWtL2Kffg"
   },
   "source": [
    "#### LR with One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QcgjR1mTKgfb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7dfcbc5b-744e-4b96-e7b2-fa4fa84c67f2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "lr_priority(runningSet=runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJRL_bZqKjh2"
   },
   "source": [
    "#### LR with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D4-tE-ufKlRa",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9b9dcd96-5a0e-4d81-d392-c3d105b2bb2c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runningSet = [tfv_train,trainL, tfv_test, testL]\n",
    "lr_priority(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOdDxWlHKn3p"
   },
   "source": [
    "#### SVM with One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "colab_type": "code",
    "id": "-Hv_8_KvKo5z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9d21fce0-c70d-40be-d6df-876db8a133f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "svm_priority(runningSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sX8vVGMhiTIg",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAq9lswlKrdY"
   },
   "source": [
    "#### Tuning LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_-1F8kCiTnn",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['Low','Medium','High','Critical']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jPZQagV_ik-g",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "61cebb75-36ab-4cb7-b0ca-fb7d87599fea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "for increaseTime in range(0, 50):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in categories:\n",
    "    print('Currrent is classifying categorie %s'%(category))\n",
    "    print()\n",
    "\n",
    "    if category == 'Low':\n",
    "      continue\n",
    "\n",
    "    if category == 'Medium' and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    # if category == 'High' and increaseTime >= 6:\n",
    "    #   continue\n",
    "\n",
    "    # if category == 7 and increaseTime >= 4:\n",
    "    #   continue\n",
    "  \n",
    "\n",
    "    # step 1: build up-sample training set\n",
    "    trainSet_temp = copy.deepcopy(trainF)\n",
    "    label_temp = copy.deepcopy(trainL)\n",
    "    \n",
    "    # print('Before length %d'%(len(label_temp)))\n",
    "    idxs = np.where(np.array(label_temp) == category)[0]\n",
    "    # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "    \n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(category)\n",
    "\n",
    "    # step 2: feature union\n",
    "    trainSet_temp = fu_count.fit_transform(trainSet_temp)\n",
    "    tfv_test_temp = fu_count.transform(testF)\n",
    "\n",
    "\n",
    "    # step 3: train the LR\n",
    "    runningSet = [trainSet_temp,np.array(label_temp), tfv_test_temp, testL]\n",
    "    cv_scores, test_scores = lr_priority(runningSet)\n",
    "    record_nine_result(train_record, val_record, test_record, cv_scores, test_scores, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lrF25k0B7lX",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample_priority(location,records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend(prop={'size': 14})\n",
    "  \n",
    "  # plt.grid(b=None)\n",
    "  for key in records[category].keys():\n",
    "    best_value = 0\n",
    "    best_x = 0\n",
    "    count = -1\n",
    "    for value in records[category][key]:\n",
    "      count += 1\n",
    "      if value > best_value:\n",
    "        best_value = value\n",
    "        best_x = count\n",
    "    \n",
    "    plt.text(best_x,best_value+0.001,str('{:.4f}'.format(best_value)),fontsize=12)\n",
    "    plt.scatter(best_x,best_value,marker='o')  \n",
    "  if not os.path.exists('priority/'+location):\n",
    "    os.makedirs('priority/'+location)  \n",
    "  plt.savefig('priority/'+location+'/'+str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PBKCN2NzB9kH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "15d8fc8b-2921-48ac-987f-110647ddc0f0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'Low':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample_priority(location,train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample_priority(location,val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample_priority(location,test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A99Qtu2JEHYz"
   },
   "source": [
    "#### Search the best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GprCCLI1EG0C",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, KFold\n",
    "import copy\n",
    "def search_best_lR_priority(increaseTimes,location):\n",
    "  \n",
    "  \n",
    "  # iterate over a range of increaseTime and all category\n",
    "\n",
    "  # store the best parameter for each increaseTime\n",
    "  metrics_all = {'f1_best_all': defaultdict(),'best_paras': defaultdict()}\n",
    "  best_parameters_all = defaultdict()\n",
    "  cv_score_all = defaultdict()\n",
    "  test_scores_all = defaultdict()\n",
    "  for increaseTime in range(0,increaseTimes):\n",
    "    metrics_one_run = {'f1_best_each_cate': defaultdict()}\n",
    "    best_parameters_one_run = defaultdict()\n",
    "    cv_score_one_run = defaultdict()\n",
    "    print(\"Performing grid search on IncreaseTime %d\" %(increaseTime))\n",
    "    for category in categories:\n",
    "      if category == 'Low':\n",
    "        continue\n",
    "\n",
    "      if category == 'Medium' and increaseTime != 5:\n",
    "        continue\n",
    "\n",
    "      if category == 'High' and increaseTime != 6:\n",
    "        continue\n",
    "\n",
    "      if category == 'Critical' and increaseTime != 4:\n",
    "        continue\n",
    "\n",
    "      print(\"Performing grid search on category %s\" %(category))\n",
    "      # df_tmp = pd.read_csv('cache/'+location+'/upsampleData_'+str(category)+'_'+str(increaseTime)+'.csv')\n",
    "      # trainF = {'full_text':df_tmp['full_text'], 'hashtags': df_tmp['hashtags'], 'favorite_count': df_tmp['favorite_count']}\n",
    "      # trainL = list(df_tmp['target'])\n",
    "\n",
    "      # step 1: build up-sample training set\n",
    "      global trainF, trainL, testF, testL\n",
    "      trainSet_temp = copy.deepcopy(trainF)\n",
    "      label_temp = copy.deepcopy(trainL)\n",
    "      \n",
    "      # print('Before length %d'%(len(label_temp)))\n",
    "      idxs = np.where(np.array(label_temp) == category)[0]\n",
    "      # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "      \n",
    "      for idx in list(idxs):\n",
    "        # Duplicate (increaseTime) times\n",
    "        for j in range(increaseTime):\n",
    "          trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "          trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "          trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "          label_temp.append(category)\n",
    "      trainF = trainSet_temp\n",
    "      trainL = label_temp\n",
    "      # build the training set for feature union and gridSerachCV\n",
    "      trainF_tmp = []\n",
    "      testF_tmp = []\n",
    "      for i in range(len(trainF['full_text'])):\n",
    "        trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                          'favorite_count':trainF['favorite_count'][i]})\n",
    "      for i in range(len(testF['full_text'])):\n",
    "        testF_tmp.append({'full_text':testF['full_text'][i],'hashtags':testF['hashtags'][i],\n",
    "                          'favorite_count':testF['favorite_count'][i]})\n",
    "      \n",
    "      fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector_gridSearch(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',Pipeline([('selector', ItemSelector_gridSearch(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',Pipeline([('selector', numericalTransformer_gridSearch(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "      \n",
    "      fullpipe = Pipeline([\n",
    "\n",
    "                  ('fu', fu),\n",
    "                  ('lr', LogisticRegression())\n",
    "                 ])\n",
    "            \n",
    "      params = {\n",
    "          # Fill in the parameter\n",
    "          \n",
    "          'fu__full_text__cv__ngram_range':[(1,2),(1,3),(1,4)],\n",
    "          'fu__full_text__cv__binary': (True, False),\n",
    "          'fu__full_text__cv__max_features': np.linspace(100,50000,100,dtype=int),\n",
    "          'lr__penalty': ['l2'],\n",
    "          'lr__C': np.linspace(0.000001,1000,100,dtype=float),\n",
    "          'lr__class_weight': ['balanced'],\n",
    "          'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "          'lr__max_iter': np.linspace(50,1000,100,dtype=int),\n",
    "      }\n",
    "\n",
    "      # Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "      grid_search = RandomizedSearchCV(fullpipe,params,verbose=3,cv=3,n_iter=450, scoring='f1_macro', n_jobs=-1,pre_dispatch='2*n_jobs',)\n",
    "      \n",
    "      # print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "      # print(\"parameters:\")\n",
    "      # print(params)\n",
    "      #n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "      # FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "      grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "      # evaluate the test result\n",
    "      # testF_union_tmp = grid_search.transform(testF)\n",
    "\n",
    "      predictions = grid_search.predict(testF_tmp)\n",
    "      precision = precision_score(predictions, testL, average='macro' )\n",
    "      recall = recall_score(predictions, testL, average='macro')\n",
    "      accuracy = accuracy_score(predictions, testL)\n",
    "      f1 = fbeta_score(predictions, testL, 1, average='macro')\n",
    "\n",
    "      test_scores = {'accuracy': accuracy,'precision': precision,'f1':f1,'recall': recall}\n",
    "      print(\"Test result:\", test_scores)\n",
    "\n",
    "      print('type')\n",
    "      print(type(trainL[0]))\n",
    "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "      # print(\"Best parameters set:\")\n",
    "      best_parameters = grid_search.best_estimator_.get_params()\n",
    "      parameters_dict = {}\n",
    "      for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        parameters_dict[param_name] = best_parameters[param_name]\n",
    "\n",
    "\n",
    "      metrics_one_run['f1_best_each_cate'][category] = grid_search.best_score_\n",
    "      cv_score_one_run[category] = grid_search.cv_results_\n",
    "      best_parameters_one_run[category] = parameters_dict\n",
    "      print(best_parameters_one_run[category])\n",
    "      test_scores_all[increaseTime] = {category: test_scores}\n",
    "\n",
    "    best_parameters_all[increaseTime] = best_parameters_one_run\n",
    "    metrics_all['f1_best_all'][increaseTime] = metrics_one_run\n",
    "    cv_score_all[increaseTime] = best_parameters_one_run\n",
    "  return metrics_all, best_parameters_all, cv_score_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qzauGUEJUQVe",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "51bed2c0-f16e-439b-b966-f42aa1760266",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR_priority(45,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFI64fYmPwSS"
   },
   "source": [
    "### Build state of the art classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ozJWcb8NP2Jh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "aa96dd40-37eb-44ef-8d02-658be64bae8a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "                  \n",
    "location = 'NY'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_priority = CountVectorizer(tokenizer=tokenize_normalize,binary=True,max_features=47983,ngram_range=(1,3))\n",
    "best_lr_priority = LogisticRegression(C=424.24242,class_weight='balanced',max_iter=6884,penalty='l2',solver='liblinear')\n",
    "\n",
    "increaseTime_priority = 6\n",
    "category_priority = 'High'\n",
    "best_lr_priority, pipe_priority, fu_priority, testF_union_priority, errors_idxs_priority, all_errors_priority = build_best_lr_priority(best_lr_priority, count_vect_priority, category_priority, increaseTime_priority, trainF, trainL, testF, testL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jDueN4-9rs6m",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b0e5230c-78aa-4920-9cd7-dff234ba8f33",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_default_lr_priority(best_lr_priority,category_priority, increaseTime_priority, trainF, trainL, testF, testL, more=[cv_train,trainL, cv_test, testL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAFFs-S9ojmz"
   },
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "NN1IMtZrojm0",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "090619c8-1447-40f4-bae8-8d8d30efe591",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Plot a bar chart graph with the F1 score for each class - (subreddit on x-axis, F1 score on Y axis)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "result_logis = classification_report(best_lr_priority.predict(testF_union_priority), testL, digits=3)\n",
    "result_logis = result_logis.split()\n",
    "f1_classlabel = result_logis[4:-15:5]\n",
    "print(len(result_logis[4:-15]))\n",
    "f1_each = result_logis[7:-15:5]\n",
    "plt.figure(figsize=(5, 3))\n",
    "for i in range(4):\n",
    "    plt.bar(i,float(f1_each[i]))\n",
    "\n",
    "plt.title('F1 score of my LR model')\n",
    "plt.xticks(np.arange(20),f1_classlabel,rotation=90)\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Class label')\n",
    "plt.savefig('barGraph.png',dpi=200,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HJ1Z80waKmY",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "def plotCM(title,classifier,X_test, y_test):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     xticks_rotation='vertical',\n",
    "                                     cmap=plt.cm.Blues,ax=ax,\n",
    "                                     normalize='true')\n",
    "    \n",
    "    disp.ax_.set_title(title)\n",
    "    plt.savefig(title+'_'+'CM'+'_'+'priority'+'.png',dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "MwfT6Ag-ojm3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "38271c60-8b5c-4f0f-e8e0-e27b9ca76b3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the confusion matrix for the best model\n",
    "plotCM('LR_model(The Best with up-smapling)',best_lr_priority,testF_union_priority,testL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHyoaSkpRfzz"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HSKRsTgJRhx2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f390139d-effb-4efa-94d5-8f7bda489def",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_priority, feature_names=fu_priority.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "mkW5bI7_hfKE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9752a8bd-52c0-4bfa-a730-1bb63bbc8c32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "QsUutJGCmBfk",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ccaeb81-c953-4b70-ffd1-5e1a034c6d94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority[all_errors_priority.true_label=='Critical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "GqdDSCD2SQ_j",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "921ea474-4898-4020-bde8-c0fb9cd31f88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority.true_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Aybcqs4inka",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_all_featrues_of_one_sample(sample,idx):\n",
    "    for key in sample.keys():\n",
    "        print(key,sample[key][idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRSJNOTFjjD1",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "s0wnkXy0SkGO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "59799a4b-582f-459c-fdd4-d1c4460761de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(all_errors_priority.iloc[0])\n",
    "print(all_errors_priority.full_text[0])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[0]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "sCCG-evskieU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3e36be55-37d8-4aa0-928d-7407ced7fc5b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "id": "uAYYDkDGmIH8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a911d59-e84e-4902-8568-59c8ef942377",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "colab_type": "code",
    "id": "nH30hxStYtDY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "89546e1f-966a-4ad8-bdf6-d4a360b9e224",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 29\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K89yDjBRmkDf",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0rQaGngm99N",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXoIRLXEzZ4l"
   },
   "source": [
    "## WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mefb8J4FzZ4m",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprossesing\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "if 'not' in all_stopwords:\n",
    "    all_stopwords.remove('not')\n",
    "\n",
    "disable=['ner']\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "    \n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if (not token.is_stop):\n",
    "        if ((token.is_alpha or token.is_digit or token.like_url)):\n",
    "            normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZSX5hYAzZ4o",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "Tweets = cv19_ws_labeled\n",
    "trainSet,testSet = generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ckyO658zZ4p",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['priority']\n",
    "trainL = EF(trainSet,labelList)['priority']\n",
    "testL = EF(testSet,labelList)['priority']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0T5i_4kzZ4r"
   },
   "source": [
    "#### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Xg2uMQlzZ4s",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "\n",
    "fu_Tfidf = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "fu_count = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "cv_train = fu_count.fit_transform(trainF)\n",
    "cv_test = fu_count.transform(testF)\n",
    "\n",
    "tfv_train = fu_Tfidf.fit_transform(trainF)\n",
    "tfv_test = fu_Tfidf.transform(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOkGc_V4zZ4u"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "izJqIYNnzZ4u",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f29729b8-4ea9-4094-92d0-2cb32edf6aa6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gElHefx-zZ4v"
   },
   "source": [
    "#### LR with One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "-GqK6Ki2zZ4w",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ab4af017-2dee-498b-ed26-f95d10baab85",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "lr_priority(runningSet=runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_R2Mq3PzZ4x"
   },
   "source": [
    "#### LR with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "oknrMXZ0zZ4x",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "156a34b9-bc5a-465e-b8a1-6b9a2ae02136",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [tfv_train,trainL, tfv_test, testL]\n",
    "lr_priority(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gx1zA6kTzZ4y"
   },
   "source": [
    "#### SVM with One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "8_S3yEpxzZ4y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8d41c837-7f6a-4488-c662-583fee61638c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "svm_priority(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR2gp9trzZ40"
   },
   "source": [
    "#### Tuning LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_KMNjwKzZ40",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['Low','Medium','High','Critical']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "STxWd1HBzZ41",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "61cebb75-36ab-4cb7-b0ca-fb7d87599fea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "for increaseTime in range(0, 50):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in categories:\n",
    "    print('Currrent is classifying categorie %s'%(category))\n",
    "    print()\n",
    "\n",
    "    if category == 'Low':\n",
    "      continue\n",
    "\n",
    "    if category == 'Medium' and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    # if category == 'High' and increaseTime >= 6:\n",
    "    #   continue\n",
    "\n",
    "    # if category == 7 and increaseTime >= 4:\n",
    "    #   continue\n",
    "  \n",
    "\n",
    "    # step 1: build up-sample training set\n",
    "    trainSet_temp = copy.deepcopy(trainF)\n",
    "    label_temp = copy.deepcopy(trainL)\n",
    "    \n",
    "    # print('Before length %d'%(len(label_temp)))\n",
    "    idxs = np.where(np.array(label_temp) == category)[0]\n",
    "    # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "    \n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(category)\n",
    "\n",
    "    # step 2: feature union\n",
    "    trainSet_temp = fu_count.fit_transform(trainSet_temp)\n",
    "    tfv_test_temp = fu_count.transform(testF)\n",
    "\n",
    "\n",
    "    # step 3: train the LR\n",
    "    runningSet = [trainSet_temp,np.array(label_temp), tfv_test_temp, testL]\n",
    "    cv_scores, test_scores = lr_priority(runningSet)\n",
    "    record_nine_result(train_record, val_record, test_record, cv_scores, test_scores, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHphYv1DzZ43",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample_priority(location,records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend(prop={'size': 14})\n",
    "  \n",
    "  # plt.grid(b=None)\n",
    "  for key in records[category].keys():\n",
    "    best_value = 0\n",
    "    best_x = 0\n",
    "    count = -1\n",
    "    for value in records[category][key]:\n",
    "      count += 1\n",
    "      if value > best_value:\n",
    "        best_value = value\n",
    "        best_x = count\n",
    "    \n",
    "    plt.text(best_x,best_value+0.001,str('{:.4f}'.format(best_value)),fontsize=12)\n",
    "    plt.scatter(best_x,best_value,marker='o')  \n",
    "  if not os.path.exists('priority/'+location):\n",
    "    os.makedirs('priority/'+location)  \n",
    "  plt.savefig('priority/'+location+'/'+str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lZ_DiQN8zZ44",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "15d8fc8b-2921-48ac-987f-110647ddc0f0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'Low':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample_priority(location,train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample_priority(location,val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample_priority(location,test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuQYRZJKzZ45"
   },
   "source": [
    "#### Search the best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLz5WVVjzZ45",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, KFold\n",
    "import copy\n",
    "def search_best_lR_priority(increaseTimes,location):\n",
    "  \n",
    "  \n",
    "  # iterate over a range of increaseTime and all category\n",
    "\n",
    "  # store the best parameter for each increaseTime\n",
    "  metrics_all = {'f1_best_all': defaultdict(),'best_paras': defaultdict()}\n",
    "  best_parameters_all = defaultdict()\n",
    "  cv_score_all = defaultdict()\n",
    "  test_scores_all = defaultdict()\n",
    "  for increaseTime in range(0,increaseTimes):\n",
    "    metrics_one_run = {'f1_best_each_cate': defaultdict()}\n",
    "    best_parameters_one_run = defaultdict()\n",
    "    cv_score_one_run = defaultdict()\n",
    "    print(\"Performing grid search on IncreaseTime %d\" %(increaseTime))\n",
    "    for category in categories:\n",
    "      if category == 'Low':\n",
    "        continue\n",
    "\n",
    "      if category == 'Medium' and increaseTime != 5:\n",
    "        continue\n",
    "\n",
    "      if category == 'High' and increaseTime != 6:\n",
    "        continue\n",
    "\n",
    "      if category == 'Critical' and increaseTime != 4:\n",
    "        continue\n",
    "\n",
    "      print(\"Performing grid search on category %s\" %(category))\n",
    "      # df_tmp = pd.read_csv('cache/'+location+'/upsampleData_'+str(category)+'_'+str(increaseTime)+'.csv')\n",
    "      # trainF = {'full_text':df_tmp['full_text'], 'hashtags': df_tmp['hashtags'], 'favorite_count': df_tmp['favorite_count']}\n",
    "      # trainL = list(df_tmp['target'])\n",
    "\n",
    "      # step 1: build up-sample training set\n",
    "      global trainF, trainL, testF, testL\n",
    "      trainSet_temp = copy.deepcopy(trainF)\n",
    "      label_temp = copy.deepcopy(trainL)\n",
    "      \n",
    "      # print('Before length %d'%(len(label_temp)))\n",
    "      idxs = np.where(np.array(label_temp) == category)[0]\n",
    "      # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "      \n",
    "      for idx in list(idxs):\n",
    "        # Duplicate (increaseTime) times\n",
    "        for j in range(increaseTime):\n",
    "          trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "          trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "          trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "          label_temp.append(category)\n",
    "      trainF = trainSet_temp\n",
    "      trainL = label_temp\n",
    "      # build the training set for feature union and gridSerachCV\n",
    "      trainF_tmp = []\n",
    "      testF_tmp = []\n",
    "      for i in range(len(trainF['full_text'])):\n",
    "        trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                          'favorite_count':trainF['favorite_count'][i]})\n",
    "      for i in range(len(testF['full_text'])):\n",
    "        testF_tmp.append({'full_text':testF['full_text'][i],'hashtags':testF['hashtags'][i],\n",
    "                          'favorite_count':testF['favorite_count'][i]})\n",
    "      \n",
    "      fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector_gridSearch(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',Pipeline([('selector', ItemSelector_gridSearch(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',Pipeline([('selector', numericalTransformer_gridSearch(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "      \n",
    "      fullpipe = Pipeline([\n",
    "\n",
    "                  ('fu', fu),\n",
    "                  ('lr', LogisticRegression())\n",
    "                 ])\n",
    "            \n",
    "      params = {\n",
    "          # Fill in the parameter\n",
    "          \n",
    "          'fu__full_text__cv__ngram_range':[(1,2),(1,3),(1,4)],\n",
    "          'fu__full_text__cv__binary': (True, False),\n",
    "          'fu__full_text__cv__max_features': np.linspace(100,50000,100,dtype=int),\n",
    "          'lr__penalty': ['l2'],\n",
    "          'lr__C': np.linspace(0.000001,1000,100,dtype=float),\n",
    "          'lr__class_weight': ['balanced'],\n",
    "          'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "          'lr__max_iter': np.linspace(50,1000,100,dtype=int),\n",
    "      }\n",
    "\n",
    "      # Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "      grid_search = RandomizedSearchCV(fullpipe,params,verbose=3,cv=3,n_iter=450, scoring='f1_macro', n_jobs=-1,pre_dispatch='2*n_jobs',)\n",
    "      \n",
    "      # print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "      # print(\"parameters:\")\n",
    "      # print(params)\n",
    "      #n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "      # FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "      grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "      # evaluate the test result\n",
    "      # testF_union_tmp = grid_search.transform(testF)\n",
    "\n",
    "      predictions = grid_search.predict(testF_tmp)\n",
    "      precision = precision_score(predictions, testL, average='macro' )\n",
    "      recall = recall_score(predictions, testL, average='macro')\n",
    "      accuracy = accuracy_score(predictions, testL)\n",
    "      f1 = fbeta_score(predictions, testL, 1, average='macro')\n",
    "\n",
    "      test_scores = {'accuracy': accuracy,'precision': precision,'f1':f1,'recall': recall}\n",
    "      print(\"Test result:\", test_scores)\n",
    "\n",
    "      print('type')\n",
    "      print(type(trainL[0]))\n",
    "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "      # print(\"Best parameters set:\")\n",
    "      best_parameters = grid_search.best_estimator_.get_params()\n",
    "      parameters_dict = {}\n",
    "      for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        parameters_dict[param_name] = best_parameters[param_name]\n",
    "\n",
    "\n",
    "      metrics_one_run['f1_best_each_cate'][category] = grid_search.best_score_\n",
    "      cv_score_one_run[category] = grid_search.cv_results_\n",
    "      best_parameters_one_run[category] = parameters_dict\n",
    "      print(best_parameters_one_run[category])\n",
    "      test_scores_all[increaseTime] = {category: test_scores}\n",
    "\n",
    "    best_parameters_all[increaseTime] = best_parameters_one_run\n",
    "    metrics_all['f1_best_all'][increaseTime] = metrics_one_run\n",
    "    cv_score_all[increaseTime] = best_parameters_one_run\n",
    "  return metrics_all, best_parameters_all, cv_score_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IRURdhjqzZ46",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "51bed2c0-f16e-439b-b966-f42aa1760266",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR_priority(45,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBG2gLZkzZ47"
   },
   "source": [
    "### Build state of the art classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "6K5wLlqkzZ47",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "70fac766-f04f-4b30-f365-aa95ac54641a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                  \n",
    "location = 'WS'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_priority = CountVectorizer(tokenizer=tokenize_normalize,binary=True,max_features=47983,ngram_range=(1,3))\n",
    "best_lr_priority = LogisticRegression(C=424.24242,class_weight='balanced',max_iter=6884,penalty='l2',solver='liblinear')\n",
    "\n",
    "increaseTime_priority = 6\n",
    "category_priority = 'High'\n",
    "best_lr_priority, pipe_priority, fu_priority, testF_union_priority, errors_idxs_priority, all_errors_priority = build_best_lr_priority(best_lr_priority, count_vect_priority, category_priority, increaseTime_priority, trainF, trainL, testF, testL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z7KMX4tNzZ48",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b0e5230c-78aa-4920-9cd7-dff234ba8f33",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_default_lr_priority(best_lr_priority,category_priority, increaseTime_priority, trainF, trainL, testF, testL, more=[cv_train,trainL, cv_test, testL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz2ukCdKzZ49"
   },
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "MXiMQCF8zZ4-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "090619c8-1447-40f4-bae8-8d8d30efe591",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Plot a bar chart graph with the F1 score for each class - (subreddit on x-axis, F1 score on Y axis)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "result_logis = classification_report(best_lr_priority.predict(testF_union_priority), testL, digits=3)\n",
    "result_logis = result_logis.split()\n",
    "f1_classlabel = result_logis[4:-15:5]\n",
    "print(len(result_logis[4:-15]))\n",
    "f1_each = result_logis[7:-15:5]\n",
    "plt.figure(figsize=(5, 3))\n",
    "for i in range(4):\n",
    "    plt.bar(i,float(f1_each[i]))\n",
    "\n",
    "plt.title('F1 score of my LR model')\n",
    "plt.xticks(np.arange(20),f1_classlabel,rotation=90)\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Class label')\n",
    "plt.savefig('barGraph.png',dpi=200,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwzheW-SzZ4_",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "def plotCM(title,classifier,X_test, y_test):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     xticks_rotation='vertical',\n",
    "                                     cmap=plt.cm.Blues,ax=ax,\n",
    "                                     normalize='true')\n",
    "    \n",
    "    disp.ax_.set_title(title)\n",
    "    plt.savefig(title+'_'+'CM'+'_'+'priority'+'.png',dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "IMaIhnIbzZ5A",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "38271c60-8b5c-4f0f-e8e0-e27b9ca76b3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the confusion matrix for the best model\n",
    "plotCM('LR_model(The Best with up-smapling)',best_lr_priority,testF_union_priority,testL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siPATscLzZ5B"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Rd48ToSkzZ5B",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f390139d-effb-4efa-94d5-8f7bda489def",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_priority, feature_names=fu_priority.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "xRoKr0WezZ5D",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9752a8bd-52c0-4bfa-a730-1bb63bbc8c32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "mQcFPBFqzZ5E",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ccaeb81-c953-4b70-ffd1-5e1a034c6d94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority[all_errors_priority.true_label=='Critical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "AMUYP4sXzZ5F",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "921ea474-4898-4020-bde8-c0fb9cd31f88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority.true_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idcZUlrrzZ5G",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_all_featrues_of_one_sample(sample,idx):\n",
    "    for key in sample.keys():\n",
    "        print(key,sample[key][idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_Sz2-59zZ5H",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "b4Q6XtpkzZ5I",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "59799a4b-582f-459c-fdd4-d1c4460761de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(all_errors_priority.iloc[0])\n",
    "print(all_errors_priority.full_text[0])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[0]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "eVXbHNFtzZ5J",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3e36be55-37d8-4aa0-928d-7407ced7fc5b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "id": "ORwuDWcuzZ5K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a911d59-e84e-4902-8568-59c8ef942377",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "colab_type": "code",
    "id": "IeiK-nPizZ5N",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "89546e1f-966a-4ad8-bdf6-d4a360b9e224",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 29\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-gK_yHmzZ5O",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaKk9RBLzZ5P",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXLzOs-51Rjt"
   },
   "source": [
    "## DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruKAyMr_1Rju",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprossesing\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "if 'not' in all_stopwords:\n",
    "    all_stopwords.remove('not')\n",
    "\n",
    "disable=['ner']\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "    \n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if (not token.is_stop):\n",
    "        if ((token.is_alpha or token.is_digit or token.like_url)):\n",
    "            normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3N4yz6M1Rjv",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose one dataset to use in this section\n",
    "Tweets = cv19_dc_labeled\n",
    "trainSet,testSet = generateTrainingSetCV(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXOzy96W1Rjw",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract training set\n",
    "featureList = ['full_text','entities','favorite_count'] #,'favorited'\n",
    "trainF = EF(trainSet,featureList)\n",
    "testF = EF(testSet,featureList)\n",
    "\n",
    "labelList = ['priority']\n",
    "trainL = EF(trainSet,labelList)['priority']\n",
    "testL = EF(testSet,labelList)['priority']\n",
    "\n",
    "trainF = Ehashtags(trainF)\n",
    "testF = Ehashtags(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7p7T5SH1Rjx"
   },
   "source": [
    "#### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g72D6kZP1Rjx",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
    "\n",
    "fu_Tfidf = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('tf_idf',Tfidf_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "fu_count = FeatureUnion([('full_text',myPipeline([('selector', ItemSelector(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',myPipeline([('selector', ItemSelector(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',myPipeline([('selector', numericalTransformer(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "\n",
    "cv_train = fu_count.fit_transform(trainF)\n",
    "cv_test = fu_count.transform(testF)\n",
    "\n",
    "tfv_train = fu_Tfidf.fit_transform(trainF)\n",
    "tfv_test = fu_Tfidf.transform(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLfAt5581Rjz"
   },
   "source": [
    "#### Dummmy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "EGmUmrCo1Rjz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "94409ac7-2b67-45df-f3d0-7a8a4cbad6c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "DummyResult(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEDlJTIE1Rj0"
   },
   "source": [
    "#### LR with One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "SPMXsndA1Rj1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e0a9dc89-d6fe-410b-a8e1-3271c4b3c44a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "lr_priority(runningSet=runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UunrhOPO1Rj3"
   },
   "source": [
    "#### LR with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "sAhe3tBv1Rj3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bcee352d-1cb1-4388-c87d-78a29807ad3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [tfv_train,trainL, tfv_test, testL]\n",
    "lr_priority(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4cK4zUD1Rj4"
   },
   "source": [
    "#### SVM with One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "RQfzjtGC1Rj4",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a08f0022-74c2-42c7-b066-f73e3c71b700",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runningSet = [cv_train,trainL, cv_test, testL]\n",
    "svm_priority(runningSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSrflN2u1Rj6"
   },
   "source": [
    "#### Tuning LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDcqrOOH1Rj6",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_record = {}\n",
    "val_record = {}\n",
    "test_record = {}\n",
    "categories = ['Low','Medium','High','Critical']\n",
    "\n",
    "for category in categories:\n",
    "    train_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    val_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "    test_record[category] = {'accuracy':[],'precision':[],'f1':[],'recall':[]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oeQZvCR41Rj7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "61cebb75-36ab-4cb7-b0ca-fb7d87599fea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "for increaseTime in range(0, 50):  \n",
    "  print('Current increase %d times' %(increaseTime))\n",
    "  # up-sample the label\n",
    "\n",
    "  # Run logistic regression on up-sample\n",
    "  for category in categories:\n",
    "    print('Currrent is classifying categorie %s'%(category))\n",
    "    print()\n",
    "\n",
    "    if category == 'Low':\n",
    "      continue\n",
    "\n",
    "    if category == 'Medium' and increaseTime >= 5:\n",
    "      continue\n",
    "\n",
    "    # if category == 'High' and increaseTime >= 6:\n",
    "    #   continue\n",
    "\n",
    "    # if category == 7 and increaseTime >= 4:\n",
    "    #   continue\n",
    "  \n",
    "\n",
    "    # step 1: build up-sample training set\n",
    "    trainSet_temp = copy.deepcopy(trainF)\n",
    "    label_temp = copy.deepcopy(trainL)\n",
    "    \n",
    "    # print('Before length %d'%(len(label_temp)))\n",
    "    idxs = np.where(np.array(label_temp) == category)[0]\n",
    "    # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "    \n",
    "    for idx in list(idxs):\n",
    "      # Duplicate (increaseTime) times\n",
    "      for j in range(increaseTime):\n",
    "        trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "        trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "        trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "        label_temp.append(category)\n",
    "\n",
    "    # step 2: feature union\n",
    "    trainSet_temp = fu_count.fit_transform(trainSet_temp)\n",
    "    tfv_test_temp = fu_count.transform(testF)\n",
    "\n",
    "\n",
    "    # step 3: train the LR\n",
    "    runningSet = [trainSet_temp,np.array(label_temp), tfv_test_temp, testL]\n",
    "    cv_scores, test_scores = lr_priority(runningSet)\n",
    "    record_nine_result(train_record, val_record, test_record, cv_scores, test_scores, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0eo7WjY1Rj9",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_Upsample_priority(location,records,category,phase):\n",
    "  plt.figure()\n",
    "  plt.title('%s result for %s .'%(phase, category))\n",
    "  length = len(records[category]['accuracy'])\n",
    "  plt.plot(range(length),records[category]['accuracy'],label='accuracy')\n",
    "  plt.plot(range(length),records[category]['f1'],label='f1')\n",
    "  plt.plot(range(length),records[category]['recall'],label='recall')\n",
    "  plt.plot(range(length),records[category]['precision'],label='precision')\n",
    "  plt.xlabel('Increase Times')\n",
    "  plt.ylabel('Values')\n",
    "  # xticks = np.arange(16)\n",
    "  # plt.xticks(xticks)\n",
    "  plt.legend(prop={'size': 14})\n",
    "  \n",
    "  # plt.grid(b=None)\n",
    "  for key in records[category].keys():\n",
    "    best_value = 0\n",
    "    best_x = 0\n",
    "    count = -1\n",
    "    for value in records[category][key]:\n",
    "      count += 1\n",
    "      if value > best_value:\n",
    "        best_value = value\n",
    "        best_x = count\n",
    "    \n",
    "    plt.text(best_x,best_value+0.001,str('{:.4f}'.format(best_value)),fontsize=12)\n",
    "    plt.scatter(best_x,best_value,marker='o')  \n",
    "  if not os.path.exists('priority/'+location):\n",
    "    os.makedirs('priority/'+location)  \n",
    "  plt.savefig('priority/'+location+'/'+str(category)+'_'+phase+'.png',dpi=300,bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Euy8h33kamna",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "15d8fc8b-2921-48ac-987f-110647ddc0f0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "phases = ['Training','Validation','Test']\n",
    "for category in categories:\n",
    "  if category == 'Low':\n",
    "    continue\n",
    "  for phase in phases:\n",
    "    if phase == 'Training':\n",
    "      plot_metrics_Upsample_priority(location,train_record,category,phase)\n",
    "    elif phase == 'Validation':\n",
    "      plot_metrics_Upsample_priority(location,val_record,category,phase)\n",
    "    else:\n",
    "      plot_metrics_Upsample_priority(location,test_record,category,phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5BcwgWQ1RkA"
   },
   "source": [
    "#### Search the best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZSKj1181RkA",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, KFold\n",
    "import copy\n",
    "def search_best_lR_priority(increaseTimes,location):\n",
    "  \n",
    "  \n",
    "  # iterate over a range of increaseTime and all category\n",
    "\n",
    "  # store the best parameter for each increaseTime\n",
    "  metrics_all = {'f1_best_all': defaultdict(),'best_paras': defaultdict()}\n",
    "  best_parameters_all = defaultdict()\n",
    "  cv_score_all = defaultdict()\n",
    "  test_scores_all = defaultdict()\n",
    "  for increaseTime in range(0,increaseTimes):\n",
    "    metrics_one_run = {'f1_best_each_cate': defaultdict()}\n",
    "    best_parameters_one_run = defaultdict()\n",
    "    cv_score_one_run = defaultdict()\n",
    "    print(\"Performing grid search on IncreaseTime %d\" %(increaseTime))\n",
    "    for category in categories:\n",
    "      if category == 'Low':\n",
    "        continue\n",
    "\n",
    "      if category == 'Medium' and increaseTime != 5:\n",
    "        continue\n",
    "\n",
    "      if category == 'High' and increaseTime != 6:\n",
    "        continue\n",
    "\n",
    "      if category == 'Critical' and increaseTime != 4:\n",
    "        continue\n",
    "\n",
    "      print(\"Performing grid search on category %s\" %(category))\n",
    "      # df_tmp = pd.read_csv('cache/'+location+'/upsampleData_'+str(category)+'_'+str(increaseTime)+'.csv')\n",
    "      # trainF = {'full_text':df_tmp['full_text'], 'hashtags': df_tmp['hashtags'], 'favorite_count': df_tmp['favorite_count']}\n",
    "      # trainL = list(df_tmp['target'])\n",
    "\n",
    "      # step 1: build up-sample training set\n",
    "      global trainF, trainL, testF, testL\n",
    "      trainSet_temp = copy.deepcopy(trainF)\n",
    "      label_temp = copy.deepcopy(trainL)\n",
    "      \n",
    "      # print('Before length %d'%(len(label_temp)))\n",
    "      idxs = np.where(np.array(label_temp) == category)[0]\n",
    "      # print('There are %d postitive sample in this category.'%(len(idxs)))\n",
    "      \n",
    "      for idx in list(idxs):\n",
    "        # Duplicate (increaseTime) times\n",
    "        for j in range(increaseTime):\n",
    "          trainSet_temp['hashtags'].append(trainSet_temp['hashtags'][idx])\n",
    "          trainSet_temp['full_text'].append(trainSet_temp['full_text'][idx])\n",
    "          trainSet_temp['favorite_count'].append(trainSet_temp['favorite_count'][idx])\n",
    "          label_temp.append(category)\n",
    "      trainF = trainSet_temp\n",
    "      trainL = label_temp\n",
    "      # build the training set for feature union and gridSerachCV\n",
    "      trainF_tmp = []\n",
    "      testF_tmp = []\n",
    "      for i in range(len(trainF['full_text'])):\n",
    "        trainF_tmp.append({'full_text':trainF['full_text'][i],'hashtags':trainF['hashtags'][i],\n",
    "                          'favorite_count':trainF['favorite_count'][i]})\n",
    "      for i in range(len(testF['full_text'])):\n",
    "        testF_tmp.append({'full_text':testF['full_text'][i],'hashtags':testF['hashtags'][i],\n",
    "                          'favorite_count':testF['favorite_count'][i]})\n",
    "      \n",
    "      fu = FeatureUnion([('full_text',Pipeline([('selector', ItemSelector_gridSearch(key='full_text')),('cv',count_vect),\n",
    "                ])),\n",
    "                ('hashtags',Pipeline([('selector', ItemSelector_gridSearch(key='hashtags')),\n",
    "                    ('cv',count_vect),\n",
    "                ])),\n",
    "                ('favorite_count',Pipeline([('selector', numericalTransformer_gridSearch(key='favorite_count')),\n",
    "                ])),\n",
    "                ],n_jobs=-1)\n",
    "      \n",
    "      fullpipe = Pipeline([\n",
    "\n",
    "                  ('fu', fu),\n",
    "                  ('lr', LogisticRegression())\n",
    "                 ])\n",
    "            \n",
    "      params = {\n",
    "          # Fill in the parameter\n",
    "          \n",
    "          'fu__full_text__cv__ngram_range':[(1,2),(1,3),(1,4)],\n",
    "          'fu__full_text__cv__binary': (True, False),\n",
    "          'fu__full_text__cv__max_features': np.linspace(100,50000,100,dtype=int),\n",
    "          'lr__penalty': ['l2'],\n",
    "          'lr__C': np.linspace(0.000001,1000,100,dtype=float),\n",
    "          'lr__class_weight': ['balanced'],\n",
    "          'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "          'lr__max_iter': np.linspace(50,1000,100,dtype=int),\n",
    "      }\n",
    "\n",
    "      # Pass in your pipeline, params (to tune), scoring, and split parameters here!\n",
    "      grid_search = RandomizedSearchCV(fullpipe,params,verbose=3,cv=3,n_iter=450, scoring='f1_macro', n_jobs=-1,pre_dispatch='2*n_jobs',)\n",
    "      \n",
    "      # print(\"pipeline:\", [name for name, _ in pipe.steps])\n",
    "      # print(\"parameters:\")\n",
    "      # print(params)\n",
    "      #n_jobs=-1,pre_dispatch='2*n_jobs'\n",
    "      # FILL IN HERE -- Fit grid_search on the combined train/validation data and labels.\n",
    "      grid_search.fit(trainF_tmp, trainL)\n",
    "\n",
    "      # evaluate the test result\n",
    "      # testF_union_tmp = grid_search.transform(testF)\n",
    "\n",
    "      predictions = grid_search.predict(testF_tmp)\n",
    "      precision = precision_score(predictions, testL, average='macro' )\n",
    "      recall = recall_score(predictions, testL, average='macro')\n",
    "      accuracy = accuracy_score(predictions, testL)\n",
    "      f1 = fbeta_score(predictions, testL, 1, average='macro')\n",
    "\n",
    "      test_scores = {'accuracy': accuracy,'precision': precision,'f1':f1,'recall': recall}\n",
    "      print(\"Test result:\", test_scores)\n",
    "\n",
    "      print('type')\n",
    "      print(type(trainL[0]))\n",
    "      print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "      # print(\"Best parameters set:\")\n",
    "      best_parameters = grid_search.best_estimator_.get_params()\n",
    "      parameters_dict = {}\n",
    "      for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        parameters_dict[param_name] = best_parameters[param_name]\n",
    "\n",
    "\n",
    "      metrics_one_run['f1_best_each_cate'][category] = grid_search.best_score_\n",
    "      cv_score_one_run[category] = grid_search.cv_results_\n",
    "      best_parameters_one_run[category] = parameters_dict\n",
    "      print(best_parameters_one_run[category])\n",
    "      test_scores_all[increaseTime] = {category: test_scores}\n",
    "\n",
    "    best_parameters_all[increaseTime] = best_parameters_one_run\n",
    "    metrics_all['f1_best_all'][increaseTime] = metrics_one_run\n",
    "    cv_score_all[increaseTime] = best_parameters_one_run\n",
    "  return metrics_all, best_parameters_all, cv_score_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pzCEsNoE1RkB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "51bed2c0-f16e-439b-b966-f42aa1760266",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = 'NY'\n",
    "metrics_all, best_parameters_all, cv_score_all = search_best_lR_priority(45,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7SWcTGW1RkB"
   },
   "source": [
    "### Build state of the art classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "8YD9yQUh1RkC",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4cb1811b-6c72-4359-857d-697bc1991300",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                  \n",
    "location = 'DC'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "count_vect_priority = CountVectorizer(tokenizer=tokenize_normalize,binary=True,max_features=47983,ngram_range=(1,3))\n",
    "best_lr_priority = LogisticRegression(C=424.24242,class_weight='balanced',max_iter=6884,penalty='l2',solver='liblinear')\n",
    "\n",
    "increaseTime_priority = 6\n",
    "category_priority = 'High'\n",
    "best_lr_priority, pipe_priority, fu_priority, testF_union_priority, errors_idxs_priority, all_errors_priority = build_best_lr_priority(best_lr_priority, count_vect_priority, category_priority, increaseTime_priority, trainF, trainL, testF, testL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RBJGCU4M1RkD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b0e5230c-78aa-4920-9cd7-dff234ba8f33",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_default_lr_priority(best_lr_priority,category_priority, increaseTime_priority, trainF, trainL, testF, testL, more=[cv_train,trainL, cv_test, testL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XD4WiQ4Q1RkE"
   },
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Qw0MW0sy1RkE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "090619c8-1447-40f4-bae8-8d8d30efe591",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Plot a bar chart graph with the F1 score for each class - (subreddit on x-axis, F1 score on Y axis)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "result_logis = classification_report(best_lr_priority.predict(testF_union_priority), testL, digits=3)\n",
    "result_logis = result_logis.split()\n",
    "f1_classlabel = result_logis[4:-15:5]\n",
    "print(len(result_logis[4:-15]))\n",
    "f1_each = result_logis[7:-15:5]\n",
    "plt.figure(figsize=(5, 3))\n",
    "for i in range(4):\n",
    "    plt.bar(i,float(f1_each[i]))\n",
    "\n",
    "plt.title('F1 score of my LR model')\n",
    "plt.xticks(np.arange(20),f1_classlabel,rotation=90)\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Class label')\n",
    "plt.savefig('barGraph.png',dpi=200,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i__4JdvP1RkF",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "def plotCM(title,classifier,X_test, y_test):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     xticks_rotation='vertical',\n",
    "                                     cmap=plt.cm.Blues,ax=ax,\n",
    "                                     normalize='true')\n",
    "    \n",
    "    disp.ax_.set_title(title)\n",
    "    plt.savefig(title+'_'+'CM'+'_'+'priority'+'.png',dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "WuFzfkFG1RkG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "38271c60-8b5c-4f0f-e8e0-e27b9ca76b3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the confusion matrix for the best model\n",
    "plotCM('LR_model(The Best with up-smapling)',best_lr_priority,testF_union_priority,testL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inGw9Mdm1RkG"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qQJmcDPN1RkH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f390139d-effb-4efa-94d5-8f7bda489def",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "eli5.show_weights(best_lr_priority, feature_names=fu_priority.get_feature_names(), top=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "hmAFIUp71RkI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9752a8bd-52c0-4bfa-a730-1bb63bbc8c32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "eSdg8wcT1RkJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3ccaeb81-c953-4b70-ffd1-5e1a034c6d94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority[all_errors_priority.true_label=='Critical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "BJizOXlN1RkK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "921ea474-4898-4020-bde8-c0fb9cd31f88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority.true_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBzgrWtM1RkL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_all_featrues_of_one_sample(sample,idx):\n",
    "    for key in sample.keys():\n",
    "        print(key,sample[key][idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LR3JQZ4z1RkM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_errors_priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "H2vzIwuN1RkM",
    "outputId": "59799a4b-582f-459c-fdd4-d1c4460761de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(all_errors_priority.iloc[0])\n",
    "print(all_errors_priority.full_text[0])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[0]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "uunYzhyU1RkN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3e36be55-37d8-4aa0-928d-7407ced7fc5b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "id": "X4EFgVrz1RkO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a911d59-e84e-4902-8568-59c8ef942377",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "colab_type": "code",
    "id": "OCyx1LrJ1RkP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "89546e1f-966a-4ad8-bdf6-d4a360b9e224",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 29\n",
    "print(all_errors_priority.iloc[idx])\n",
    "print(all_errors_priority.full_text[idx])\n",
    "eli5.show_prediction(best_lr_priority, testF_union_priority[errors_idxs_priority[idx]], feature_names=fu_priority.get_feature_names(), top=(50,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rL5mlfb1RkR",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrYzy4G61RkS",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9wxJWlcr8hF5",
    "a_h-EaDAiEcn",
    "KzF081HBiEcr",
    "IjpIaE2-iEc1",
    "VbOaWEixt0zz",
    "wZ2sV7y4t0z9",
    "NHg06OmFt00A",
    "B41iNn2lt00E"
   ],
   "name": "COVID19_all_locations_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
